{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2208b03f",
   "metadata": {},
   "source": [
    "# LLM Client 테스트 노트북\n",
    "이 노트북에서는 backend/app/llm/client.py의 control flow graph 생성 함수를 테스트합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3247dd37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from llm import client\n",
    "import asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f30a70a",
   "metadata": {},
   "source": [
    "## 테스트할 디렉토리와 파일 타입 지정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ddc885d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = \"/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/poc\"\n",
    "test2_dir = \"/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification\"\n",
    "file_type = \"py\"  # 파이썬 파일만"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b76ba98a",
   "metadata": {},
   "source": [
    "## 디렉토리 내 모든 파일에 대해 CFG 생성 (비동기 실행)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a9a1cee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Jupyter 환경에서는 이미 이벤트 루프가 실행 중이므로 nest_asyncio를 사용하거나, 아래와 같이 실행합니다.\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# results = await client.generate_control_flow_graphs_for_directory(test_dir, file_type)\n",
    "# results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d7f419d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source files found: ['/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_gradcam_demo.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_gender_classifier.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_gender_demo.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_color_demo.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_gradcam_demo.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_emotion_classifier.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_emotion_gender_demo.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/faces.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/emotion_gender_processor.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/datasets.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/preprocessor.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/grad_cam.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/inference.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/data_augmentation.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/visualizer.py', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/models/cnn.py']\n",
      "=== FILE: image_gradcam_demo.py ===\n",
      "   1: import sys\n",
      "   2: \n",
      "   3: import cv2\n",
      "   4: import numpy as np\n",
      "   5: from keras.models import load_model\n",
      "   6: \n",
      "   7: from utils.grad_cam import compile_gradient_function\n",
      "   8: from utils.grad_cam import compile_saliency_function\n",
      "   9: from utils.grad_cam import register_gradient\n",
      "  10: from utils.grad_cam import modify_backprop\n",
      "  11: from utils.grad_cam import calculate_guided_gradient_CAM\n",
      "  12: from utils.datasets import get_labels\n",
      "  13: from utils.inference import detect_faces\n",
      "  14: from utils.inference import apply_offsets\n",
      "  15: from utils.inference import load_detection_model\n",
      "  16: from utils.preprocessor import preprocess_input\n",
      "  17: from utils.inference import draw_bounding_box\n",
      "  18: from utils.inference import load_image\n",
      "  19: \n",
      "  20: \n",
      "  21: # parameters\n",
      "  22: image_path = sys.argv[1]\n",
      "  23: # task = sys.argv[2]\n",
      "  24: task = 'emotion'\n",
      "  25: if task == 'emotion':\n",
      "  26:     labels = get_labels('fer2013')\n",
      "  27:     offsets = (0, 0)\n",
      "  28:     # model_filename = '../trained_models/fer2013_big_XCEPTION.54-0.66.hdf5'\n",
      "  29:     model_filename = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
      "  30: elif task == 'gender':\n",
      "  31:     labels = get_labels('imdb')\n",
      "  32:     offsets = (30, 60)\n",
      "  33:     model_filename = '../trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5'\n",
      "  34: \n",
      "  35: color = (0, 255, 0)\n",
      "  36: \n",
      "  37: # loading models\n",
      "  38: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
      "  39: model = load_model(model_filename, compile=False)\n",
      "  40: target_size = model.input_shape[1:3]\n",
      "  41: face_detection = load_detection_model(detection_model_path)\n",
      "  42: \n",
      "  43: # loading images\n",
      "  44: rgb_image = load_image(image_path, grayscale=False)\n",
      "  45: gray_image = load_image(image_path, grayscale=True)\n",
      "  46: gray_image = np.squeeze(gray_image)\n",
      "  47: gray_image = gray_image.astype('uint8')\n",
      "  48: faces = detect_faces(face_detection, gray_image)\n",
      "  49: \n",
      "  50: # start prediction for every image\n",
      "  51: for face_coordinates in faces:\n",
      "  52: \n",
      "  53:     x1, x2, y1, y2 = apply_offsets(face_coordinates, offsets)\n",
      "  54:     rgb_face = rgb_image[y1:y2, x1:x2]\n",
      "  55: \n",
      "  56:     x1, x2, y1, y2 = apply_offsets(face_coordinates, offsets)\n",
      "  57:     gray_face = gray_image[y1:y2, x1:x2]\n",
      "  58: \n",
      "  59:     # processing input\n",
      "  60:     try:\n",
      "  61:         gray_face = cv2.resize(gray_face, (target_size))\n",
      "  62:     except:\n",
      "  63:         continue\n",
      "  64:     gray_face = preprocess_input(gray_face, True)\n",
      "  65:     gray_face = np.expand_dims(gray_face, 0)\n",
      "  66:     gray_face = np.expand_dims(gray_face, -1)\n",
      "  67: \n",
      "  68:     # prediction\n",
      "  69:     predicted_class = np.argmax(model.predict(gray_face))\n",
      "  70:     label_text = labels[predicted_class]\n",
      "  71: \n",
      "  72:     gradient_function = compile_gradient_function(model,\n",
      "  73:                             predicted_class, 'conv2d_7')\n",
      "  74:     register_gradient()\n",
      "  75:     guided_model = modify_backprop(model, 'GuidedBackProp', task)\n",
      "  76:     saliency_function = compile_saliency_function(guided_model, 'conv2d_7')\n",
      "  77: \n",
      "  78:     guided_gradCAM = calculate_guided_gradient_CAM(gray_face,\n",
      "  79:                         gradient_function, saliency_function)\n",
      "  80:     guided_gradCAM = cv2.resize(guided_gradCAM, (x2-x1, y2-y1))\n",
      "  81:     rgb_guided_gradCAM = np.repeat(guided_gradCAM[:, :, np.newaxis], 3, axis=2)\n",
      "  82:     rgb_image[y1:y2, x1:x2, :] = rgb_guided_gradCAM\n",
      "  83:     draw_bounding_box((x1, y1, x2 - x1, y2 - y1), rgb_image, color)\n",
      "  84: bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
      "  85: cv2.imwrite('../images/guided_gradCAM.png', bgr_image)\n",
      "=== END FILE: image_gradcam_demo.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_gradcam_demo.py: {\"nodes\": [{\"id\": \"image_gradcam_demo.main\", \"file\": \"image_gradcam_demo.py\", \"line_start\": 1, \"line_end\": 85, \"label\": \"main\", \"description\": \"Top-level script execution entry point.\"}], \"edges\": [\n",
      "  {\"id\": \"e_main_get_labels\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.datasets.get_labels\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_load_model\", \"source\": \"image_gradcam_demo.main\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_load_detection_model\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.inference.load_detection_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_load_image_gray\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.inference.load_image\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_load_image_rgb\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.inference.load_image\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_detect_faces\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.inference.detect_faces\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_apply_offsets_1\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.inference.apply_offsets\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_apply_offsets_2\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.inference.apply_offsets\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_preprocess_input\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.preprocessor.preprocess_input\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_compile_gradient_function\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.grad_cam.compile_gradient_function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_register_gradient\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.grad_cam.register_gradient\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_modify_backprop\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.grad_cam.modify_backprop\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_compile_saliency_function\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.grad_cam.compile_saliency_function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_calculate_guided_gradient_CAM\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.grad_cam.calculate_guided_gradient_CAM\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_draw_bounding_box\", \"source\": \"image_gradcam_demo.main\", \"target\": \"utils.inference.draw_bounding_box\", \"type\": \"call\"}\n",
      "]}\n",
      "Reasoning: {'id': 'rs_6829fa786220819182df01e4c247c7900ae142800097c3e3', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: train_gender_classifier.py ===\n",
      "   1: \"\"\"\n",
      "   2: File: train_gender_classifier.py\n",
      "   3: Author: Octavio Arriaga\n",
      "   4: Email: arriaga.camargo@gmail.com\n",
      "   5: Github: https://github.com/oarriaga\n",
      "   6: Description: Train gender classification model\n",
      "   7: \"\"\"\n",
      "   8: \n",
      "   9: from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
      "  10: from keras.callbacks import ReduceLROnPlateau\n",
      "  11: from utils.datasets import DataManager\n",
      "  12: from models.cnn import mini_XCEPTION\n",
      "  13: from utils.data_augmentation import ImageGenerator\n",
      "  14: from utils.datasets import split_imdb_data\n",
      "  15: \n",
      "  16: # parameters\n",
      "  17: batch_size = 32\n",
      "  18: num_epochs = 1000\n",
      "  19: validation_split = .2\n",
      "  20: do_random_crop = False\n",
      "  21: patience = 100\n",
      "  22: num_classes = 2\n",
      "  23: dataset_name = 'imdb'\n",
      "  24: input_shape = (64, 64, 1)\n",
      "  25: if input_shape[2] == 1:\n",
      "  26:     grayscale = True\n",
      "  27: images_path = '../datasets/imdb_crop/'\n",
      "  28: log_file_path = '../trained_models/gender_models/gender_training.log'\n",
      "  29: trained_models_path = '../trained_models/gender_models/gender_mini_XCEPTION'\n",
      "  30: \n",
      "  31: # data loader\n",
      "  32: data_loader = DataManager(dataset_name)\n",
      "  33: ground_truth_data = data_loader.get_data()\n",
      "  34: train_keys, val_keys = split_imdb_data(ground_truth_data, validation_split)\n",
      "  35: print('Number of training samples:', len(train_keys))\n",
      "  36: print('Number of validation samples:', len(val_keys))\n",
      "  37: image_generator = ImageGenerator(ground_truth_data, batch_size,\n",
      "  38:                                  input_shape[:2],\n",
      "  39:                                  train_keys, val_keys, None,\n",
      "  40:                                  path_prefix=images_path,\n",
      "  41:                                  vertical_flip_probability=0,\n",
      "  42:                                  grayscale=grayscale,\n",
      "  43:                                  do_random_crop=do_random_crop)\n",
      "  44: \n",
      "  45: # model parameters/compilation\n",
      "  46: model = mini_XCEPTION(input_shape, num_classes)\n",
      "  47: model.compile(optimizer='adam',\n",
      "  48:               loss='categorical_crossentropy',\n",
      "  49:               metrics=['accuracy'])\n",
      "  50: model.summary()\n",
      "  51: \n",
      "  52: # model callbacks\n",
      "  53: early_stop = EarlyStopping('val_loss', patience=patience)\n",
      "  54: reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
      "  55:                               patience=int(patience/2), verbose=1)\n",
      "  56: csv_logger = CSVLogger(log_file_path, append=False)\n",
      "  57: model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
      "  58: model_checkpoint = ModelCheckpoint(model_names,\n",
      "  59:                                    monitor='val_loss',\n",
      "  60:                                    verbose=1,\n",
      "  61:                                    save_best_only=True,\n",
      "  62:                                    save_weights_only=False)\n",
      "  63: callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
      "  64: \n",
      "  65: # training model\n",
      "  66: model.fit_generator(image_generator.flow(mode='train'),\n",
      "  67:                     steps_per_epoch=int(len(train_keys) / batch_size),\n",
      "  68:                     epochs=num_epochs, verbose=1,\n",
      "  69:                     callbacks=callbacks,\n",
      "  70:                     validation_data=image_generator.flow('val'),\n",
      "  71:                     validation_steps=int(len(val_keys) / batch_size))\n",
      "=== END FILE: train_gender_classifier.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_gender_classifier.py: {\"nodes\":[{\"id\":\"train_gender_classifier.main\",\"file\":\"train_gender_classifier.py\",\"line_start\":1,\"line_end\":71,\"label\":\"main\",\"description\":\"Top-level script that loads data, builds and trains the gender classification model.\"}],\"edges\":[{\"id\":\"e_main_utils.datasets.DataManager\",\"source\":\"train_gender_classifier.main\",\"target\":\"utils.datasets.DataManager\",\"type\":\"call\"},{\"id\":\"e_main_utils.datasets.DataManager.get_data\",\"source\":\"train_gender_classifier.main\",\"target\":\"utils.datasets.DataManager.get_data\",\"type\":\"call\"},{\"id\":\"e_main_utils.datasets.split_imdb_data\",\"source\":\"train_gender_classifier.main\",\"target\":\"utils.datasets.split_imdb_data\",\"type\":\"call\"},{\"id\":\"e_main_utils.data_augmentation.ImageGenerator\",\"source\":\"train_gender_classifier.main\",\"target\":\"utils.data_augmentation.ImageGenerator\",\"type\":\"call\"},{\"id\":\"e_main_models.cnn.mini_XCEPTION\",\"source\":\"train_gender_classifier.main\",\"target\":\"models.cnn.mini_XCEPTION\",\"type\":\"call\"},{\"id\":\"e_main_model.compile\",\"source\":\"train_gender_classifier.main\",\"target\":\"model.compile\",\"type\":\"call\"},{\"id\":\"e_main_model.summary\",\"source\":\"train_gender_classifier.main\",\"target\":\"model.summary\",\"type\":\"call\"},{\"id\":\"e_main_keras.callbacks.EarlyStopping\",\"source\":\"train_gender_classifier.main\",\"target\":\"keras.callbacks.EarlyStopping\",\"type\":\"call\"},{\"id\":\"e_main_keras.callbacks.ReduceLROnPlateau\",\"source\":\"train_gender_classifier.main\",\"target\":\"keras.callbacks.ReduceLROnPlateau\",\"type\":\"call\"},{\"id\":\"e_main_keras.callbacks.CSVLogger\",\"source\":\"train_gender_classifier.main\",\"target\":\"keras.callbacks.CSVLogger\",\"type\":\"call\"},{\"id\":\"e_main_keras.callbacks.ModelCheckpoint\",\"source\":\"train_gender_classifier.main\",\"target\":\"keras.callbacks.ModelCheckpoint\",\"type\":\"call\"},{\"id\":\"e_main_image_generator.flow\",\"source\":\"train_gender_classifier.main\",\"target\":\"utils.data_augmentation.ImageGenerator.flow\",\"type\":\"call\"},{\"id\":\"e_main_model.fit_generator\",\"source\":\"train_gender_classifier.main\",\"target\":\"model.fit_generator\",\"type\":\"call\"}]}\n",
      "Reasoning: {'id': 'rs_6829fa861084819187a05f0f57cac216070a63564aa8f2fd', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: video_emotion_gender_demo.py ===\n",
      "   1: from statistics import mode\n",
      "   2: \n",
      "   3: import cv2\n",
      "   4: from keras.models import load_model\n",
      "   5: import numpy as np\n",
      "   6: \n",
      "   7: from utils.datasets import get_labels\n",
      "   8: from utils.inference import detect_faces\n",
      "   9: from utils.inference import draw_text\n",
      "  10: from utils.inference import draw_bounding_box\n",
      "  11: from utils.inference import apply_offsets\n",
      "  12: from utils.inference import load_detection_model\n",
      "  13: from utils.preprocessor import preprocess_input\n",
      "  14: \n",
      "  15: # parameters for loading data and images\n",
      "  16: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
      "  17: emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
      "  18: gender_model_path = '../trained_models/gender_models/simple_CNN.81-0.96.hdf5'\n",
      "  19: emotion_labels = get_labels('fer2013')\n",
      "  20: gender_labels = get_labels('imdb')\n",
      "  21: font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "  22: \n",
      "  23: # hyper-parameters for bounding boxes shape\n",
      "  24: frame_window = 10\n",
      "  25: gender_offsets = (30, 60)\n",
      "  26: emotion_offsets = (20, 40)\n",
      "  27: \n",
      "  28: # loading models\n",
      "  29: face_detection = load_detection_model(detection_model_path)\n",
      "  30: emotion_classifier = load_model(emotion_model_path, compile=False)\n",
      "  31: gender_classifier = load_model(gender_model_path, compile=False)\n",
      "  32: \n",
      "  33: # getting input model shapes for inference\n",
      "  34: emotion_target_size = emotion_classifier.input_shape[1:3]\n",
      "  35: gender_target_size = gender_classifier.input_shape[1:3]\n",
      "  36: \n",
      "  37: # starting lists for calculating modes\n",
      "  38: gender_window = []\n",
      "  39: emotion_window = []\n",
      "  40: \n",
      "  41: # starting video streaming\n",
      "  42: cv2.namedWindow('window_frame')\n",
      "  43: video_capture = cv2.VideoCapture(0)\n",
      "  44: while True:\n",
      "  45: \n",
      "  46:     bgr_image = video_capture.read()[1]\n",
      "  47:     gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
      "  48:     rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
      "  49:     faces = detect_faces(face_detection, gray_image)\n",
      "  50: \n",
      "  51:     for face_coordinates in faces:\n",
      "  52: \n",
      "  53:         x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)\n",
      "  54:         rgb_face = rgb_image[y1:y2, x1:x2]\n",
      "  55: \n",
      "  56:         x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
      "  57:         gray_face = gray_image[y1:y2, x1:x2]\n",
      "  58:         try:\n",
      "  59:             rgb_face = cv2.resize(rgb_face, (gender_target_size))\n",
      "  60:             gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
      "  61:         except:\n",
      "  62:             continue\n",
      "  63:         gray_face = preprocess_input(gray_face, False)\n",
      "  64:         gray_face = np.expand_dims(gray_face, 0)\n",
      "  65:         gray_face = np.expand_dims(gray_face, -1)\n",
      "  66:         emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n",
      "  67:         emotion_text = emotion_labels[emotion_label_arg]\n",
      "  68:         emotion_window.append(emotion_text)\n",
      "  69: \n",
      "  70:         rgb_face = np.expand_dims(rgb_face, 0)\n",
      "  71:         rgb_face = preprocess_input(rgb_face, False)\n",
      "  72:         gender_prediction = gender_classifier.predict(rgb_face)\n",
      "  73:         gender_label_arg = np.argmax(gender_prediction)\n",
      "  74:         gender_text = gender_labels[gender_label_arg]\n",
      "  75:         gender_window.append(gender_text)\n",
      "  76: \n",
      "  77:         if len(gender_window) > frame_window:\n",
      "  78:             emotion_window.pop(0)\n",
      "  79:             gender_window.pop(0)\n",
      "  80:         try:\n",
      "  81:             emotion_mode = mode(emotion_window)\n",
      "  82:             gender_mode = mode(gender_window)\n",
      "  83:         except:\n",
      "  84:             continue\n",
      "  85: \n",
      "  86:         if gender_text == gender_labels[0]:\n",
      "  87:             color = (0, 0, 255)\n",
      "  88:         else:\n",
      "  89:             color = (255, 0, 0)\n",
      "  90: \n",
      "  91:         draw_bounding_box(face_coordinates, rgb_image, color)\n",
      "  92:         draw_text(face_coordinates, rgb_image, gender_mode,\n",
      "  93:                   color, 0, -20, 1, 1)\n",
      "  94:         draw_text(face_coordinates, rgb_image, emotion_mode,\n",
      "  95:                   color, 0, -45, 1, 1)\n",
      "  96: \n",
      "  97:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
      "  98:     cv2.imshow('window_frame', bgr_image)\n",
      "  99:     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      " 100:         break\n",
      " 101: video_capture.release()\n",
      " 102: cv2.destroyAllWindows()\n",
      "=== END FILE: video_emotion_gender_demo.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_gender_demo.py: {\"nodes\": [{\"id\": \"video_emotion_gender_demo.main\", \"file\": \"video_emotion_gender_demo.py\", \"line_start\": 1, \"line_end\": 102, \"label\": \"main\", \"description\": \"Main script entry point: loads models and runs the video processing loop.\"}], \"edges\": [\n",
      "  {\"id\": \"e_main_utils.datasets.get_labels_fer2013\", \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.datasets.get_labels\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.datasets.get_labels_imdb\",   \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.datasets.get_labels\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.load_detection_model\", \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.inference.load_detection_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_keras.models.load_model_emotion\",    \"source\": \"video_emotion_gender_demo.main\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_keras.models.load_model_gender\",     \"source\": \"video_emotion_gender_demo.main\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.namedWindow\",                   \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.namedWindow\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.VideoCapture\",                  \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.VideoCapture\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_detect_faces\",                      \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.inference.detect_faces\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_apply_offsets_gender\",              \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.inference.apply_offsets\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_apply_offsets_emotion\",             \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.inference.apply_offsets\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.resize_rgb_face\",               \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.resize\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.resize_gray_face\",              \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.resize\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.preprocessor.preprocess_input_gray\", \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.preprocessor.preprocess_input\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_np.expand_dims_1\",                  \"source\": \"video_emotion_gender_demo.main\", \"target\": \"numpy.expand_dims\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_np.expand_dims_2\",                  \"source\": \"video_emotion_gender_demo.main\", \"target\": \"numpy.expand_dims\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_emotion_classifier.predict\",        \"source\": \"video_emotion_gender_demo.main\", \"target\": \"keras.models.Model.predict\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_np.argmax_emotion\",                 \"source\": \"video_emotion_gender_demo.main\", \"target\": \"numpy.argmax\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_np.argmax_gender\",                  \"source\": \"video_emotion_gender_demo.main\", \"target\": \"numpy.argmax\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.preprocessor.preprocess_input_rgb\", \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.preprocessor.preprocess_input\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_gender_classifier.predict\",         \"source\": \"video_emotion_gender_demo.main\", \"target\": \"keras.models.Model.predict\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_statistics.mode_emotion\",           \"source\": \"video_emotion_gender_demo.main\", \"target\": \"statistics.mode\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_statistics.mode_gender\",            \"source\": \"video_emotion_gender_demo.main\", \"target\": \"statistics.mode\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.draw_bounding_box\", \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.inference.draw_bounding_box\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.draw_text_gender\",  \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.inference.draw_text\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.draw_text_emotion\", \"source\": \"video_emotion_gender_demo.main\", \"target\": \"utils.inference.draw_text\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.cvtColor_gray\",                 \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.cvtColor\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.cvtColor_bgr\",                  \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.cvtColor\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.imshow\",                        \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.imshow\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.waitKey\",                       \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.waitKey\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_video_capture.release\",             \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.VideoCapture.release\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.destroyAllWindows\",             \"source\": \"video_emotion_gender_demo.main\", \"target\": \"cv2.destroyAllWindows\", \"type\": \"call\"}\n",
      "]}\n",
      "Reasoning: {'id': 'rs_6829fa8f75208191a9f37f05957a94ec01b2ce7f89503692', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: video_emotion_color_demo.py ===\n",
      "   1: from statistics import mode\n",
      "   2: \n",
      "   3: import cv2\n",
      "   4: from keras.models import load_model\n",
      "   5: import numpy as np\n",
      "   6: \n",
      "   7: from utils.datasets import get_labels\n",
      "   8: from utils.inference import detect_faces\n",
      "   9: from utils.inference import draw_text\n",
      "  10: from utils.inference import draw_bounding_box\n",
      "  11: from utils.inference import apply_offsets\n",
      "  12: from utils.inference import load_detection_model\n",
      "  13: from utils.preprocessor import preprocess_input\n",
      "  14: \n",
      "  15: # parameters for loading data and images\n",
      "  16: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
      "  17: emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
      "  18: emotion_labels = get_labels('fer2013')\n",
      "  19: \n",
      "  20: # hyper-parameters for bounding boxes shape\n",
      "  21: frame_window = 10\n",
      "  22: emotion_offsets = (20, 40)\n",
      "  23: \n",
      "  24: # loading models\n",
      "  25: face_detection = load_detection_model(detection_model_path)\n",
      "  26: emotion_classifier = load_model(emotion_model_path, compile=False)\n",
      "  27: \n",
      "  28: # getting input model shapes for inference\n",
      "  29: emotion_target_size = emotion_classifier.input_shape[1:3]\n",
      "  30: \n",
      "  31: # starting lists for calculating modes\n",
      "  32: emotion_window = []\n",
      "  33: \n",
      "  34: # starting video streaming\n",
      "  35: cv2.namedWindow('window_frame')\n",
      "  36: video_capture = cv2.VideoCapture(0)\n",
      "  37: while True:\n",
      "  38:     bgr_image = video_capture.read()[1]\n",
      "  39:     gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
      "  40:     rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
      "  41:     faces = detect_faces(face_detection, gray_image)\n",
      "  42: \n",
      "  43:     for face_coordinates in faces:\n",
      "  44: \n",
      "  45:         x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
      "  46:         gray_face = gray_image[y1:y2, x1:x2]\n",
      "  47:         try:\n",
      "  48:             gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
      "  49:         except:\n",
      "  50:             continue\n",
      "  51: \n",
      "  52:         gray_face = preprocess_input(gray_face, True)\n",
      "  53:         gray_face = np.expand_dims(gray_face, 0)\n",
      "  54:         gray_face = np.expand_dims(gray_face, -1)\n",
      "  55:         emotion_prediction = emotion_classifier.predict(gray_face)\n",
      "  56:         emotion_probability = np.max(emotion_prediction)\n",
      "  57:         emotion_label_arg = np.argmax(emotion_prediction)\n",
      "  58:         emotion_text = emotion_labels[emotion_label_arg]\n",
      "  59:         emotion_window.append(emotion_text)\n",
      "  60: \n",
      "  61:         if len(emotion_window) > frame_window:\n",
      "  62:             emotion_window.pop(0)\n",
      "  63:         try:\n",
      "  64:             emotion_mode = mode(emotion_window)\n",
      "  65:         except:\n",
      "  66:             continue\n",
      "  67: \n",
      "  68:         if emotion_text == 'angry':\n",
      "  69:             color = emotion_probability * np.asarray((255, 0, 0))\n",
      "  70:         elif emotion_text == 'sad':\n",
      "  71:             color = emotion_probability * np.asarray((0, 0, 255))\n",
      "  72:         elif emotion_text == 'happy':\n",
      "  73:             color = emotion_probability * np.asarray((255, 255, 0))\n",
      "  74:         elif emotion_text == 'surprise':\n",
      "  75:             color = emotion_probability * np.asarray((0, 255, 255))\n",
      "  76:         else:\n",
      "  77:             color = emotion_probability * np.asarray((0, 255, 0))\n",
      "  78: \n",
      "  79:         color = color.astype(int)\n",
      "  80:         color = color.tolist()\n",
      "  81: \n",
      "  82:         draw_bounding_box(face_coordinates, rgb_image, color)\n",
      "  83:         draw_text(face_coordinates, rgb_image, emotion_mode,\n",
      "  84:                   color, 0, -45, 1, 1)\n",
      "  85: \n",
      "  86:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
      "  87:     cv2.imshow('window_frame', bgr_image)\n",
      "  88:     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "  89:         break\n",
      "  90: video_capture.release()\n",
      "  91: cv2.destroyAllWindows()\n",
      "=== END FILE: video_emotion_color_demo.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_color_demo.py: {\"nodes\": [\n",
      "  {\n",
      "    \"id\": \"video_emotion_color_demo.main\",\n",
      "    \"file\": \"video_emotion_color_demo.py\",\n",
      "    \"line_start\": 1,\n",
      "    \"line_end\": 91,\n",
      "    \"label\": \"main\",\n",
      "    \"description\": \"Top-level script executing video capture, face detection, emotion recognition, and annotated display loop.\"\n",
      "  }\n",
      "],\n",
      " \"edges\": [\n",
      "  {\n",
      "    \"id\": \"e_main_utils.datasets.get_labels\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"utils.datasets.get_labels\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Load emotion labels from dataset definitions\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"e_main_keras.models.load_model\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"keras.models.load_model\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Load the pretrained emotion classification model\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"e_main_utils.inference.load_detection_model\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"utils.inference.load_detection_model\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Load the face detection model (Haar cascade)\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"e_main_utils.inference.detect_faces\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"utils.inference.detect_faces\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Detect face bounding boxes in a grayscale frame\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"e_main_utils.inference.apply_offsets\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"utils.inference.apply_offsets\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Apply offsets to face coordinates for emotion ROI\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"e_main_utils.preprocessor.preprocess_input\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"utils.preprocessor.preprocess_input\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Normalize and scale the face image for model input\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"e_main_utils.inference.draw_bounding_box\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"utils.inference.draw_bounding_box\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Draw the colored rectangle around detected face\"\n",
      "  },\n",
      "  {\n",
      "    \"id\": \"e_main_utils.inference.draw_text\",\n",
      "    \"source\": \"video_emotion_color_demo.main\",\n",
      "    \"target\": \"utils.inference.draw_text\",\n",
      "    \"type\": \"call\",\n",
      "    \"description\": \"Render the emotion label text on the image\"\n",
      "  }\n",
      " ]\n",
      "}\n",
      "Reasoning: {'id': 'rs_6829faa3836c81919127b4ae0b6abc340059f3dc5abdc57b', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: video_gradcam_demo.py ===\n",
      "   1: import sys\n",
      "   2: \n",
      "   3: import cv2\n",
      "   4: import numpy as np\n",
      "   5: from keras.models import load_model\n",
      "   6: from utils.grad_cam import compile_gradient_function\n",
      "   7: from utils.grad_cam import compile_saliency_function\n",
      "   8: from utils.grad_cam import register_gradient\n",
      "   9: from utils.grad_cam import modify_backprop\n",
      "  10: from utils.grad_cam import calculate_guided_gradient_CAM\n",
      "  11: from utils.inference import detect_faces\n",
      "  12: from utils.inference import apply_offsets\n",
      "  13: from utils.inference import load_detection_model\n",
      "  14: from utils.preprocessor import preprocess_input\n",
      "  15: from utils.inference import draw_bounding_box\n",
      "  16: from utils.datasets import get_class_to_arg\n",
      "  17: \n",
      "  18: # getting the correct model given the input\n",
      "  19: # task = sys.argv[1]\n",
      "  20: # class_name = sys.argv[2]\n",
      "  21: task = 'emotion'\n",
      "  22: if task == 'gender':\n",
      "  23:     model_filename = '../trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5'\n",
      "  24:     class_to_arg = get_class_to_arg('imdb')\n",
      "  25:     # predicted_class = class_to_arg[class_name]\n",
      "  26:     predicted_class = 0\n",
      "  27:     offsets = (0, 0)\n",
      "  28: elif task == 'emotion':\n",
      "  29:     model_filename = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
      "  30:     # model_filename = '../trained_models/fer2013_big_XCEPTION.54-0.66.hdf5'\n",
      "  31:     class_to_arg = get_class_to_arg('fer2013')\n",
      "  32:     # predicted_class = class_to_arg[class_name]\n",
      "  33:     predicted_class = 0\n",
      "  34:     offsets = (0, 0)\n",
      "  35: \n",
      "  36: model = load_model(model_filename, compile=False)\n",
      "  37: gradient_function = compile_gradient_function(model, predicted_class, 'conv2d_7')\n",
      "  38: register_gradient()\n",
      "  39: guided_model = modify_backprop(model, 'GuidedBackProp', task)\n",
      "  40: saliency_function = compile_saliency_function(guided_model, 'conv2d_7')\n",
      "  41: \n",
      "  42: # parameters for loading data and images \n",
      "  43: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
      "  44: face_detection = load_detection_model(detection_model_path)\n",
      "  45: color = (0, 255, 0)\n",
      "  46: \n",
      "  47: # getting input model shapes for inference\n",
      "  48: target_size = model.input_shape[1:3]\n",
      "  49: \n",
      "  50: # starting lists for calculating modes\n",
      "  51: emotion_window = []\n",
      "  52: \n",
      "  53: # starting video streaming\n",
      "  54: cv2.namedWindow('window_frame')\n",
      "  55: video_capture = cv2.VideoCapture(0)\n",
      "  56: while True:\n",
      "  57:     bgr_image = video_capture.read()[1]\n",
      "  58:     gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n",
      "  59:     rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n",
      "  60:     faces = detect_faces(face_detection, gray_image)\n",
      "  61: \n",
      "  62:     for face_coordinates in faces:\n",
      "  63: \n",
      "  64:         x1, x2, y1, y2 = apply_offsets(face_coordinates, offsets)\n",
      "  65:         gray_face = gray_image[y1:y2, x1:x2]\n",
      "  66:         try:\n",
      "  67:             gray_face = cv2.resize(gray_face, (target_size))\n",
      "  68:         except:\n",
      "  69:             continue\n",
      "  70: \n",
      "  71:         gray_face = preprocess_input(gray_face, True)\n",
      "  72:         gray_face = np.expand_dims(gray_face, 0)\n",
      "  73:         gray_face = np.expand_dims(gray_face, -1)\n",
      "  74:         guided_gradCAM = calculate_guided_gradient_CAM(gray_face,\n",
      "  75:                             gradient_function, saliency_function)\n",
      "  76:         guided_gradCAM = cv2.resize(guided_gradCAM, (x2-x1, y2-y1))\n",
      "  77:         try:\n",
      "  78:             rgb_guided_gradCAM = np.repeat(guided_gradCAM[:, :, np.newaxis],\n",
      "  79:                                                                 3, axis=2)\n",
      "  80:             rgb_image[y1:y2, x1:x2, :] = rgb_guided_gradCAM\n",
      "  81:         except:\n",
      "  82:             continue\n",
      "  83:         draw_bounding_box((x1, y1, x2 - x1, y2 - y1), rgb_image, color)\n",
      "  84:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
      "  85:     try:\n",
      "  86:         cv2.imshow('window_frame', bgr_image)\n",
      "  87:     except:\n",
      "  88:         continue\n",
      "  89:     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
      "  90:         break\n",
      "  91: \n",
      "  92: \n",
      "=== END FILE: video_gradcam_demo.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_gradcam_demo.py: {\"nodes\": [{\"id\": \"video_gradcam_demo.main\", \"file\": \"video_gradcam_demo.py\", \"line_start\": 1, \"line_end\": 92, \"label\": \"main\", \"description\": \"Main script executing the video Grad-CAM demo.\"}], \"edges\": [  {\"id\": \"e_main_load_model\", \"source\": \"video_gradcam_demo.main\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},  {\"id\": \"e_main_compile_gradient_function\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.grad_cam.compile_gradient_function\", \"type\": \"call\"},  {\"id\": \"e_main_register_gradient\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.grad_cam.register_gradient\", \"type\": \"call\"},  {\"id\": \"e_main_modify_backprop\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.grad_cam.modify_backprop\", \"type\": \"call\"},  {\"id\": \"e_main_compile_saliency_function\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.grad_cam.compile_saliency_function\", \"type\": \"call\"},  {\"id\": \"e_main_load_detection_model\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.inference.load_detection_model\", \"type\": \"call\"},  {\"id\": \"e_main_detect_faces\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.inference.detect_faces\", \"type\": \"call\"},  {\"id\": \"e_main_apply_offsets\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.inference.apply_offsets\", \"type\": \"call\"},  {\"id\": \"e_main_preprocess_input\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.preprocessor.preprocess_input\", \"type\": \"call\"},  {\"id\": \"e_main_calculate_guided_gradient_CAM\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.grad_cam.calculate_guided_gradient_CAM\", \"type\": \"call\"},  {\"id\": \"e_main_draw_bounding_box\", \"source\": \"video_gradcam_demo.main\", \"target\": \"utils.inference.draw_bounding_box\", \"type\": \"call\"},  {\"id\": \"e_main_namedWindow\", \"source\": \"video_gradcam_demo.main\", \"target\": \"cv2.namedWindow\", \"type\": \"call\"},  {\"id\": \"e_main_VideoCapture\", \"source\": \"video_gradcam_demo.main\", \"target\": \"cv2.VideoCapture\", \"type\": \"call\"},  {\"id\": \"e_main_read\", \"source\": \"video_gradcam_demo.main\", \"target\": \"cv2.VideoCapture.read\", \"type\": \"call\"},  {\"id\": \"e_main_cvtColor\", \"source\": \"video_gradcam_demo.main\", \"target\": \"cv2.cvtColor\", \"type\": \"call\"},  {\"id\": \"e_main_resize\", \"source\": \"video_gradcam_demo.main\", \"target\": \"cv2.resize\", \"type\": \"call\"},  {\"id\": \"e_main_expand_dims\", \"source\": \"video_gradcam_demo.main\", \"target\": \"numpy.expand_dims\", \"type\": \"call\"},  {\"id\": \"e_main_repeat\", \"source\": \"video_gradcam_demo.main\", \"target\": \"numpy.repeat\", \"type\": \"call\"},  {\"id\": \"e_main_imshow\", \"source\": \"video_gradcam_demo.main\", \"target\": \"cv2.imshow\", \"type\": \"call\"},  {\"id\": \"e_main_waitKey\", \"source\": \"video_gradcam_demo.main\", \"target\": \"cv2.waitKey\", \"type\": \"call\"}]}\n",
      "Reasoning: {'id': 'rs_6829fab27c588191865acb99796c87b30c99b1a77d011a24', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: train_emotion_classifier.py ===\n",
      "   1: \"\"\"\n",
      "   2: File: train_emotion_classifier.py\n",
      "   3: Author: Octavio Arriaga\n",
      "   4: Email: arriaga.camargo@gmail.com\n",
      "   5: Github: https://github.com/oarriaga\n",
      "   6: Description: Train emotion classification model\n",
      "   7: \"\"\"\n",
      "   8: \n",
      "   9: from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n",
      "  10: from keras.callbacks import ReduceLROnPlateau\n",
      "  11: from keras.preprocessing.image import ImageDataGenerator\n",
      "  12: \n",
      "  13: from models.cnn import mini_XCEPTION\n",
      "  14: from utils.datasets import DataManager\n",
      "  15: from utils.datasets import split_data\n",
      "  16: from utils.preprocessor import preprocess_input\n",
      "  17: \n",
      "  18: # parameters\n",
      "  19: batch_size = 32\n",
      "  20: num_epochs = 10000\n",
      "  21: input_shape = (64, 64, 1)\n",
      "  22: validation_split = .2\n",
      "  23: verbose = 1\n",
      "  24: num_classes = 7\n",
      "  25: patience = 50\n",
      "  26: base_path = '../trained_models/emotion_models/'\n",
      "  27: \n",
      "  28: # data generator\n",
      "  29: data_generator = ImageDataGenerator(\n",
      "  30:                         featurewise_center=False,\n",
      "  31:                         featurewise_std_normalization=False,\n",
      "  32:                         rotation_range=10,\n",
      "  33:                         width_shift_range=0.1,\n",
      "  34:                         height_shift_range=0.1,\n",
      "  35:                         zoom_range=.1,\n",
      "  36:                         horizontal_flip=True)\n",
      "  37: \n",
      "  38: # model parameters/compilation\n",
      "  39: model = mini_XCEPTION(input_shape, num_classes)\n",
      "  40: model.compile(optimizer='adam', loss='categorical_crossentropy',\n",
      "  41:               metrics=['accuracy'])\n",
      "  42: model.summary()\n",
      "  43: \n",
      "  44: \n",
      "  45: datasets = ['fer2013']\n",
      "  46: for dataset_name in datasets:\n",
      "  47:     print('Training dataset:', dataset_name)\n",
      "  48: \n",
      "  49:     # callbacks\n",
      "  50:     log_file_path = base_path + dataset_name + '_emotion_training.log'\n",
      "  51:     csv_logger = CSVLogger(log_file_path, append=False)\n",
      "  52:     early_stop = EarlyStopping('val_loss', patience=patience)\n",
      "  53:     reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1,\n",
      "  54:                                   patience=int(patience/4), verbose=1)\n",
      "  55:     trained_models_path = base_path + dataset_name + '_mini_XCEPTION'\n",
      "  56:     model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'\n",
      "  57:     model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,\n",
      "  58:                                                     save_best_only=True)\n",
      "  59:     callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n",
      "  60: \n",
      "  61:     # loading dataset\n",
      "  62:     data_loader = DataManager(dataset_name, image_size=input_shape[:2])\n",
      "  63:     faces, emotions = data_loader.get_data()\n",
      "  64:     faces = preprocess_input(faces)\n",
      "  65:     num_samples, num_classes = emotions.shape\n",
      "  66:     train_data, val_data = split_data(faces, emotions, validation_split)\n",
      "  67:     train_faces, train_emotions = train_data\n",
      "  68:     model.fit_generator(data_generator.flow(train_faces, train_emotions,\n",
      "  69:                                             batch_size),\n",
      "  70:                         steps_per_epoch=len(train_faces) / batch_size,\n",
      "  71:                         epochs=num_epochs, verbose=1, callbacks=callbacks,\n",
      "  72:                         validation_data=val_data)\n",
      "=== END FILE: train_emotion_classifier.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_emotion_classifier.py: {\"nodes\": [{\"id\": \"train_emotion_classifier.main\", \"file\": \"train_emotion_classifier.py\", \"line_start\": 1, \"line_end\": 72, \"label\": \"main\", \"description\": \"Top‐level script that configures data generators, builds and trains the emotion classification model.\"}], \"edges\": [{\"id\": \"e_main_to_ImageDataGenerator\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.preprocessing.image.ImageDataGenerator\", \"type\": \"call\", \"description\": \"Instantiate image data generator for augmentation.\"}, {\"id\": \"e_main_to_mini_XCEPTION\", \"source\": \"train_emotion_classifier.main\", \"target\": \"models.cnn.mini_XCEPTION\", \"type\": \"call\", \"description\": \"Create the mini_XCEPTION convolutional model.\"}, {\"id\": \"e_main_to_Model_compile\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.models.Model.compile\", \"type\": \"call\", \"description\": \"Compile the model with optimizer, loss, and metrics.\"}, {\"id\": \"e_main_to_Model_summary\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.models.Model.summary\", \"type\": \"call\", \"description\": \"Print model architecture summary.\"}, {\"id\": \"e_main_to_CSVLogger\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.callbacks.CSVLogger\", \"type\": \"call\", \"description\": \"Create CSV logger callback.\"}, {\"id\": \"e_main_to_ModelCheckpoint\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.callbacks.ModelCheckpoint\", \"type\": \"call\", \"description\": \"Create model checkpoint callback.\"}, {\"id\": \"e_main_to_EarlyStopping\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.callbacks.EarlyStopping\", \"type\": \"call\", \"description\": \"Create early stopping callback.\"}, {\"id\": \"e_main_to_ReduceLROnPlateau\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.callbacks.ReduceLROnPlateau\", \"type\": \"call\", \"description\": \"Create learning rate reduction callback.\"}, {\"id\": \"e_main_to_DataManager\", \"source\": \"train_emotion_classifier.main\", \"target\": \"utils.datasets.DataManager\", \"type\": \"call\", \"description\": \"Instantiate data manager for loading dataset.\"}, {\"id\": \"e_main_to_get_data\", \"source\": \"train_emotion_classifier.main\", \"target\": \"utils.datasets.DataManager.get_data\", \"type\": \"call\", \"description\": \"Load faces and emotion labels.\"}, {\"id\": \"e_main_to_preprocess_input\", \"source\": \"train_emotion_classifier.main\", \"target\": \"utils.preprocessor.preprocess_input\", \"type\": \"call\", \"description\": \"Apply preprocessing to face images.\"}, {\"id\": \"e_main_to_split_data\", \"source\": \"train_emotion_classifier.main\", \"target\": \"utils.datasets.split_data\", \"type\": \"call\", \"description\": \"Split dataset into training and validation sets.\"}, {\"id\": \"e_main_to_flow\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.preprocessing.image.ImageDataGenerator.flow\", \"type\": \"call\", \"description\": \"Generate augmented batches from training data.\"}, {\"id\": \"e_main_to_fit_generator\", \"source\": \"train_emotion_classifier.main\", \"target\": \"keras.models.Model.fit_generator\", \"type\": \"call\", \"description\": \"Train the model using a data generator.\"}]}\n",
      "Reasoning: {'id': 'rs_6829fac21f6c819190ce11e0d4a0274c0b11d6f8d02b46aa', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: image_emotion_gender_demo.py ===\n",
      "   1: import sys\n",
      "   2: \n",
      "   3: import cv2\n",
      "   4: from keras.models import load_model\n",
      "   5: import numpy as np\n",
      "   6: \n",
      "   7: from utils.datasets import get_labels\n",
      "   8: from utils.inference import detect_faces\n",
      "   9: from utils.inference import draw_text\n",
      "  10: from utils.inference import draw_bounding_box\n",
      "  11: from utils.inference import apply_offsets\n",
      "  12: from utils.inference import load_detection_model\n",
      "  13: from utils.inference import load_image\n",
      "  14: from utils.preprocessor import preprocess_input\n",
      "  15: \n",
      "  16: # parameters for loading data and images\n",
      "  17: image_path = sys.argv[1]\n",
      "  18: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
      "  19: emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
      "  20: gender_model_path = '../trained_models/gender_models/simple_CNN.81-0.96.hdf5'\n",
      "  21: emotion_labels = get_labels('fer2013')\n",
      "  22: gender_labels = get_labels('imdb')\n",
      "  23: font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "  24: \n",
      "  25: # hyper-parameters for bounding boxes shape\n",
      "  26: gender_offsets = (30, 60)\n",
      "  27: gender_offsets = (10, 10)\n",
      "  28: emotion_offsets = (20, 40)\n",
      "  29: emotion_offsets = (0, 0)\n",
      "  30: \n",
      "  31: # loading models\n",
      "  32: face_detection = load_detection_model(detection_model_path)\n",
      "  33: emotion_classifier = load_model(emotion_model_path, compile=False)\n",
      "  34: gender_classifier = load_model(gender_model_path, compile=False)\n",
      "  35: \n",
      "  36: # getting input model shapes for inference\n",
      "  37: emotion_target_size = emotion_classifier.input_shape[1:3]\n",
      "  38: gender_target_size = gender_classifier.input_shape[1:3]\n",
      "  39: \n",
      "  40: # loading images\n",
      "  41: rgb_image = load_image(image_path, grayscale=False)\n",
      "  42: gray_image = load_image(image_path, grayscale=True)\n",
      "  43: gray_image = np.squeeze(gray_image)\n",
      "  44: gray_image = gray_image.astype('uint8')\n",
      "  45: \n",
      "  46: faces = detect_faces(face_detection, gray_image)\n",
      "  47: for face_coordinates in faces:\n",
      "  48:     x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)\n",
      "  49:     rgb_face = rgb_image[y1:y2, x1:x2]\n",
      "  50: \n",
      "  51:     x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
      "  52:     gray_face = gray_image[y1:y2, x1:x2]\n",
      "  53: \n",
      "  54:     try:\n",
      "  55:         rgb_face = cv2.resize(rgb_face, (gender_target_size))\n",
      "  56:         gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
      "  57:     except:\n",
      "  58:         continue\n",
      "  59: \n",
      "  60:     rgb_face = preprocess_input(rgb_face, False)\n",
      "  61:     rgb_face = np.expand_dims(rgb_face, 0)\n",
      "  62:     gender_prediction = gender_classifier.predict(rgb_face)\n",
      "  63:     gender_label_arg = np.argmax(gender_prediction)\n",
      "  64:     gender_text = gender_labels[gender_label_arg]\n",
      "  65: \n",
      "  66:     gray_face = preprocess_input(gray_face, True)\n",
      "  67:     gray_face = np.expand_dims(gray_face, 0)\n",
      "  68:     gray_face = np.expand_dims(gray_face, -1)\n",
      "  69:     emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n",
      "  70:     emotion_text = emotion_labels[emotion_label_arg]\n",
      "  71: \n",
      "  72:     if gender_text == gender_labels[0]:\n",
      "  73:         color = (0, 0, 255)\n",
      "  74:     else:\n",
      "  75:         color = (255, 0, 0)\n",
      "  76: \n",
      "  77:     draw_bounding_box(face_coordinates, rgb_image, color)\n",
      "  78:     draw_text(face_coordinates, rgb_image, gender_text, color, 0, -20, 1, 2)\n",
      "  79:     draw_text(face_coordinates, rgb_image, emotion_text, color, 0, -50, 1, 2)\n",
      "  80: \n",
      "  81: bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
      "  82: cv2.imwrite('../images/predicted_test_image.png', bgr_image)\n",
      "=== END FILE: image_emotion_gender_demo.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_emotion_gender_demo.py: {\"nodes\": [{\"id\": \"image_emotion_gender_demo.main\", \"file\": \"image_emotion_gender_demo.py\", \"line_start\": 1, \"line_end\": 82, \"label\": \"main\", \"description\": \"Script entry point that loads models, processes the image, performs face detection, emotion and gender prediction, and outputs the annotated image.\"}], \"edges\": [\n",
      "  {\"id\": \"e_main_utils.datasets.get_labels\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.datasets.get_labels\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_keras.models.load_model\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.load_detection_model\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.inference.load_detection_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.load_image\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.inference.load_image\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.detect_faces\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.inference.detect_faces\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.apply_offsets\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.inference.apply_offsets\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.preprocessor.preprocess_input\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.preprocessor.preprocess_input\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.draw_bounding_box\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.inference.draw_bounding_box\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_utils.inference.draw_text\", \"source\": \"image_emotion_gender_demo.main\", \"target\": \"utils.inference.draw_text\", \"type\": \"call\"}\n",
      "]}\n",
      "Reasoning: {'id': 'rs_6829facf01f08191b68f0d1ed03299910b2ffc10a1352b72', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: faces.py ===\n",
      "   1: from flask import Flask, jsonify, make_response, request, abort, redirect, send_file\n",
      "   2: import logging\n",
      "   3: \n",
      "   4: import emotion_gender_processor as eg_processor\n",
      "   5: \n",
      "   6: app = Flask(__name__)\n",
      "   7: \n",
      "   8: @app.route('/')\n",
      "   9: def index():\n",
      "  10:     return redirect(\"https://ekholabs.ai\", code=302)\n",
      "  11: \n",
      "  12: @app.route('/classifyImage', methods=['POST'])\n",
      "  13: def upload():\n",
      "  14:     try:\n",
      "  15:         image = request.files['image'].read()\n",
      "  16:         eg_processor.process_image(image)\n",
      "  17:         return send_file('/ekholabs/face-classifier/result/predicted_image.png', mimetype='image/png')\n",
      "  18:     except Exception as err:\n",
      "  19:         logging.error('An error has occurred whilst processing the file: \"{0}\"'.format(err))\n",
      "  20:         abort(400)\n",
      "  21: \n",
      "  22: @app.errorhandler(400)\n",
      "  23: def bad_request(erro):\n",
      "  24:     return make_response(jsonify({'error': 'We cannot process the file sent in the request.'}), 400)\n",
      "  25: \n",
      "  26: @app.errorhandler(404)\n",
      "  27: def not_found(error):\n",
      "  28:     return make_response(jsonify({'error': 'Resource no found.'}), 404)\n",
      "  29: \n",
      "  30: if __name__ == '__main__':\n",
      "  31:     app.run(debug=True, host='0.0.0.0', port=8084)\n",
      "=== END FILE: faces.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/faces.py: {\"nodes\": [{\"id\": \"faces.index\", \"file\": \"faces.py\", \"line_start\": 8, \"line_end\": 10, \"label\": \"index\", \"description\": \"Redirects root URL to external site.\"}, {\"id\": \"faces.upload\", \"file\": \"faces.py\", \"line_start\": 12, \"line_end\": 20, \"label\": \"upload\", \"description\": \"Handles image upload, processes it, and returns the annotated image.\"}, {\"id\": \"faces.bad_request\", \"file\": \"faces.py\", \"line_start\": 22, \"line_end\": 24, \"label\": \"bad_request\", \"description\": \"Custom handler for 400 errors, returns JSON error message.\"}, {\"id\": \"faces.not_found\", \"file\": \"faces.py\", \"line_start\": 26, \"line_end\": 28, \"label\": \"not_found\", \"description\": \"Custom handler for 404 errors, returns JSON error message.\"}, {\"id\": \"faces.main\", \"file\": \"faces.py\", \"line_start\": 30, \"line_end\": 31, \"label\": \"main\", \"description\": \"Starts the Flask application when run as a script.\"}], \"edges\": [{\"id\": \"e_faces.index_flask.redirect\", \"source\": \"faces.index\", \"target\": \"flask.redirect\", \"type\": \"call\", \"description\": \"Redirect to external URL.\"}, {\"id\": \"e_faces.upload_emotion_gender_processor.process_image\", \"source\": \"faces.upload\", \"target\": \"emotion_gender_processor.process_image\", \"type\": \"call\", \"description\": \"Process the uploaded image.\"}, {\"id\": \"e_faces.upload_flask.send_file\", \"source\": \"faces.upload\", \"target\": \"flask.send_file\", \"type\": \"call\", \"description\": \"Send back the processed image file.\"}, {\"id\": \"e_faces.upload_flask.abort\", \"source\": \"faces.upload\", \"target\": \"flask.abort\", \"type\": \"call\", \"description\": \"Abort the request on error.\"}, {\"id\": \"e_faces.bad_request_flask.make_response\", \"source\": \"faces.bad_request\", \"target\": \"flask.make_response\", \"type\": \"call\", \"description\": \"Construct the HTTP response for 400 errors.\"}, {\"id\": \"e_faces.bad_request_flask.jsonify\", \"source\": \"faces.bad_request\", \"target\": \"flask.jsonify\", \"type\": \"call\", \"description\": \"Serialize error message to JSON.\"}, {\"id\": \"e_faces.not_found_flask.make_response\", \"source\": \"faces.not_found\", \"target\": \"flask.make_response\", \"type\": \"call\", \"description\": \"Construct the HTTP response for 404 errors.\"}, {\"id\": \"e_faces.not_found_flask.jsonify\", \"source\": \"faces.not_found\", \"target\": \"flask.jsonify\", \"type\": \"call\", \"description\": \"Serialize error message to JSON.\"}, {\"id\": \"e_faces.main_flask.Flask.run\", \"source\": \"faces.main\", \"target\": \"flask.Flask.run\", \"type\": \"call\", \"description\": \"Run the Flask development server.\"}]}\n",
      "Reasoning: {'id': 'rs_6829fad89a888191ab2d1e9133cfb0370984bc2725290fb1', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: emotion_gender_processor.py ===\n",
      "   1: import os\n",
      "   2: import sys\n",
      "   3: import logging\n",
      "   4: \n",
      "   5: import cv2\n",
      "   6: from keras.models import load_model\n",
      "   7: import numpy as np\n",
      "   8: \n",
      "   9: from utils.datasets import get_labels\n",
      "  10: from utils.inference import detect_faces\n",
      "  11: from utils.inference import draw_text\n",
      "  12: from utils.inference import draw_bounding_box\n",
      "  13: from utils.inference import apply_offsets\n",
      "  14: from utils.inference import load_detection_model\n",
      "  15: from utils.inference import load_image\n",
      "  16: from utils.preprocessor import preprocess_input\n",
      "  17: \n",
      "  18: def process_image(image):\n",
      "  19: \n",
      "  20:     try:\n",
      "  21:         # parameters for loading data and images\n",
      "  22:         detection_model_path = './trained_models/detection_models/haarcascade_frontalface_default.xml'\n",
      "  23:         emotion_model_path = './trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
      "  24:         gender_model_path = './trained_models/gender_models/simple_CNN.81-0.96.hdf5'\n",
      "  25:         emotion_labels = get_labels('fer2013')\n",
      "  26:         gender_labels = get_labels('imdb')\n",
      "  27:         font = cv2.FONT_HERSHEY_SIMPLEX\n",
      "  28: \n",
      "  29:         # hyper-parameters for bounding boxes shape\n",
      "  30:         gender_offsets = (30, 60)\n",
      "  31:         gender_offsets = (10, 10)\n",
      "  32:         emotion_offsets = (20, 40)\n",
      "  33:         emotion_offsets = (0, 0)\n",
      "  34: \n",
      "  35:         # loading models\n",
      "  36:         face_detection = load_detection_model(detection_model_path)\n",
      "  37:         emotion_classifier = load_model(emotion_model_path, compile=False)\n",
      "  38:         gender_classifier = load_model(gender_model_path, compile=False)\n",
      "  39: \n",
      "  40:         # getting input model shapes for inference\n",
      "  41:         emotion_target_size = emotion_classifier.input_shape[1:3]\n",
      "  42:         gender_target_size = gender_classifier.input_shape[1:3]\n",
      "  43: \n",
      "  44:         # loading images\n",
      "  45:         image_array = np.fromstring(image, np.uint8)\n",
      "  46:         unchanged_image = cv2.imdecode(image_array, cv2.IMREAD_UNCHANGED)\n",
      "  47: \n",
      "  48:         rgb_image = cv2.cvtColor(unchanged_image, cv2.COLOR_BGR2RGB)\n",
      "  49:         gray_image = cv2.cvtColor(unchanged_image, cv2.COLOR_BGR2GRAY)\n",
      "  50: \n",
      "  51:         faces = detect_faces(face_detection, gray_image)\n",
      "  52:         for face_coordinates in faces:\n",
      "  53:             x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)\n",
      "  54:             rgb_face = rgb_image[y1:y2, x1:x2]\n",
      "  55: \n",
      "  56:             x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n",
      "  57:             gray_face = gray_image[y1:y2, x1:x2]\n",
      "  58: \n",
      "  59:             try:\n",
      "  60:                 rgb_face = cv2.resize(rgb_face, (gender_target_size))\n",
      "  61:                 gray_face = cv2.resize(gray_face, (emotion_target_size))\n",
      "  62:             except:\n",
      "  63:                 continue\n",
      "  64: \n",
      "  65:             rgb_face = preprocess_input(rgb_face, False)\n",
      "  66:             rgb_face = np.expand_dims(rgb_face, 0)\n",
      "  67:             gender_prediction = gender_classifier.predict(rgb_face)\n",
      "  68:             gender_label_arg = np.argmax(gender_prediction)\n",
      "  69:             gender_text = gender_labels[gender_label_arg]\n",
      "  70: \n",
      "  71:             gray_face = preprocess_input(gray_face, True)\n",
      "  72:             gray_face = np.expand_dims(gray_face, 0)\n",
      "  73:             gray_face = np.expand_dims(gray_face, -1)\n",
      "  74:             emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n",
      "  75:             emotion_text = emotion_labels[emotion_label_arg]\n",
      "  76: \n",
      "  77:             if gender_text == gender_labels[0]:\n",
      "  78:                 color = (0, 0, 255)\n",
      "  79:             else:\n",
      "  80:                 color = (255, 0, 0)\n",
      "  81: \n",
      "  82:             draw_bounding_box(face_coordinates, rgb_image, color)\n",
      "  83:             draw_text(face_coordinates, rgb_image, gender_text, color, 0, -20, 1, 2)\n",
      "  84:             draw_text(face_coordinates, rgb_image, emotion_text, color, 0, -50, 1, 2)\n",
      "  85:     except Exception as err:\n",
      "  86:         logging.error('Error in emotion gender processor: \"{0}\"'.format(err))\n",
      "  87: \n",
      "  88:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n",
      "  89: \n",
      "  90:     dirname = 'result'\n",
      "  91:     if not os.path.exists(dirname):\n",
      "  92:         os.mkdir(dirname)\n",
      "  93: \n",
      "  94:     cv2.imwrite(os.path.join(dirname, 'predicted_image.png'), bgr_image)\n",
      "=== END FILE: emotion_gender_processor.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/emotion_gender_processor.py: {\"nodes\": [{\"id\": \"emotion_gender_processor.process_image\", \"file\": \"emotion_gender_processor.py\", \"line_start\": 18, \"line_end\": 87, \"label\": \"process_image\", \"description\": \"Processes an input image: loads models, detects faces, predicts gender and emotion, and annotates the image.\"}], \"edges\": [\n",
      "  {\"id\": \"e_process_image_utils.datasets.get_labels\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"utils.datasets.get_labels\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_keras.models.load_model\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_utils.inference.load_detection_model\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"utils.inference.load_detection_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_numpy.fromstring\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"numpy.fromstring\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_cv2.imdecode\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"cv2.imdecode\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_cv2.cvtColor\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"cv2.cvtColor\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_utils.inference.detect_faces\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"utils.inference.detect_faces\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_utils.inference.apply_offsets\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"utils.inference.apply_offsets\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_cv2.resize\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"cv2.resize\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_utils.preprocessor.preprocess_input\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"utils.preprocessor.preprocess_input\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_numpy.expand_dims\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"numpy.expand_dims\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_gender_classifier.predict\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"gender_classifier.predict\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_numpy.argmax\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"numpy.argmax\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_emotion_classifier.predict\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"emotion_classifier.predict\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_utils.inference.draw_bounding_box\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"utils.inference.draw_bounding_box\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_utils.inference.draw_text\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"utils.inference.draw_text\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_process_image_cv2.imwrite\", \"source\": \"emotion_gender_processor.process_image\", \"target\": \"cv2.imwrite\", \"type\": \"call\"}\n",
      "]}\n",
      "Reasoning: {'id': 'rs_6829fae7094c8191807f61dc8a20d2900c460f7088a295d8', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: datasets.py ===\n",
      "   1: from scipy.io import loadmat\n",
      "   2: import pandas as pd\n",
      "   3: import numpy as np\n",
      "   4: from random import shuffle\n",
      "   5: import os\n",
      "   6: import cv2\n",
      "   7: \n",
      "   8: \n",
      "   9: class DataManager(object):\n",
      "  10:     \"\"\"Class for loading fer2013 emotion classification dataset or\n",
      "  11:         imdb gender classification dataset.\"\"\"\n",
      "  12:     def __init__(self, dataset_name='imdb',\n",
      "  13:                  dataset_path=None, image_size=(48, 48)):\n",
      "  14: \n",
      "  15:         self.dataset_name = dataset_name\n",
      "  16:         self.dataset_path = dataset_path\n",
      "  17:         self.image_size = image_size\n",
      "  18:         if self.dataset_path is not None:\n",
      "  19:             self.dataset_path = dataset_path\n",
      "  20:         elif self.dataset_name == 'imdb':\n",
      "  21:             self.dataset_path = '../datasets/imdb_crop/imdb.mat'\n",
      "  22:         elif self.dataset_name == 'fer2013':\n",
      "  23:             self.dataset_path = '../datasets/fer2013/fer2013.csv'\n",
      "  24:         elif self.dataset_name == 'KDEF':\n",
      "  25:             self.dataset_path = '../datasets/KDEF/'\n",
      "  26:         else:\n",
      "  27:             raise Exception(\n",
      "  28:                     'Incorrect dataset name, please input imdb or fer2013')\n",
      "  29: \n",
      "  30:     def get_data(self):\n",
      "  31:         if self.dataset_name == 'imdb':\n",
      "  32:             ground_truth_data = self._load_imdb()\n",
      "  33:         elif self.dataset_name == 'fer2013':\n",
      "  34:             ground_truth_data = self._load_fer2013()\n",
      "  35:         elif self.dataset_name == 'KDEF':\n",
      "  36:             ground_truth_data = self._load_KDEF()\n",
      "  37:         return ground_truth_data\n",
      "  38: \n",
      "  39:     def _load_imdb(self):\n",
      "  40:         face_score_treshold = 3\n",
      "  41:         dataset = loadmat(self.dataset_path)\n",
      "  42:         image_names_array = dataset['imdb']['full_path'][0, 0][0]\n",
      "  43:         gender_classes = dataset['imdb']['gender'][0, 0][0]\n",
      "  44:         face_score = dataset['imdb']['face_score'][0, 0][0]\n",
      "  45:         second_face_score = dataset['imdb']['second_face_score'][0, 0][0]\n",
      "  46:         face_score_mask = face_score > face_score_treshold\n",
      "  47:         second_face_score_mask = np.isnan(second_face_score)\n",
      "  48:         unknown_gender_mask = np.logical_not(np.isnan(gender_classes))\n",
      "  49:         mask = np.logical_and(face_score_mask, second_face_score_mask)\n",
      "  50:         mask = np.logical_and(mask, unknown_gender_mask)\n",
      "  51:         image_names_array = image_names_array[mask]\n",
      "  52:         gender_classes = gender_classes[mask].tolist()\n",
      "  53:         image_names = []\n",
      "  54:         for image_name_arg in range(image_names_array.shape[0]):\n",
      "  55:             image_name = image_names_array[image_name_arg][0]\n",
      "  56:             image_names.append(image_name)\n",
      "  57:         return dict(zip(image_names, gender_classes))\n",
      "  58: \n",
      "  59:     def _load_fer2013(self):\n",
      "  60:         data = pd.read_csv(self.dataset_path)\n",
      "  61:         pixels = data['pixels'].tolist()\n",
      "  62:         width, height = 48, 48\n",
      "  63:         faces = []\n",
      "  64:         for pixel_sequence in pixels:\n",
      "  65:             face = [int(pixel) for pixel in pixel_sequence.split(' ')]\n",
      "  66:             face = np.asarray(face).reshape(width, height)\n",
      "  67:             face = cv2.resize(face.astype('uint8'), self.image_size)\n",
      "  68:             faces.append(face.astype('float32'))\n",
      "  69:         faces = np.asarray(faces)\n",
      "  70:         faces = np.expand_dims(faces, -1)\n",
      "  71:         emotions = pd.get_dummies(data['emotion']).as_matrix()\n",
      "  72:         return faces, emotions\n",
      "  73: \n",
      "  74:     def _load_KDEF(self):\n",
      "  75:         class_to_arg = get_class_to_arg(self.dataset_name)\n",
      "  76:         num_classes = len(class_to_arg)\n",
      "  77: \n",
      "  78:         file_paths = []\n",
      "  79:         for folder, subfolders, filenames in os.walk(self.dataset_path):\n",
      "  80:             for filename in filenames:\n",
      "  81:                 if filename.lower().endswith(('.jpg')):\n",
      "  82:                     file_paths.append(os.path.join(folder, filename))\n",
      "  83: \n",
      "  84:         num_faces = len(file_paths)\n",
      "  85:         y_size, x_size = self.image_size\n",
      "  86:         faces = np.zeros(shape=(num_faces, y_size, x_size))\n",
      "  87:         emotions = np.zeros(shape=(num_faces, num_classes))\n",
      "  88:         for file_arg, file_path in enumerate(file_paths):\n",
      "  89:             image_array = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n",
      "  90:             image_array = cv2.resize(image_array, (y_size, x_size))\n",
      "  91:             faces[file_arg] = image_array\n",
      "  92:             file_basename = os.path.basename(file_path)\n",
      "  93:             file_emotion = file_basename[4:6]\n",
      "  94:             # there are two file names in the dataset\n",
      "  95:             # that don't match the given classes\n",
      "  96:             try:\n",
      "  97:                 emotion_arg = class_to_arg[file_emotion]\n",
      "  98:             except:\n",
      "  99:                 continue\n",
      " 100:             emotions[file_arg, emotion_arg] = 1\n",
      " 101:         faces = np.expand_dims(faces, -1)\n",
      " 102:         return faces, emotions\n",
      " 103: \n",
      " 104: \n",
      " 105: def get_labels(dataset_name):\n",
      " 106:     if dataset_name == 'fer2013':\n",
      " 107:         return {0: 'angry', 1: 'disgust', 2: 'fear', 3: 'happy',\n",
      " 108:                 4: 'sad', 5: 'surprise', 6: 'neutral'}\n",
      " 109:     elif dataset_name == 'imdb':\n",
      " 110:         return {0: 'woman', 1: 'man'}\n",
      " 111:     elif dataset_name == 'KDEF':\n",
      " 112:         return {0: 'AN', 1: 'DI', 2: 'AF', 3: 'HA', 4: 'SA', 5: 'SU', 6: 'NE'}\n",
      " 113:     else:\n",
      " 114:         raise Exception('Invalid dataset name')\n",
      " 115: \n",
      " 116: \n",
      " 117: def get_class_to_arg(dataset_name='fer2013'):\n",
      " 118:     if dataset_name == 'fer2013':\n",
      " 119:         return {'angry': 0, 'disgust': 1, 'fear': 2, 'happy': 3, 'sad': 4,\n",
      " 120:                 'surprise': 5, 'neutral': 6}\n",
      " 121:     elif dataset_name == 'imdb':\n",
      " 122:         return {'woman': 0, 'man': 1}\n",
      " 123:     elif dataset_name == 'KDEF':\n",
      " 124:         return {'AN': 0, 'DI': 1, 'AF': 2, 'HA': 3, 'SA': 4, 'SU': 5, 'NE': 6}\n",
      " 125:     else:\n",
      " 126:         raise Exception('Invalid dataset name')\n",
      " 127: \n",
      " 128: \n",
      " 129: def split_imdb_data(ground_truth_data, validation_split=.2, do_shuffle=False):\n",
      " 130:     ground_truth_keys = sorted(ground_truth_data.keys())\n",
      " 131:     if do_shuffle is not False:\n",
      " 132:         shuffle(ground_truth_keys)\n",
      " 133:     training_split = 1 - validation_split\n",
      " 134:     num_train = int(training_split * len(ground_truth_keys))\n",
      " 135:     train_keys = ground_truth_keys[:num_train]\n",
      " 136:     validation_keys = ground_truth_keys[num_train:]\n",
      " 137:     return train_keys, validation_keys\n",
      " 138: \n",
      " 139: \n",
      " 140: def split_data(x, y, validation_split=.2):\n",
      " 141:     num_samples = len(x)\n",
      " 142:     num_train_samples = int((1 - validation_split)*num_samples)\n",
      " 143:     train_x = x[:num_train_samples]\n",
      " 144:     train_y = y[:num_train_samples]\n",
      " 145:     val_x = x[num_train_samples:]\n",
      " 146:     val_y = y[num_train_samples:]\n",
      " 147:     train_data = (train_x, train_y)\n",
      " 148:     val_data = (val_x, val_y)\n",
      " 149:     return train_data, val_data\n",
      "=== END FILE: datasets.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/datasets.py: {\"nodes\":[{\"id\":\"datasets.DataManager.__init__\",\"file\":\"datasets.py\",\"line_start\":12,\"line_end\":28,\"label\":\"__init__\",\"description\":\"Initializes DataManager, sets dataset_name, dataset_path, image_size, and resolves default paths.\"},{\"id\":\"datasets.DataManager.get_data\",\"file\":\"datasets.py\",\"line_start\":30,\"line_end\":37,\"label\":\"get_data\",\"description\":\"Selects and invokes the appropriate dataset loading method based on dataset_name.\"},{\"id\":\"datasets.DataManager._load_imdb\",\"file\":\"datasets.py\",\"line_start\":39,\"line_end\":57,\"label\":\"_load_imdb\",\"description\":\"Loads and filters the IMDB dataset from a .mat file, returning a dict of image paths to gender labels.\"},{\"id\":\"datasets.DataManager._load_fer2013\",\"file\":\"datasets.py\",\"line_start\":59,\"line_end\":72,\"label\":\"_load_fer2013\",\"description\":\"Loads the FER2013 CSV dataset, processes pixel strings into resized images and one-hot emotion labels.\"},{\"id\":\"datasets.DataManager._load_KDEF\",\"file\":\"datasets.py\",\"line_start\":74,\"line_end\":102,\"label\":\"_load_KDEF\",\"description\":\"Traverses KDEF image folders, reads and resizes grayscale images, maps filenames to emotion classes.\"},{\"id\":\"datasets.get_labels\",\"file\":\"datasets.py\",\"line_start\":105,\"line_end\":114,\"label\":\"get_labels\",\"description\":\"Returns a mapping from numeric label to class name for a given dataset_name.\"},{\"id\":\"datasets.get_class_to_arg\",\"file\":\"datasets.py\",\"line_start\":117,\"line_end\":126,\"label\":\"get_class_to_arg\",\"description\":\"Returns a mapping from class name to numeric index for a given dataset_name.\"},{\"id\":\"datasets.split_imdb_data\",\"file\":\"datasets.py\",\"line_start\":129,\"line_end\":137,\"label\":\"split_imdb_data\",\"description\":\"Splits IMDB ground truth dict keys into training and validation lists, optionally shuffling.\"},{\"id\":\"datasets.split_data\",\"file\":\"datasets.py\",\"line_start\":140,\"line_end\":149,\"label\":\"split_data\",\"description\":\"Splits arrays x and y into training and validation subsets based on validation_split.\"}],\"edges\":[{\"id\":\"e_get_data__load_imdb\",\"source\":\"datasets.DataManager.get_data\",\"target\":\"datasets.DataManager._load_imdb\",\"type\":\"call\"},{\"id\":\"e_get_data__load_fer2013\",\"source\":\"datasets.DataManager.get_data\",\"target\":\"datasets.DataManager._load_fer2013\",\"type\":\"call\"},{\"id\":\"e_get_data__load_KDEF\",\"source\":\"datasets.DataManager.get_data\",\"target\":\"datasets.DataManager._load_KDEF\",\"type\":\"call\"},{\"id\":\"e__load_imdb_loadmat\",\"source\":\"datasets.DataManager._load_imdb\",\"target\":\"scipy.io.loadmat\",\"type\":\"call\"},{\"id\":\"e__load_imdb_numpy.isnan\",\"source\":\"datasets.DataManager._load_imdb\",\"target\":\"numpy.isnan\",\"type\":\"call\"},{\"id\":\"e__load_imdb_numpy.logical_not\",\"source\":\"datasets.DataManager._load_imdb\",\"target\":\"numpy.logical_not\",\"type\":\"call\"},{\"id\":\"e__load_imdb_numpy.logical_and\",\"source\":\"datasets.DataManager._load_imdb\",\"target\":\"numpy.logical_and\",\"type\":\"call\"},{\"id\":\"e__load_fer2013_pandas.read_csv\",\"source\":\"datasets.DataManager._load_fer2013\",\"target\":\"pandas.read_csv\",\"type\":\"call\"},{\"id\":\"e__load_fer2013_numpy.asarray\",\"source\":\"datasets.DataManager._load_fer2013\",\"target\":\"numpy.asarray\",\"type\":\"call\"},{\"id\":\"e__load_fer2013_cv2.resize\",\"source\":\"datasets.DataManager._load_fer2013\",\"target\":\"cv2.resize\",\"type\":\"call\"},{\"id\":\"e__load_fer2013_numpy.expand_dims\",\"source\":\"datasets.DataManager._load_fer2013\",\"target\":\"numpy.expand_dims\",\"type\":\"call\"},{\"id\":\"e__load_fer2013_pandas.get_dummies\",\"source\":\"datasets.DataManager._load_fer2013\",\"target\":\"pandas.get_dummies\",\"type\":\"call\"},{\"id\":\"e__load_KDEF_get_class_to_arg\",\"source\":\"datasets.DataManager._load_KDEF\",\"target\":\"datasets.get_class_to_arg\",\"type\":\"call\"},{\"id\":\"e__load_KDEF_os.walk\",\"source\":\"datasets.DataManager._load_KDEF\",\"target\":\"os.walk\",\"type\":\"call\"},{\"id\":\"e__load_KDEF_os.path.join\",\"source\":\"datasets.DataManager._load_KDEF\",\"target\":\"os.path.join\",\"type\":\"call\"},{\"id\":\"e__load_KDEF_cv2.imread\",\"source\":\"datasets.DataManager._load_KDEF\",\"target\":\"cv2.imread\",\"type\":\"call\"},{\"id\":\"e__load_KDEF_cv2.resize\",\"source\":\"datasets.DataManager._load_KDEF\",\"target\":\"cv2.resize\",\"type\":\"call\"},{\"id\":\"e__load_KDEF_os.path.basename\",\"source\":\"datasets.DataManager._load_KDEF\",\"target\":\"os.path.basename\",\"type\":\"call\"},{\"id\":\"e_split_imdb_data_shuffle\",\"source\":\"datasets.split_imdb_data\",\"target\":\"random.shuffle\",\"type\":\"call\"}]}\n",
      "Reasoning: {'id': 'rs_6829faf563dc8191831a7ffda4b506bf0570050df5324de5', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: preprocessor.py ===\n",
      "   1: import numpy as np\n",
      "   2: from scipy.misc import imread, imresize\n",
      "   3: \n",
      "   4: \n",
      "   5: def preprocess_input(x, v2=True):\n",
      "   6:     x = x.astype('float32')\n",
      "   7:     x = x / 255.0\n",
      "   8:     if v2:\n",
      "   9:         x = x - 0.5\n",
      "  10:         x = x * 2.0\n",
      "  11:     return x\n",
      "  12: \n",
      "  13: \n",
      "  14: def _imread(image_name):\n",
      "  15:         return imread(image_name)\n",
      "  16: \n",
      "  17: \n",
      "  18: def _imresize(image_array, size):\n",
      "  19:         return imresize(image_array, size)\n",
      "  20: \n",
      "  21: \n",
      "  22: def to_categorical(integer_classes, num_classes=2):\n",
      "  23:     integer_classes = np.asarray(integer_classes, dtype='int')\n",
      "  24:     num_samples = integer_classes.shape[0]\n",
      "  25:     categorical = np.zeros((num_samples, num_classes))\n",
      "  26:     categorical[np.arange(num_samples), integer_classes] = 1\n",
      "  27:     return categorical\n",
      "=== END FILE: preprocessor.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/preprocessor.py: {\"nodes\":[{\"id\":\"preprocessor.preprocess_input\",\"file\":\"preprocessor.py\",\"line_start\":5,\"line_end\":11,\"label\":\"preprocess_input\",\"description\":\"Convert image array to float, normalize to [0,1], optionally scale to [-1,1]\"},{\"id\":\"preprocessor._imread\",\"file\":\"preprocessor.py\",\"line_start\":14,\"line_end\":16,\"label\":\"_imread\",\"description\":\"Wrapper around scipy.misc.imread to read an image from disk\"},{\"id\":\"preprocessor._imresize\",\"file\":\"preprocessor.py\",\"line_start\":18,\"line_end\":19,\"label\":\"_imresize\",\"description\":\"Wrapper around scipy.misc.imresize to resize an image array\"},{\"id\":\"preprocessor.to_categorical\",\"file\":\"preprocessor.py\",\"line_start\":22,\"line_end\":27,\"label\":\"to_categorical\",\"description\":\"Convert integer class labels to one-hot encoded (categorical) format\"}],\"edges\":[{\"id\":\"e_preprocess_input_imread\",\"source\":\"preprocessor._imread\",\"target\":\"scipy.misc.imread\",\"type\":\"call\"},{\"id\":\"e__imresize_imresize\",\"source\":\"preprocessor._imresize\",\"target\":\"scipy.misc.imresize\",\"type\":\"call\"},{\"id\":\"e_to_categorical_np_asarray\",\"source\":\"preprocessor.to_categorical\",\"target\":\"numpy.asarray\",\"type\":\"call\"},{\"id\":\"e_to_categorical_np_zeros\",\"source\":\"preprocessor.to_categorical\",\"target\":\"numpy.zeros\",\"type\":\"call\"},{\"id\":\"e_to_categorical_np_arange\",\"source\":\"preprocessor.to_categorical\",\"target\":\"numpy.arange\",\"type\":\"call\"}]}\n",
      "Reasoning: {'id': 'rs_6829fb0edf2081919cc3970ccaac5b0e0609ae60bf783196', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: grad_cam.py ===\n",
      "   1: import cv2\n",
      "   2: import h5py\n",
      "   3: import keras\n",
      "   4: import keras.backend as K\n",
      "   5: from keras.layers.core import Lambda\n",
      "   6: from keras.models import Sequential\n",
      "   7: from keras.models import load_model\n",
      "   8: import numpy as np\n",
      "   9: import tensorflow as tf\n",
      "  10: from tensorflow.python.framework import ops\n",
      "  11: \n",
      "  12: from .preprocessor import preprocess_input\n",
      "  13: \n",
      "  14: \n",
      "  15: def reset_optimizer_weights(model_filename):\n",
      "  16:     model = h5py.File(model_filename, 'r+')\n",
      "  17:     del model['optimizer_weights']\n",
      "  18:     model.close()\n",
      "  19: \n",
      "  20: \n",
      "  21: def target_category_loss(x, category_index, num_classes):\n",
      "  22:     return tf.multiply(x, K.one_hot([category_index], num_classes))\n",
      "  23: \n",
      "  24: \n",
      "  25: def target_category_loss_output_shape(input_shape):\n",
      "  26:     return input_shape\n",
      "  27: \n",
      "  28: \n",
      "  29: def normalize(x):\n",
      "  30:     # utility function to normalize a tensor by its L2 norm\n",
      "  31:     return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n",
      "  32: \n",
      "  33: \n",
      "  34: def load_image(image_array):\n",
      "  35:     image_array = np.expand_dims(image_array, axis=0)\n",
      "  36:     image_array = preprocess_input(image_array)\n",
      "  37:     return image_array\n",
      "  38: \n",
      "  39: \n",
      "  40: def register_gradient():\n",
      "  41:     if \"GuidedBackProp\" not in ops._gradient_registry._registry:\n",
      "  42:         @ops.RegisterGradient(\"GuidedBackProp\")\n",
      "  43:         def _GuidedBackProp(op, gradient):\n",
      "  44:             dtype = op.inputs[0].dtype\n",
      "  45:             guided_gradient = (gradient * tf.cast(gradient > 0., dtype) *\n",
      "  46:                                tf.cast(op.inputs[0] > 0., dtype))\n",
      "  47:             return guided_gradient\n",
      "  48: \n",
      "  49: \n",
      "  50: def compile_saliency_function(model, activation_layer='conv2d_7'):\n",
      "  51:     input_image = model.input\n",
      "  52:     layer_output = model.get_layer(activation_layer).output\n",
      "  53:     max_output = K.max(layer_output, axis=3)\n",
      "  54:     saliency = K.gradients(K.sum(max_output), input_image)[0]\n",
      "  55:     return K.function([input_image, K.learning_phase()], [saliency])\n",
      "  56: \n",
      "  57: \n",
      "  58: def modify_backprop(model, name, task):\n",
      "  59:     graph = tf.get_default_graph()\n",
      "  60:     with graph.gradient_override_map({'Relu': name}):\n",
      "  61: \n",
      "  62:         # get layers that have an activation\n",
      "  63:         activation_layers = [layer for layer in model.layers\n",
      "  64:                              if hasattr(layer, 'activation')]\n",
      "  65: \n",
      "  66:         # replace relu activation\n",
      "  67:         for layer in activation_layers:\n",
      "  68:             if layer.activation == keras.activations.relu:\n",
      "  69:                 layer.activation = tf.nn.relu\n",
      "  70: \n",
      "  71:         # re-instanciate a new model\n",
      "  72:         if task == 'gender':\n",
      "  73:             model_path = '../trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5'\n",
      "  74:         elif task == 'emotion':\n",
      "  75:             model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n",
      "  76:             # model_path = '../trained_models/fer2013_mini_XCEPTION.119-0.65.hdf5'\n",
      "  77:             # model_path = '../trained_models/fer2013_big_XCEPTION.54-0.66.hdf5'\n",
      "  78:         new_model = load_model(model_path, compile=False)\n",
      "  79:     return new_model\n",
      "  80: \n",
      "  81: \n",
      "  82: def deprocess_image(x):\n",
      "  83:     \"\"\" Same normalization as in:\n",
      "  84:     https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n",
      "  85:     \"\"\"\n",
      "  86:     if np.ndim(x) > 3:\n",
      "  87:         x = np.squeeze(x)\n",
      "  88:     # normalize tensor: center on 0., ensure std is 0.1\n",
      "  89:     x = x - x.mean()\n",
      "  90:     x = x / (x.std() + 1e-5)\n",
      "  91:     x = x * 0.1\n",
      "  92: \n",
      "  93:     # clip to [0, 1]\n",
      "  94:     x = x + 0.5\n",
      "  95:     x = np.clip(x, 0, 1)\n",
      "  96: \n",
      "  97:     # convert to RGB array\n",
      "  98:     x = x * 255\n",
      "  99:     if K.image_dim_ordering() == 'th':\n",
      " 100:         x = x.transpose((1, 2, 0))\n",
      " 101:     x = np.clip(x, 0, 255).astype('uint8')\n",
      " 102:     return x\n",
      " 103: \n",
      " 104: \n",
      " 105: def compile_gradient_function(input_model, category_index, layer_name):\n",
      " 106:     model = Sequential()\n",
      " 107:     model.add(input_model)\n",
      " 108: \n",
      " 109:     num_classes = model.output_shape[1]\n",
      " 110:     target_layer = lambda x: target_category_loss(x, category_index, num_classes)\n",
      " 111:     model.add(Lambda(target_layer,\n",
      " 112:                      output_shape=target_category_loss_output_shape))\n",
      " 113: \n",
      " 114:     loss = K.sum(model.layers[-1].output)\n",
      " 115:     conv_output = model.layers[0].get_layer(layer_name).output\n",
      " 116:     gradients = normalize(K.gradients(loss, conv_output)[0])\n",
      " 117:     gradient_function = K.function([model.layers[0].input, K.learning_phase()],\n",
      " 118:                                    [conv_output, gradients])\n",
      " 119:     return gradient_function\n",
      " 120: \n",
      " 121: \n",
      " 122: def calculate_gradient_weighted_CAM(gradient_function, image):\n",
      " 123:     output, evaluated_gradients = gradient_function([image, False])\n",
      " 124:     output, evaluated_gradients = output[0, :], evaluated_gradients[0, :, :, :]\n",
      " 125:     weights = np.mean(evaluated_gradients, axis=(0, 1))\n",
      " 126:     CAM = np.ones(output.shape[0: 2], dtype=np.float32)\n",
      " 127:     for weight_arg, weight in enumerate(weights):\n",
      " 128:         CAM = CAM + (weight * output[:, :, weight_arg])\n",
      " 129:     CAM = cv2.resize(CAM, (64, 64))\n",
      " 130:     CAM = np.maximum(CAM, 0)\n",
      " 131:     heatmap = CAM / np.max(CAM)\n",
      " 132: \n",
      " 133:     # Return to BGR [0..255] from the preprocessed image\n",
      " 134:     image = image[0, :]\n",
      " 135:     image = image - np.min(image)\n",
      " 136:     image = np.minimum(image, 255)\n",
      " 137: \n",
      " 138:     CAM = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n",
      " 139:     CAM = np.float32(CAM) + np.float32(image)\n",
      " 140:     CAM = 255 * CAM / np.max(CAM)\n",
      " 141:     return np.uint8(CAM), heatmap\n",
      " 142: \n",
      " 143: \n",
      " 144: def calculate_guided_gradient_CAM(\n",
      " 145:         preprocessed_input, gradient_function, saliency_function):\n",
      " 146:     CAM, heatmap = calculate_gradient_weighted_CAM(\n",
      " 147:             gradient_function, preprocessed_input)\n",
      " 148:     saliency = saliency_function([preprocessed_input, 0])\n",
      " 149:     # gradCAM = saliency[0] * heatmap[..., np.newaxis]\n",
      " 150:     # return deprocess_image(gradCAM)\n",
      " 151:     return deprocess_image(saliency[0])\n",
      " 152:     # return saliency[0]\n",
      " 153: \n",
      " 154: \n",
      " 155: def calculate_guided_gradient_CAM_v2(\n",
      " 156:         preprocessed_input, gradient_function,\n",
      " 157:         saliency_function, target_size=(128, 128)):\n",
      " 158:     CAM, heatmap = calculate_gradient_weighted_CAM(\n",
      " 159:             gradient_function, preprocessed_input)\n",
      " 160:     heatmap = np.squeeze(heatmap)\n",
      " 161:     heatmap = cv2.resize(heatmap.astype('uint8'), target_size)\n",
      " 162:     saliency = saliency_function([preprocessed_input, 0])\n",
      " 163:     saliency = np.squeeze(saliency[0])\n",
      " 164:     saliency = cv2.resize(saliency.astype('uint8'), target_size)\n",
      " 165:     gradCAM = saliency * heatmap\n",
      " 166:     gradCAM = deprocess_image(gradCAM)\n",
      " 167:     return np.expand_dims(gradCAM, -1)\n",
      " 168: \n",
      " 169: \n",
      " 170: if __name__ == '__main__':\n",
      " 171:     import pickle\n",
      " 172:     faces = pickle.load(open('faces.pkl', 'rb'))\n",
      " 173:     face = faces[0]\n",
      " 174:     model_filename = '../../trained_models/emotion_models/mini_XCEPTION.523-0.65.hdf5'\n",
      " 175:     # reset_optimizer_weights(model_filename)\n",
      " 176:     model = load_model(model_filename)\n",
      " 177: \n",
      " 178:     preprocessed_input = load_image(face)\n",
      " 179:     predictions = model.predict(preprocessed_input)\n",
      " 180:     predicted_class = np.argmax(predictions)\n",
      " 181:     gradient_function = compile_gradient_function(\n",
      " 182:             model, predicted_class, 'conv2d_6')\n",
      " 183:     register_gradient()\n",
      " 184:     guided_model = modify_backprop(model, 'GuidedBackProp')\n",
      " 185:     saliency_function = compile_saliency_function(guided_model)\n",
      " 186:     guided_gradCAM = calculate_guided_gradient_CAM(\n",
      " 187:             preprocessed_input, gradient_function, saliency_function)\n",
      " 188: \n",
      " 189:     cv2.imwrite('guided_gradCAM.jpg', guided_gradCAM)\n",
      " 190: \n",
      " 191: \n",
      "=== END FILE: grad_cam.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/grad_cam.py: {\"nodes\": [\n",
      "  {\"id\": \"grad_cam.reset_optimizer_weights\", \"file\": \"grad_cam.py\", \"line_start\": 15, \"line_end\": 18, \"label\": \"reset_optimizer_weights\", \"description\": \"Remove optimizer weights from an HDF5 model file.\"},\n",
      "  {\"id\": \"grad_cam.target_category_loss\", \"file\": \"grad_cam.py\", \"line_start\": 21, \"line_end\": 22, \"label\": \"target_category_loss\", \"description\": \"Apply one-hot mask to model output for a target category.\"},\n",
      "  {\"id\": \"grad_cam.target_category_loss_output_shape\", \"file\": \"grad_cam.py\", \"line_start\": 25, \"line_end\": 26, \"label\": \"target_category_loss_output_shape\", \"description\": \"Return output shape unchanged for Lambda layer.\"},\n",
      "  {\"id\": \"grad_cam.normalize\", \"file\": \"grad_cam.py\", \"line_start\": 29, \"line_end\": 31, \"label\": \"normalize\", \"description\": \"Normalize a tensor by its L2 norm.\"},\n",
      "  {\"id\": \"grad_cam.load_image\", \"file\": \"grad_cam.py\", \"line_start\": 34, \"line_end\": 37, \"label\": \"load_image\", \"description\": \"Expand dims and preprocess input image array.\"},\n",
      "  {\"id\": \"grad_cam.register_gradient\", \"file\": \"grad_cam.py\", \"line_start\": 40, \"line_end\": 47, \"label\": \"register_gradient\", \"description\": \"Register guided backpropagation gradient override in TensorFlow.\"},\n",
      "  {\"id\": \"grad_cam.compile_saliency_function\", \"file\": \"grad_cam.py\", \"line_start\": 50, \"line_end\": 55, \"label\": \"compile_saliency_function\", \"description\": \"Compile a Keras function to compute saliency maps.\"},\n",
      "  {\"id\": \"grad_cam.modify_backprop\", \"file\": \"grad_cam.py\", \"line_start\": 58, \"line_end\": 79, \"label\": \"modify_backprop\", \"description\": \"Override ReLU gradient and reload model for guided backpropagation.\"},\n",
      "  {\"id\": \"grad_cam.deprocess_image\", \"file\": \"grad_cam.py\", \"line_start\": 82, \"line_end\": 102, \"label\": \"deprocess_image\", \"description\": \"Convert a tensor into a displayable uint8 image.\"},\n",
      "  {\"id\": \"grad_cam.compile_gradient_function\", \"file\": \"grad_cam.py\", \"line_start\": 105, \"line_end\": 119, \"label\": \"compile_gradient_function\", \"description\": \"Create a function to compute gradients and feature maps for CAM.\"},\n",
      "  {\"id\": \"grad_cam.calculate_gradient_weighted_CAM\", \"file\": \"grad_cam.py\", \"line_start\": 122, \"line_end\": 141, \"label\": \"calculate_gradient_weighted_CAM\", \"description\": \"Compute class activation map (CAM) using gradients.\"},\n",
      "  {\"id\": \"grad_cam.calculate_guided_gradient_CAM\", \"file\": \"grad_cam.py\", \"line_start\": 144, \"line_end\": 151, \"label\": \"calculate_guided_gradient_CAM\", \"description\": \"Combine CAM and guided saliency to produce guided Grad-CAM.\"},\n",
      "  {\"id\": \"grad_cam.calculate_guided_gradient_CAM_v2\", \"file\": \"grad_cam.py\", \"line_start\": 155, \"line_end\": 167, \"label\": \"calculate_guided_gradient_CAM_v2\", \"description\": \"Alternative guided Grad-CAM with resizing.\"},\n",
      "  {\"id\": \"grad_cam.__main__\", \"file\": \"grad_cam.py\", \"line_start\": 170, \"line_end\": 190, \"label\": \"main\", \"description\": \"Script entry point: load model, compute and save guided Grad-CAM.\"}\n",
      "],\n",
      " \"edges\": [\n",
      "  {\"id\": \"e_reset_optimizer_weights_h5py.File\", \"source\": \"grad_cam.reset_optimizer_weights\", \"target\": \"h5py.File\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_target_category_loss_tf.multiply\", \"source\": \"grad_cam.target_category_loss\", \"target\": \"tf.multiply\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_target_category_loss_K.one_hot\", \"source\": \"grad_cam.target_category_loss\", \"target\": \"keras.backend.one_hot\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_normalize_K.sqrt\", \"source\": \"grad_cam.normalize\", \"target\": \"keras.backend.sqrt\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_normalize_K.mean\", \"source\": \"grad_cam.normalize\", \"target\": \"keras.backend.mean\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_normalize_K.square\", \"source\": \"grad_cam.normalize\", \"target\": \"keras.backend.square\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_load_image_np.expand_dims\", \"source\": \"grad_cam.load_image\", \"target\": \"numpy.expand_dims\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_load_image_preprocess_input\", \"source\": \"grad_cam.load_image\", \"target\": \"grad_cam.preprocess_input\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_register_gradient_ops.RegisterGradient\", \"source\": \"grad_cam.register_gradient\", \"target\": \"tensorflow.python.framework.ops.RegisterGradient\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_saliency_function_model.get_layer\", \"source\": \"grad_cam.compile_saliency_function\", \"target\": \"keras.models.Model.get_layer\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_saliency_function_K.max\", \"source\": \"grad_cam.compile_saliency_function\", \"target\": \"keras.backend.max\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_saliency_function_K.gradients\", \"source\": \"grad_cam.compile_saliency_function\", \"target\": \"keras.backend.gradients\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_saliency_function_K.function\", \"source\": \"grad_cam.compile_saliency_function\", \"target\": \"keras.backend.function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_modify_backprop_tf.get_default_graph\", \"source\": \"grad_cam.modify_backprop\", \"target\": \"tensorflow.get_default_graph\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_modify_backprop_graph.gradient_override_map\", \"source\": \"grad_cam.modify_backprop\", \"target\": \"tensorflow.Graph.gradient_override_map\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_modify_backprop_load_model\", \"source\": \"grad_cam.modify_backprop\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_gradient_function_Sequential\", \"source\": \"grad_cam.compile_gradient_function\", \"target\": \"keras.models.Sequential\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_gradient_function_Lambda\", \"source\": \"grad_cam.compile_gradient_function\", \"target\": \"keras.layers.core.Lambda\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_gradient_function_K.sum\", \"source\": \"grad_cam.compile_gradient_function\", \"target\": \"keras.backend.sum\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_gradient_function_K.gradients\", \"source\": \"grad_cam.compile_gradient_function\", \"target\": \"keras.backend.gradients\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_gradient_function_normalize\", \"source\": \"grad_cam.compile_gradient_function\", \"target\": \"grad_cam.normalize\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_compile_gradient_function_K.function\", \"source\": \"grad_cam.compile_gradient_function\", \"target\": \"keras.backend.function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_gradient_weighted_CAM_gradient_function\", \"source\": \"grad_cam.calculate_gradient_weighted_CAM\", \"target\": \"gradient_function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_gradient_weighted_CAM_np.mean\", \"source\": \"grad_cam.calculate_gradient_weighted_CAM\", \"target\": \"numpy.mean\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_gradient_weighted_CAM_cv2.resize\", \"source\": \"grad_cam.calculate_gradient_weighted_CAM\", \"target\": \"cv2.resize\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_gradient_weighted_CAM_cv2.applyColorMap\", \"source\": \"grad_cam.calculate_gradient_weighted_CAM\", \"target\": \"cv2.applyColorMap\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_calculate_gradient_weighted_CAM\", \"source\": \"grad_cam.calculate_guided_gradient_CAM\", \"target\": \"grad_cam.calculate_gradient_weighted_CAM\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_saliency_function\", \"source\": \"grad_cam.calculate_guided_gradient_CAM\", \"target\": \"saliency_function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_deprocess_image\", \"source\": \"grad_cam.calculate_guided_gradient_CAM\", \"target\": \"grad_cam.deprocess_image\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_v2_calculate_gradient_weighted_CAM\", \"source\": \"grad_cam.calculate_guided_gradient_CAM_v2\", \"target\": \"grad_cam.calculate_gradient_weighted_CAM\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_v2_cv2.resize_1\", \"source\": \"grad_cam.calculate_guided_gradient_CAM_v2\", \"target\": \"cv2.resize\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_v2_saliency_function\", \"source\": \"grad_cam.calculate_guided_gradient_CAM_v2\", \"target\": \"saliency_function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_v2_cv2.resize_2\", \"source\": \"grad_cam.calculate_guided_gradient_CAM_v2\", \"target\": \"cv2.resize\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_calculate_guided_gradient_CAM_v2_deprocess_image\", \"source\": \"grad_cam.calculate_guided_gradient_CAM_v2\", \"target\": \"grad_cam.deprocess_image\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_pickle.load\", \"source\": \"grad_cam.__main__\", \"target\": \"pickle.load\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_load_model\", \"source\": \"grad_cam.__main__\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_load_image\", \"source\": \"grad_cam.__main__\", \"target\": \"grad_cam.load_image\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_model.predict\", \"source\": \"grad_cam.__main__\", \"target\": \"model.predict\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_np.argmax\", \"source\": \"grad_cam.__main__\", \"target\": \"numpy.argmax\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_compile_gradient_function\", \"source\": \"grad_cam.__main__\", \"target\": \"grad_cam.compile_gradient_function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_register_gradient\", \"source\": \"grad_cam.__main__\", \"target\": \"grad_cam.register_gradient\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_modify_backprop\", \"source\": \"grad_cam.__main__\", \"target\": \"grad_cam.modify_backprop\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_compile_saliency_function\", \"source\": \"grad_cam.__main__\", \"target\": \"grad_cam.compile_saliency_function\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_calculate_guided_gradient_CAM\", \"source\": \"grad_cam.__main__\", \"target\": \"grad_cam.calculate_guided_gradient_CAM\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_main_cv2.imwrite\", \"source\": \"grad_cam.__main__\", \"target\": \"cv2.imwrite\", \"type\": \"call\"}\n",
      " ]}\n",
      "Reasoning: {'id': 'rs_6829fb1b35388191bbb96bb140d15197017be3a1bdce4c0c', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: inference.py ===\n",
      "   1: import cv2\n",
      "   2: import matplotlib.pyplot as plt\n",
      "   3: import numpy as np\n",
      "   4: from keras.preprocessing import image\n",
      "   5: \n",
      "   6: def load_image(image_path, grayscale=False, target_size=None):\n",
      "   7:     pil_image = image.load_img(image_path, grayscale, target_size)\n",
      "   8:     return image.img_to_array(pil_image)\n",
      "   9: \n",
      "  10: def load_detection_model(model_path):\n",
      "  11:     detection_model = cv2.CascadeClassifier(model_path)\n",
      "  12:     return detection_model\n",
      "  13: \n",
      "  14: def detect_faces(detection_model, gray_image_array):\n",
      "  15:     return detection_model.detectMultiScale(gray_image_array, 1.3, 5)\n",
      "  16: \n",
      "  17: def draw_bounding_box(face_coordinates, image_array, color):\n",
      "  18:     x, y, w, h = face_coordinates\n",
      "  19:     cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n",
      "  20: \n",
      "  21: def apply_offsets(face_coordinates, offsets):\n",
      "  22:     x, y, width, height = face_coordinates\n",
      "  23:     x_off, y_off = offsets\n",
      "  24:     return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n",
      "  25: \n",
      "  26: def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,\n",
      "  27:                                                 font_scale=2, thickness=2):\n",
      "  28:     x, y = coordinates[:2]\n",
      "  29:     cv2.putText(image_array, text, (x + x_offset, y + y_offset),\n",
      "  30:                 cv2.FONT_HERSHEY_SIMPLEX,\n",
      "  31:                 font_scale, color, thickness, cv2.LINE_AA)\n",
      "  32: \n",
      "  33: def get_colors(num_classes):\n",
      "  34:     colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n",
      "  35:     colors = np.asarray(colors) * 255\n",
      "  36:     return colors\n",
      "  37: \n",
      "=== END FILE: inference.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/inference.py: {\"nodes\":[{\"id\":\"inference.load_image\",\"file\":\"inference.py\",\"line_start\":6,\"line_end\":8,\"label\":\"load_image\",\"description\":\"Loads an image from disk and converts it to a numerical array.\"},{\"id\":\"inference.load_detection_model\",\"file\":\"inference.py\",\"line_start\":10,\"line_end\":12,\"label\":\"load_detection_model\",\"description\":\"Loads an OpenCV CascadeClassifier from the given model path.\"},{\"id\":\"inference.detect_faces\",\"file\":\"inference.py\",\"line_start\":14,\"line_end\":15,\"label\":\"detect_faces\",\"description\":\"Uses a CascadeClassifier to detect faces in a grayscale image array.\"},{\"id\":\"inference.draw_bounding_box\",\"file\":\"inference.py\",\"line_start\":17,\"line_end\":20,\"label\":\"draw_bounding_box\",\"description\":\"Draws a rectangle on an image array around detected face coordinates.\"},{\"id\":\"inference.apply_offsets\",\"file\":\"inference.py\",\"line_start\":21,\"line_end\":24,\"label\":\"apply_offsets\",\"description\":\"Applies horizontal and vertical offsets to face coordinates.\"},{\"id\":\"inference.draw_text\",\"file\":\"inference.py\",\"line_start\":26,\"line_end\":31,\"label\":\"draw_text\",\"description\":\"Renders text onto an image array at specified coordinates.\"},{\"id\":\"inference.get_colors\",\"file\":\"inference.py\",\"line_start\":33,\"line_end\":36,\"label\":\"get_colors\",\"description\":\"Generates distinct colors for each class using an HSV colormap.\"}],\"edges\":[{\"id\":\"e_load_image_keras.preprocessing.image.load_img\",\"source\":\"inference.load_image\",\"target\":\"keras.preprocessing.image.load_img\",\"type\":\"call\"},{\"id\":\"e_load_image_keras.preprocessing.image.img_to_array\",\"source\":\"inference.load_image\",\"target\":\"keras.preprocessing.image.img_to_array\",\"type\":\"call\"},{\"id\":\"e_load_detection_model_cv2.CascadeClassifier\",\"source\":\"inference.load_detection_model\",\"target\":\"cv2.CascadeClassifier\",\"type\":\"call\"},{\"id\":\"e_detect_faces_cv2.CascadeClassifier.detectMultiScale\",\"source\":\"inference.detect_faces\",\"target\":\"cv2.CascadeClassifier.detectMultiScale\",\"type\":\"call\"},{\"id\":\"e_draw_bounding_box_cv2.rectangle\",\"source\":\"inference.draw_bounding_box\",\"target\":\"cv2.rectangle\",\"type\":\"call\"},{\"id\":\"e_draw_text_cv2.putText\",\"source\":\"inference.draw_text\",\"target\":\"cv2.putText\",\"type\":\"call\"},{\"id\":\"e_get_colors_matplotlib.pyplot.cm.hsv\",\"source\":\"inference.get_colors\",\"target\":\"matplotlib.pyplot.cm.hsv\",\"type\":\"call\"},{\"id\":\"e_get_colors_numpy.linspace\",\"source\":\"inference.get_colors\",\"target\":\"numpy.linspace\",\"type\":\"call\"},{\"id\":\"e_get_colors_numpy.asarray\",\"source\":\"inference.get_colors\",\"target\":\"numpy.asarray\",\"type\":\"call\"}]}\n",
      "Reasoning: {'id': 'rs_6829fb3459088191a9981bcdb0f3f9ec0c3bf0e4672dc02c', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: data_augmentation.py ===\n",
      "   1: import numpy as np\n",
      "   2: from random import shuffle\n",
      "   3: from .preprocessor import preprocess_input\n",
      "   4: from .preprocessor import _imread as imread\n",
      "   5: from .preprocessor import _imresize as imresize\n",
      "   6: from .preprocessor import to_categorical\n",
      "   7: import scipy.ndimage as ndi\n",
      "   8: import cv2\n",
      "   9: \n",
      "  10: \n",
      "  11: class ImageGenerator(object):\n",
      "  12:     \"\"\" Image generator with saturation, brightness, lighting, contrast,\n",
      "  13:     horizontal flip and vertical flip transformations. It supports\n",
      "  14:     bounding boxes coordinates.\n",
      "  15: \n",
      "  16:     TODO:\n",
      "  17:         - Finish support for not using bounding_boxes\n",
      "  18:             - Random crop\n",
      "  19:             - Test other transformations\n",
      "  20:     \"\"\"\n",
      "  21:     def __init__(self, ground_truth_data, batch_size, image_size,\n",
      "  22:                  train_keys, validation_keys,\n",
      "  23:                  ground_truth_transformer=None,\n",
      "  24:                  path_prefix=None,\n",
      "  25:                  saturation_var=0.5,\n",
      "  26:                  brightness_var=0.5,\n",
      "  27:                  contrast_var=0.5,\n",
      "  28:                  lighting_std=0.5,\n",
      "  29:                  horizontal_flip_probability=0.5,\n",
      "  30:                  vertical_flip_probability=0.5,\n",
      "  31:                  do_random_crop=False,\n",
      "  32:                  grayscale=False,\n",
      "  33:                  zoom_range=[0.75, 1.25],\n",
      "  34:                  translation_factor=.3):\n",
      "  35: \n",
      "  36:         self.ground_truth_data = ground_truth_data\n",
      "  37:         self.ground_truth_transformer = ground_truth_transformer\n",
      "  38:         self.batch_size = batch_size\n",
      "  39:         self.path_prefix = path_prefix\n",
      "  40:         self.train_keys = train_keys\n",
      "  41:         self.validation_keys = validation_keys\n",
      "  42:         self.image_size = image_size\n",
      "  43:         self.grayscale = grayscale\n",
      "  44:         self.color_jitter = []\n",
      "  45:         if saturation_var:\n",
      "  46:             self.saturation_var = saturation_var\n",
      "  47:             self.color_jitter.append(self.saturation)\n",
      "  48:         if brightness_var:\n",
      "  49:             self.brightness_var = brightness_var\n",
      "  50:             self.color_jitter.append(self.brightness)\n",
      "  51:         if contrast_var:\n",
      "  52:             self.contrast_var = contrast_var\n",
      "  53:             self.color_jitter.append(self.contrast)\n",
      "  54:         self.lighting_std = lighting_std\n",
      "  55:         self.horizontal_flip_probability = horizontal_flip_probability\n",
      "  56:         self.vertical_flip_probability = vertical_flip_probability\n",
      "  57:         self.do_random_crop = do_random_crop\n",
      "  58:         self.zoom_range = zoom_range\n",
      "  59:         self.translation_factor = translation_factor\n",
      "  60: \n",
      "  61:     def _do_random_crop(self, image_array):\n",
      "  62:         \"\"\"IMPORTANT: random crop only works for classification since the\n",
      "  63:         current implementation does no transform bounding boxes\"\"\"\n",
      "  64:         height = image_array.shape[0]\n",
      "  65:         width = image_array.shape[1]\n",
      "  66:         x_offset = np.random.uniform(0, self.translation_factor * width)\n",
      "  67:         y_offset = np.random.uniform(0, self.translation_factor * height)\n",
      "  68:         offset = np.array([x_offset, y_offset])\n",
      "  69:         scale_factor = np.random.uniform(self.zoom_range[0],\n",
      "  70:                                          self.zoom_range[1])\n",
      "  71:         crop_matrix = np.array([[scale_factor, 0],\n",
      "  72:                                 [0, scale_factor]])\n",
      "  73: \n",
      "  74:         image_array = np.rollaxis(image_array, axis=-1, start=0)\n",
      "  75:         image_channel = [ndi.interpolation.affine_transform(image_channel,\n",
      "  76:                          crop_matrix, offset=offset, order=0, mode='nearest',\n",
      "  77:                          cval=0.0) for image_channel in image_array]\n",
      "  78: \n",
      "  79:         image_array = np.stack(image_channel, axis=0)\n",
      "  80:         image_array = np.rollaxis(image_array, 0, 3)\n",
      "  81:         return image_array\n",
      "  82: \n",
      "  83:     def do_random_rotation(self, image_array):\n",
      "  84:         \"\"\"IMPORTANT: random rotation only works for classification since the\n",
      "  85:         current implementation does no transform bounding boxes\"\"\"\n",
      "  86:         height = image_array.shape[0]\n",
      "  87:         width = image_array.shape[1]\n",
      "  88:         x_offset = np.random.uniform(0, self.translation_factor * width)\n",
      "  89:         y_offset = np.random.uniform(0, self.translation_factor * height)\n",
      "  90:         offset = np.array([x_offset, y_offset])\n",
      "  91:         scale_factor = np.random.uniform(self.zoom_range[0],\n",
      "  92:                                          self.zoom_range[1])\n",
      "  93:         crop_matrix = np.array([[scale_factor, 0],\n",
      "  94:                                 [0, scale_factor]])\n",
      "  95: \n",
      "  96:         image_array = np.rollaxis(image_array, axis=-1, start=0)\n",
      "  97:         image_channel = [ndi.interpolation.affine_transform(image_channel,\n",
      "  98:                          crop_matrix, offset=offset, order=0, mode='nearest',\n",
      "  99:                          cval=0.0) for image_channel in image_array]\n",
      " 100: \n",
      " 101:         image_array = np.stack(image_channel, axis=0)\n",
      " 102:         image_array = np.rollaxis(image_array, 0, 3)\n",
      " 103:         return image_array\n",
      " 104: \n",
      " 105:     def _gray_scale(self, image_array):\n",
      " 106:         return image_array.dot([0.299, 0.587, 0.114])\n",
      " 107: \n",
      " 108:     def saturation(self, image_array):\n",
      " 109:         gray_scale = self._gray_scale(image_array)\n",
      " 110:         alpha = 2.0 * np.random.random() * self.brightness_var\n",
      " 111:         alpha = alpha + 1 - self.saturation_var\n",
      " 112:         image_array = (alpha * image_array + (1 - alpha) *\n",
      " 113:                        gray_scale[:, :, None])\n",
      " 114:         return np.clip(image_array, 0, 255)\n",
      " 115: \n",
      " 116:     def brightness(self, image_array):\n",
      " 117:         alpha = 2 * np.random.random() * self.brightness_var\n",
      " 118:         alpha = alpha + 1 - self.saturation_var\n",
      " 119:         image_array = alpha * image_array\n",
      " 120:         return np.clip(image_array, 0, 255)\n",
      " 121: \n",
      " 122:     def contrast(self, image_array):\n",
      " 123:         gray_scale = (self._gray_scale(image_array).mean() *\n",
      " 124:                       np.ones_like(image_array))\n",
      " 125:         alpha = 2 * np.random.random() * self.contrast_var\n",
      " 126:         alpha = alpha + 1 - self.contrast_var\n",
      " 127:         image_array = image_array * alpha + (1 - alpha) * gray_scale\n",
      " 128:         return np.clip(image_array, 0, 255)\n",
      " 129: \n",
      " 130:     def lighting(self, image_array):\n",
      " 131:         covariance_matrix = np.cov(image_array.reshape(-1, 3) /\n",
      " 132:                                    255.0, rowvar=False)\n",
      " 133:         eigen_values, eigen_vectors = np.linalg.eigh(covariance_matrix)\n",
      " 134:         noise = np.random.randn(3) * self.lighting_std\n",
      " 135:         noise = eigen_vectors.dot(eigen_values * noise) * 255\n",
      " 136:         image_array = image_array + noise\n",
      " 137:         return np.clip(image_array, 0, 255)\n",
      " 138: \n",
      " 139:     def horizontal_flip(self, image_array, box_corners=None):\n",
      " 140:         if np.random.random() < self.horizontal_flip_probability:\n",
      " 141:             image_array = image_array[:, ::-1]\n",
      " 142:             if box_corners is not None:\n",
      " 143:                 box_corners[:, [0, 2]] = 1 - box_corners[:, [2, 0]]\n",
      " 144:         return image_array, box_corners\n",
      " 145: \n",
      " 146:     def vertical_flip(self, image_array, box_corners=None):\n",
      " 147:         if (np.random.random() < self.vertical_flip_probability):\n",
      " 148:             image_array = image_array[::-1]\n",
      " 149:             if box_corners is not None:\n",
      " 150:                 box_corners[:, [1, 3]] = 1 - box_corners[:, [3, 1]]\n",
      " 151:         return image_array, box_corners\n",
      " 152: \n",
      " 153:     def transform(self, image_array, box_corners=None):\n",
      " 154:         shuffle(self.color_jitter)\n",
      " 155:         for jitter in self.color_jitter:\n",
      " 156:             image_array = jitter(image_array)\n",
      " 157: \n",
      " 158:         if self.lighting_std:\n",
      " 159:             image_array = self.lighting(image_array)\n",
      " 160: \n",
      " 161:         if self.horizontal_flip_probability > 0:\n",
      " 162:             image_array, box_corners = self.horizontal_flip(image_array,\n",
      " 163:                                                             box_corners)\n",
      " 164: \n",
      " 165:         if self.vertical_flip_probability > 0:\n",
      " 166:             image_array, box_corners = self.vertical_flip(image_array,\n",
      " 167:                                                           box_corners)\n",
      " 168:         return image_array, box_corners\n",
      " 169: \n",
      " 170:     def preprocess_images(self, image_array):\n",
      " 171:         return preprocess_input(image_array)\n",
      " 172: \n",
      " 173:     def flow(self, mode='train'):\n",
      " 174:             while True:\n",
      " 175:                 if mode == 'train':\n",
      " 176:                     shuffle(self.train_keys)\n",
      " 177:                     keys = self.train_keys\n",
      " 178:                 elif mode == 'val' or mode == 'demo':\n",
      " 179:                     shuffle(self.validation_keys)\n",
      " 180:                     keys = self.validation_keys\n",
      " 181:                 else:\n",
      " 182:                     raise Exception('invalid mode: %s' % mode)\n",
      " 183: \n",
      " 184:                 inputs = []\n",
      " 185:                 targets = []\n",
      " 186:                 for key in keys:\n",
      " 187:                     image_path = self.path_prefix + key\n",
      " 188:                     image_array = imread(image_path)\n",
      " 189:                     image_array = imresize(image_array, self.image_size)\n",
      " 190: \n",
      " 191:                     num_image_channels = len(image_array.shape)\n",
      " 192:                     if num_image_channels != 3:\n",
      " 193:                         continue\n",
      " 194: \n",
      " 195:                     ground_truth = self.ground_truth_data[key]\n",
      " 196: \n",
      " 197:                     if self.do_random_crop:\n",
      " 198:                         image_array = self._do_random_crop(image_array)\n",
      " 199: \n",
      " 200:                     image_array = image_array.astype('float32')\n",
      " 201:                     if mode == 'train' or mode == 'demo':\n",
      " 202:                         if self.ground_truth_transformer is not None:\n",
      " 203:                             image_array, ground_truth = self.transform(\n",
      " 204:                                                                 image_array,\n",
      " 205:                                                                 ground_truth)\n",
      " 206:                             ground_truth = (\n",
      " 207:                                 self.ground_truth_transformer.assign_boxes(\n",
      " 208:                                                             ground_truth))\n",
      " 209:                         else:\n",
      " 210:                             image_array = self.transform(image_array)[0]\n",
      " 211: \n",
      " 212:                     if self.grayscale:\n",
      " 213:                         image_array = cv2.cvtColor(\n",
      " 214:                                 image_array.astype('uint8'),\n",
      " 215:                                 cv2.COLOR_RGB2GRAY).astype('float32')\n",
      " 216:                         image_array = np.expand_dims(image_array, -1)\n",
      " 217: \n",
      " 218:                     inputs.append(image_array)\n",
      " 219:                     targets.append(ground_truth)\n",
      " 220:                     if len(targets) == self.batch_size:\n",
      " 221:                         inputs = np.asarray(inputs)\n",
      " 222:                         targets = np.asarray(targets)\n",
      " 223:                         # this will not work for boxes\n",
      " 224:                         targets = to_categorical(targets)\n",
      " 225:                         if mode == 'train' or mode == 'val':\n",
      " 226:                             inputs = self.preprocess_images(inputs)\n",
      " 227:                             yield self._wrap_in_dictionary(inputs, targets)\n",
      " 228:                         if mode == 'demo':\n",
      " 229:                             yield self._wrap_in_dictionary(inputs, targets)\n",
      " 230:                         inputs = []\n",
      " 231:                         targets = []\n",
      " 232: \n",
      " 233:     def _wrap_in_dictionary(self, image_array, targets):\n",
      " 234:         return [{'input_1': image_array},\n",
      " 235:                 {'predictions': targets}]\n",
      "=== END FILE: data_augmentation.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/data_augmentation.py: {\"nodes\": [\n",
      "  {\"id\": \"data_augmentation.ImageGenerator\", \"file\": \"data_augmentation.py\", \"line_start\": 11, \"line_end\": 235, \"label\": \"ImageGenerator\", \"description\": \"Class providing image augmentation: color jitter, flips, lighting, cropping, rotation, flow control.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.__init__\", \"file\": \"data_augmentation.py\", \"line_start\": 21, \"line_end\": 60, \"label\": \"__init__\", \"description\": \"Initialize augmentation parameters and collect active color jitter methods.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator._do_random_crop\", \"file\": \"data_augmentation.py\", \"line_start\": 61, \"line_end\": 81, \"label\": \"_do_random_crop\", \"description\": \"Apply a random scale and translation crop to the image (classification only).\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.do_random_rotation\", \"file\": \"data_augmentation.py\", \"line_start\": 83, \"line_end\": 103, \"label\": \"do_random_rotation\", \"description\": \"Apply random rotation (scaling + translation) to the image (classification only).\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator._gray_scale\", \"file\": \"data_augmentation.py\", \"line_start\": 105, \"line_end\": 106, \"label\": \"_gray_scale\", \"description\": \"Compute per-pixel grayscale luminance.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.saturation\", \"file\": \"data_augmentation.py\", \"line_start\": 108, \"line_end\": 114, \"label\": \"saturation\", \"description\": \"Adjust image saturation by blending with grayscale.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.brightness\", \"file\": \"data_augmentation.py\", \"line_start\": 116, \"line_end\": 120, \"label\": \"brightness\", \"description\": \"Scale image brightness.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.contrast\", \"file\": \"data_augmentation.py\", \"line_start\": 122, \"line_end\": 128, \"label\": \"contrast\", \"description\": \"Adjust image contrast relative to mean grayscale.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.lighting\", \"file\": \"data_augmentation.py\", \"line_start\": 130, \"line_end\": 137, \"label\": \"lighting\", \"description\": \"Add PCA-based lighting noise.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.horizontal_flip\", \"file\": \"data_augmentation.py\", \"line_start\": 139, \"line_end\": 144, \"label\": \"horizontal_flip\", \"description\": \"Randomly flip image horizontally and adjust box corners.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.vertical_flip\", \"file\": \"data_augmentation.py\", \"line_start\": 146, \"line_end\": 151, \"label\": \"vertical_flip\", \"description\": \"Randomly flip image vertically and adjust box corners.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.transform\", \"file\": \"data_augmentation.py\", \"line_start\": 153, \"line_end\": 168, \"label\": \"transform\", \"description\": \"Apply color jitters, lighting, and flips to image (and boxes).\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.preprocess_images\", \"file\": \"data_augmentation.py\", \"line_start\": 170, \"line_end\": 172, \"label\": \"preprocess_images\", \"description\": \"Wrap call to external preprocess_input.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator.flow\", \"file\": \"data_augmentation.py\", \"line_start\": 173, \"line_end\": 232, \"label\": \"flow\", \"description\": \"Main generator loop: read, optionally crop, augment, batch and yield.\"},\n",
      "  {\"id\": \"data_augmentation.ImageGenerator._wrap_in_dictionary\", \"file\": \"data_augmentation.py\", \"line_start\": 233, \"line_end\": 235, \"label\": \"_wrap_in_dictionary\", \"description\": \"Package inputs and targets into dictionaries for model consumption.\"}\n",
      "],\n",
      " \"edges\": [\n",
      "  {\"id\": \"e_transform__do_random_crop\", \"source\": \"data_augmentation.ImageGenerator.flow\", \"target\": \"data_augmentation.ImageGenerator._do_random_crop\", \"type\": \"call\", \"description\": \"Apply random crop if enabled.\"},\n",
      "  {\"id\": \"e_flow_imread\", \"source\": \"data_augmentation.ImageGenerator.flow\", \"target\": \"preprocessor._imread\", \"type\": \"call\", \"description\": \"Load image from disk.\"},\n",
      "  {\"id\": \"e_flow_imresize\", \"source\": \"data_augmentation.ImageGenerator.flow\", \"target\": \"preprocessor._imresize\", \"type\": \"call\", \"description\": \"Resize image to target size.\"},\n",
      "  {\"id\": \"e_flow_transform\", \"source\": \"data_augmentation.ImageGenerator.flow\", \"target\": \"data_augmentation.ImageGenerator.transform\", \"type\": \"call\", \"description\": \"Apply augmentations during training/demo.\"},\n",
      "  {\"id\": \"e_flow_preprocess_images\", \"source\": \"data_augmentation.ImageGenerator.flow\", \"target\": \"data_augmentation.ImageGenerator.preprocess_images\", \"type\": \"call\", \"description\": \"Final preprocessing before yield.\"},\n",
      "  {\"id\": \"e_flow_to_categorical\", \"source\": \"data_augmentation.ImageGenerator.flow\", \"target\": \"preprocessor.to_categorical\", \"type\": \"call\", \"description\": \"Convert labels to one-hot encoding.\"},\n",
      "  {\"id\": \"e_flow_wrap\", \"source\": \"data_augmentation.ImageGenerator.flow\", \"target\": \"data_augmentation.ImageGenerator._wrap_in_dictionary\", \"type\": \"call\", \"description\": \"Package batch for output.\"},\n",
      "  {\"id\": \"e_transform_saturation\", \"source\": \"data_augmentation.ImageGenerator.transform\", \"target\": \"data_augmentation.ImageGenerator.saturation\", \"type\": \"call\", \"description\": \"Randomly adjust saturation.\"},\n",
      "  {\"id\": \"e_transform_brightness\", \"source\": \"data_augmentation.ImageGenerator.transform\", \"target\": \"data_augmentation.ImageGenerator.brightness\", \"type\": \"call\", \"description\": \"Randomly adjust brightness.\"},\n",
      "  {\"id\": \"e_transform_contrast\", \"source\": \"data_augmentation.ImageGenerator.transform\", \"target\": \"data_augmentation.ImageGenerator.contrast\", \"type\": \"call\", \"description\": \"Randomly adjust contrast.\"},\n",
      "  {\"id\": \"e_transform_lighting\", \"source\": \"data_augmentation.ImageGenerator.transform\", \"target\": \"data_augmentation.ImageGenerator.lighting\", \"type\": \"call\", \"description\": \"Add lighting noise.\"},\n",
      "  {\"id\": \"e_transform_hflip\", \"source\": \"data_augmentation.ImageGenerator.transform\", \"target\": \"data_augmentation.ImageGenerator.horizontal_flip\", \"type\": \"call\", \"description\": \"Random horizontal flip.\"},\n",
      "  {\"id\": \"e_transform_vflip\", \"source\": \"data_augmentation.ImageGenerator.transform\", \"target\": \"data_augmentation.ImageGenerator.vertical_flip\", \"type\": \"call\", \"description\": \"Random vertical flip.\"}\n",
      " ]\n",
      "}\n",
      "Reasoning: {'id': 'rs_6829fb437b948191ae6e28394284f9ed0f40fb190987caf7', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: visualizer.py ===\n",
      "   1: import numpy as np\n",
      "   2: import matplotlib.cm as cm\n",
      "   3: from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
      "   4: import matplotlib.pyplot as plt\n",
      "   5: import numpy.ma as ma\n",
      "   6: \n",
      "   7: \n",
      "   8: def make_mosaic(images, num_rows, num_cols, border=1, class_names=None):\n",
      "   9:     num_images = len(images)\n",
      "  10:     image_shape = images.shape[1:]\n",
      "  11:     mosaic = ma.masked_all(\n",
      "  12:             (num_rows * image_shape[0] + (num_rows - 1) * border,\n",
      "  13:              num_cols * image_shape[1] + (num_cols - 1) * border),\n",
      "  14:             dtype=np.float32)\n",
      "  15:     paddedh = image_shape[0] + border\n",
      "  16:     paddedw = image_shape[1] + border\n",
      "  17:     for image_arg in range(num_images):\n",
      "  18:         row = int(np.floor(image_arg / num_cols))\n",
      "  19:         col = image_arg % num_cols\n",
      "  20:         image = np.squeeze(images[image_arg])\n",
      "  21:         image_shape = image.shape\n",
      "  22:         mosaic[row * paddedh:row * paddedh + image_shape[0],\n",
      "  23:                col * paddedw:col * paddedw + image_shape[1]] = image\n",
      "  24:     return mosaic\n",
      "  25: \n",
      "  26: \n",
      "  27: def make_mosaic_v2(images, num_mosaic_rows=None,\n",
      "  28:                    num_mosaic_cols=None, border=1):\n",
      "  29:     images = np.squeeze(images)\n",
      "  30:     num_images, image_pixels_rows, image_pixels_cols = images.shape\n",
      "  31:     if num_mosaic_rows is None and num_mosaic_cols is None:\n",
      "  32:         box_size = int(np.ceil(np.sqrt(num_images)))\n",
      "  33:         num_mosaic_rows = num_mosaic_cols = box_size\n",
      "  34:     num_mosaic_pixel_rows = num_mosaic_rows * (image_pixels_rows + border)\n",
      "  35:     num_mosaic_pixel_cols = num_mosaic_cols * (image_pixels_cols + border)\n",
      "  36:     mosaic = np.empty(shape=(num_mosaic_pixel_rows, num_mosaic_pixel_cols))\n",
      "  37:     mosaic_col_arg = 0\n",
      "  38:     mosaic_row_arg = 0\n",
      "  39:     for image_arg in range(num_images):\n",
      "  40:         if image_arg % num_mosaic_cols == 0 and image_arg != 0:\n",
      "  41:             mosaic_col_arg = mosaic_col_arg + 1\n",
      "  42:             mosaic_row_arg = 0\n",
      "  43:         x0 = image_pixels_cols * (mosaic_row_arg)\n",
      "  44:         x1 = image_pixels_cols * (mosaic_row_arg + 1)\n",
      "  45:         y0 = image_pixels_rows * (mosaic_col_arg)\n",
      "  46:         y1 = image_pixels_rows * (mosaic_col_arg + 1)\n",
      "  47:         image = images[image_arg]\n",
      "  48:         mosaic[y0:y1, x0:x1] = image\n",
      "  49:         mosaic_row_arg = mosaic_row_arg + 1\n",
      "  50:     return mosaic\n",
      "  51: \n",
      "  52: \n",
      "  53: def pretty_imshow(axis, data, vmin=None, vmax=None, cmap=None):\n",
      "  54:     if cmap is None:\n",
      "  55:         cmap = cm.jet\n",
      "  56:     if vmin is None:\n",
      "  57:         vmin = data.min()\n",
      "  58:     if vmax is None:\n",
      "  59:         vmax = data.max()\n",
      "  60:     cax = None\n",
      "  61:     divider = make_axes_locatable(axis)\n",
      "  62:     cax = divider.append_axes('right', size='5%', pad=0.05)\n",
      "  63:     image = axis.imshow(data, vmin=vmin, vmax=vmax,\n",
      "  64:                         interpolation='nearest', cmap=cmap)\n",
      "  65:     plt.colorbar(image, cax=cax)\n",
      "  66: \n",
      "  67: \n",
      "  68: def normal_imshow(axis, data, vmin=None, vmax=None,\n",
      "  69:                   cmap=None, axis_off=True):\n",
      "  70:     if cmap is None:\n",
      "  71:         cmap = cm.jet\n",
      "  72:     if vmin is None:\n",
      "  73:         vmin = data.min()\n",
      "  74:     if vmax is None:\n",
      "  75:         vmax = data.max()\n",
      "  76:     image = axis.imshow(data, vmin=vmin, vmax=vmax,\n",
      "  77:                         interpolation='nearest', cmap=cmap)\n",
      "  78:     if axis_off:\n",
      "  79:         plt.axis('off')\n",
      "  80:     return image\n",
      "  81: \n",
      "  82: \n",
      "  83: def display_image(face, class_vector=None,\n",
      "  84:                   class_decoder=None, pretty=False):\n",
      "  85:     if class_vector is not None and class_decoder is None:\n",
      "  86:         raise Exception('Provide class decoder')\n",
      "  87:     face = np.squeeze(face)\n",
      "  88:     color_map = None\n",
      "  89:     if len(face.shape) < 3:\n",
      "  90:         color_map = 'gray'\n",
      "  91:     plt.figure()\n",
      "  92:     if class_vector is not None:\n",
      "  93:         class_arg = np.argmax(class_vector)\n",
      "  94:         class_name = class_decoder[class_arg]\n",
      "  95:         plt.title(class_name)\n",
      "  96:     if pretty:\n",
      "  97:         pretty_imshow(plt.gca(), face, cmap=color_map)\n",
      "  98:     else:\n",
      "  99:         plt.imshow(face, color_map)\n",
      " 100: \n",
      " 101: \n",
      " 102: def draw_mosaic(data, num_rows, num_cols, class_vectors=None,\n",
      " 103:                 class_decoder=None, cmap='gray'):\n",
      " 104: \n",
      " 105:     if class_vectors is not None and class_decoder is None:\n",
      " 106:         raise Exception('Provide class decoder')\n",
      " 107: \n",
      " 108:     figure, axis_array = plt.subplots(num_rows, num_cols)\n",
      " 109:     figure.set_size_inches(8, 8, forward=True)\n",
      " 110:     titles = []\n",
      " 111:     if class_vectors is not None:\n",
      " 112:         for vector_arg in range(len(class_vectors)):\n",
      " 113:             class_arg = np.argmax(class_vectors[vector_arg])\n",
      " 114:             class_name = class_decoder[class_arg]\n",
      " 115:             titles.append(class_name)\n",
      " 116: \n",
      " 117:     image_arg = 0\n",
      " 118:     for row_arg in range(num_rows):\n",
      " 119:         for col_arg in range(num_cols):\n",
      " 120:             image = data[image_arg]\n",
      " 121:             image = np.squeeze(image)\n",
      " 122:             axis_array[row_arg, col_arg].axis('off')\n",
      " 123:             axis_array[row_arg, col_arg].imshow(image, cmap=cmap)\n",
      " 124:             axis_array[row_arg, col_arg].set_title(titles[image_arg])\n",
      " 125:             image_arg = image_arg + 1\n",
      " 126:     plt.tight_layout()\n",
      " 127: \n",
      " 128: \n",
      " 129: if __name__ == '__main__':\n",
      " 130:     # from utils.data_manager import DataManager\n",
      " 131:     from utils.utils import get_labels\n",
      " 132:     from keras.models import load_model\n",
      " 133:     import pickle\n",
      " 134: \n",
      " 135:     # dataset_name = 'fer2013'\n",
      " 136:     # model_path = '../trained_models/emotion_models/simple_CNN.985-0.66.hdf5'\n",
      " 137:     dataset_name = 'fer2013'\n",
      " 138:     class_decoder = get_labels(dataset_name)\n",
      " 139:     # data_manager = DataManager(dataset_name)\n",
      " 140:     # faces, emotions = data_manager.get_data()\n",
      " 141:     faces = pickle.load(open('faces.pkl', 'rb'))\n",
      " 142:     emotions = pickle.load(open('emotions.pkl', 'rb'))\n",
      " 143:     pretty_imshow(plt.gca(), make_mosaic(faces[:4], 2, 2), cmap='gray')\n",
      " 144:     plt.show()\n",
      " 145: \n",
      " 146:     \"\"\"\n",
      " 147:     image_arg = 0\n",
      " 148:     face = faces[image_arg:image_arg + 1]\n",
      " 149:     emotion = emotions[image_arg:image_arg + 1]\n",
      " 150:     display_image(face, emotion, class_decoder)\n",
      " 151:     plt.show()\n",
      " 152: \n",
      " 153:     normal_imshow(plt.gca(), make_mosaic(faces[:4], 3, 3), cmap='gray')\n",
      " 154:     plt.show()\n",
      " 155: \n",
      " 156:     draw_mosaic(faces, 2, 2, emotions, class_decoder)\n",
      " 157:     plt.show()\n",
      " 158: \n",
      " 159:     \"\"\"\n",
      " 160:     model = load_model('../trained_models/emotion_models/simple_CNN.985-0.66.hdf5')\n",
      " 161:     conv1_weights = model.layers[2].get_weights()\n",
      " 162:     kernel_conv1_weights = conv1_weights[0]\n",
      " 163:     kernel_conv1_weights = np.squeeze(kernel_conv1_weights)\n",
      " 164:     kernel_conv1_weights = np.rollaxis(kernel_conv1_weights, 2, 0)\n",
      " 165:     kernel_conv1_weights = np.expand_dims(kernel_conv1_weights, -1)\n",
      " 166:     num_kernels = kernel_conv1_weights.shape[0]\n",
      " 167:     box_size = int(np.ceil(np.sqrt(num_kernels)))\n",
      " 168:     print('Box size:', box_size)\n",
      " 169: \n",
      " 170:     print('Kernel shape', kernel_conv1_weights.shape)\n",
      " 171:     plt.figure(figsize=(15, 15))\n",
      " 172:     plt.title('conv1 weights')\n",
      " 173:     pretty_imshow(\n",
      " 174:             plt.gca(),\n",
      " 175:             make_mosaic(kernel_conv1_weights, box_size, box_size),\n",
      " 176:             cmap=cm.binary)\n",
      " 177:     plt.show()\n",
      "=== END FILE: visualizer.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/visualizer.py: {\"nodes\": [\n",
      "  {\"id\": \"visualizer.make_mosaic\", \"file\": \"visualizer.py\", \"line_start\": 8, \"line_end\": 24, \"label\": \"make_mosaic\", \"description\": \"Arrange a set of images into a mosaic with optional border.\"},\n",
      "  {\"id\": \"visualizer.make_mosaic_v2\", \"file\": \"visualizer.py\", \"line_start\": 27, \"line_end\": 50, \"label\": \"make_mosaic_v2\", \"description\": \"Alternate implementation to assemble images into a grid mosaic.\"},\n",
      "  {\"id\": \"visualizer.pretty_imshow\", \"file\": \"visualizer.py\", \"line_start\": 53, \"line_end\": 66, \"label\": \"pretty_imshow\", \"description\": \"Display an image with a colorbar alongside using a specified colormap.\"},\n",
      "  {\"id\": \"visualizer.normal_imshow\", \"file\": \"visualizer.py\", \"line_start\": 68, \"line_end\": 80, \"label\": \"normal_imshow\", \"description\": \"Display an image normally, optionally turning off axes.\"},\n",
      "  {\"id\": \"visualizer.display_image\", \"file\": \"visualizer.py\", \"line_start\": 83, \"line_end\": 100, \"label\": \"display_image\", \"description\": \"Show a single image with optional class title and pretty or normal display.\"},\n",
      "  {\"id\": \"visualizer.draw_mosaic\", \"file\": \"visualizer.py\", \"line_start\": 102, \"line_end\": 127, \"label\": \"draw_mosaic\", \"description\": \"Draw multiple images in a subplot grid with optional class titles.\"},\n",
      "  {\"id\": \"visualizer.__main__\", \"file\": \"visualizer.py\", \"line_start\": 129, \"line_end\": 177, \"label\": \"__main__\", \"description\": \"Script entry point: load data, create mosaics, visualize kernels of a trained model.\"}\n",
      "],\n",
      "\"edges\": [\n",
      "  {\"id\": \"e___main___make_mosaic\", \"source\": \"visualizer.__main__\", \"target\": \"visualizer.make_mosaic\", \"type\": \"call\"},\n",
      "  {\"id\": \"e___main___pretty_imshow\", \"source\": \"visualizer.__main__\", \"target\": \"visualizer.pretty_imshow\", \"type\": \"call\"},\n",
      "  {\"id\": \"e___main___get_labels\", \"source\": \"visualizer.__main__\", \"target\": \"utils.utils.get_labels\", \"type\": \"call\"},\n",
      "  {\"id\": \"e___main___load_model\", \"source\": \"visualizer.__main__\", \"target\": \"keras.models.load_model\", \"type\": \"call\"},\n",
      "  {\"id\": \"e_display_image_pretty_imshow\", \"source\": \"visualizer.display_image\", \"target\": \"visualizer.pretty_imshow\", \"type\": \"call\"}\n",
      "]}\n",
      "Reasoning: {'id': 'rs_6829fb5a490881918fb1f5db9e0dd0d001902198b6c39cdb', 'summary': [], 'type': 'reasoning'}\n",
      "=== FILE: cnn.py ===\n",
      "   1: from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n",
      "   2: from keras.layers import AveragePooling2D, BatchNormalization\n",
      "   3: from keras.layers import GlobalAveragePooling2D\n",
      "   4: from keras.models import Sequential\n",
      "   5: from keras.layers import Flatten\n",
      "   6: from keras.models import Model\n",
      "   7: from keras.layers import Input\n",
      "   8: from keras.layers import MaxPooling2D\n",
      "   9: from keras.layers import SeparableConv2D\n",
      "  10: from keras import layers\n",
      "  11: from keras.regularizers import l2\n",
      "  12: \n",
      "  13: \n",
      "  14: def simple_CNN(input_shape, num_classes):\n",
      "  15: \n",
      "  16:     model = Sequential()\n",
      "  17:     model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same',\n",
      "  18:                             name='image_array', input_shape=input_shape))\n",
      "  19:     model.add(BatchNormalization())\n",
      "  20:     model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding='same'))\n",
      "  21:     model.add(BatchNormalization())\n",
      "  22:     model.add(Activation('relu'))\n",
      "  23:     model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
      "  24:     model.add(Dropout(.5))\n",
      "  25: \n",
      "  26:     model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
      "  27:     model.add(BatchNormalization())\n",
      "  28:     model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
      "  29:     model.add(BatchNormalization())\n",
      "  30:     model.add(Activation('relu'))\n",
      "  31:     model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
      "  32:     model.add(Dropout(.5))\n",
      "  33: \n",
      "  34:     model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
      "  35:     model.add(BatchNormalization())\n",
      "  36:     model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
      "  37:     model.add(BatchNormalization())\n",
      "  38:     model.add(Activation('relu'))\n",
      "  39:     model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
      "  40:     model.add(Dropout(.5))\n",
      "  41: \n",
      "  42:     model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
      "  43:     model.add(BatchNormalization())\n",
      "  44:     model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding='same'))\n",
      "  45:     model.add(BatchNormalization())\n",
      "  46:     model.add(Activation('relu'))\n",
      "  47:     model.add(AveragePooling2D(pool_size=(2, 2), padding='same'))\n",
      "  48:     model.add(Dropout(.5))\n",
      "  49: \n",
      "  50:     model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding='same'))\n",
      "  51:     model.add(BatchNormalization())\n",
      "  52:     model.add(Convolution2D(\n",
      "  53:         filters=num_classes, kernel_size=(3, 3), padding='same'))\n",
      "  54:     model.add(GlobalAveragePooling2D())\n",
      "  55:     model.add(Activation('softmax', name='predictions'))\n",
      "  56:     return model\n",
      "  57: \n",
      "  58: \n",
      "  59: def simpler_CNN(input_shape, num_classes):\n",
      "  60: \n",
      "  61:     model = Sequential()\n",
      "  62:     model.add(Convolution2D(filters=16, kernel_size=(5, 5), padding='same',\n",
      "  63:                             name='image_array', input_shape=input_shape))\n",
      "  64:     model.add(BatchNormalization())\n",
      "  65:     model.add(Convolution2D(filters=16, kernel_size=(5, 5),\n",
      "  66:                             strides=(2, 2), padding='same'))\n",
      "  67:     model.add(BatchNormalization())\n",
      "  68:     model.add(Activation('relu'))\n",
      "  69:     model.add(Dropout(.25))\n",
      "  70: \n",
      "  71:     model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding='same'))\n",
      "  72:     model.add(BatchNormalization())\n",
      "  73:     model.add(Convolution2D(filters=32, kernel_size=(5, 5),\n",
      "  74:                             strides=(2, 2), padding='same'))\n",
      "  75:     model.add(BatchNormalization())\n",
      "  76:     model.add(Activation('relu'))\n",
      "  77:     model.add(Dropout(.25))\n",
      "  78: \n",
      "  79:     model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding='same'))\n",
      "  80:     model.add(BatchNormalization())\n",
      "  81:     model.add(Convolution2D(filters=64, kernel_size=(3, 3),\n",
      "  82:                             strides=(2, 2), padding='same'))\n",
      "  83:     model.add(BatchNormalization())\n",
      "  84:     model.add(Activation('relu'))\n",
      "  85:     model.add(Dropout(.25))\n",
      "  86: \n",
      "  87:     model.add(Convolution2D(filters=64, kernel_size=(1, 1), padding='same'))\n",
      "  88:     model.add(BatchNormalization())\n",
      "  89:     model.add(Convolution2D(filters=128, kernel_size=(3, 3),\n",
      "  90:                             strides=(2, 2), padding='same'))\n",
      "  91:     model.add(BatchNormalization())\n",
      "  92:     model.add(Activation('relu'))\n",
      "  93:     model.add(Dropout(.25))\n",
      "  94: \n",
      "  95:     model.add(Convolution2D(filters=256, kernel_size=(1, 1), padding='same'))\n",
      "  96:     model.add(BatchNormalization())\n",
      "  97:     model.add(Convolution2D(filters=128, kernel_size=(3, 3),\n",
      "  98:                             strides=(2, 2), padding='same'))\n",
      "  99: \n",
      " 100:     model.add(Convolution2D(filters=256, kernel_size=(1, 1), padding='same'))\n",
      " 101:     model.add(BatchNormalization())\n",
      " 102:     model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3),\n",
      " 103:                             strides=(2, 2), padding='same'))\n",
      " 104: \n",
      " 105:     model.add(Flatten())\n",
      " 106:     # model.add(GlobalAveragePooling2D())\n",
      " 107:     model.add(Activation('softmax', name='predictions'))\n",
      " 108:     return model\n",
      " 109: \n",
      " 110: \n",
      " 111: def tiny_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
      " 112:     regularization = l2(l2_regularization)\n",
      " 113: \n",
      " 114:     # base\n",
      " 115:     img_input = Input(input_shape)\n",
      " 116:     x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
      " 117:                use_bias=False)(img_input)\n",
      " 118:     x = BatchNormalization()(x)\n",
      " 119:     x = Activation('relu')(x)\n",
      " 120:     x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
      " 121:                use_bias=False)(x)\n",
      " 122:     x = BatchNormalization()(x)\n",
      " 123:     x = Activation('relu')(x)\n",
      " 124: \n",
      " 125:     # module 1\n",
      " 126:     residual = Conv2D(8, (1, 1), strides=(2, 2),\n",
      " 127:                       padding='same', use_bias=False)(x)\n",
      " 128:     residual = BatchNormalization()(residual)\n",
      " 129: \n",
      " 130:     x = SeparableConv2D(8, (3, 3), padding='same',\n",
      " 131:                         kernel_regularizer=regularization,\n",
      " 132:                         use_bias=False)(x)\n",
      " 133:     x = BatchNormalization()(x)\n",
      " 134:     x = Activation('relu')(x)\n",
      " 135:     x = SeparableConv2D(8, (3, 3), padding='same',\n",
      " 136:                         kernel_regularizer=regularization,\n",
      " 137:                         use_bias=False)(x)\n",
      " 138:     x = BatchNormalization()(x)\n",
      " 139: \n",
      " 140:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 141:     x = layers.add([x, residual])\n",
      " 142: \n",
      " 143:     # module 2\n",
      " 144:     residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
      " 145:                       padding='same', use_bias=False)(x)\n",
      " 146:     residual = BatchNormalization()(residual)\n",
      " 147: \n",
      " 148:     x = SeparableConv2D(16, (3, 3), padding='same',\n",
      " 149:                         kernel_regularizer=regularization,\n",
      " 150:                         use_bias=False)(x)\n",
      " 151:     x = BatchNormalization()(x)\n",
      " 152:     x = Activation('relu')(x)\n",
      " 153:     x = SeparableConv2D(16, (3, 3), padding='same',\n",
      " 154:                         kernel_regularizer=regularization,\n",
      " 155:                         use_bias=False)(x)\n",
      " 156:     x = BatchNormalization()(x)\n",
      " 157: \n",
      " 158:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 159:     x = layers.add([x, residual])\n",
      " 160: \n",
      " 161:     # module 3\n",
      " 162:     residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
      " 163:                       padding='same', use_bias=False)(x)\n",
      " 164:     residual = BatchNormalization()(residual)\n",
      " 165: \n",
      " 166:     x = SeparableConv2D(32, (3, 3), padding='same',\n",
      " 167:                         kernel_regularizer=regularization,\n",
      " 168:                         use_bias=False)(x)\n",
      " 169:     x = BatchNormalization()(x)\n",
      " 170:     x = Activation('relu')(x)\n",
      " 171:     x = SeparableConv2D(32, (3, 3), padding='same',\n",
      " 172:                         kernel_regularizer=regularization,\n",
      " 173:                         use_bias=False)(x)\n",
      " 174:     x = BatchNormalization()(x)\n",
      " 175: \n",
      " 176:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 177:     x = layers.add([x, residual])\n",
      " 178: \n",
      " 179:     # module 4\n",
      " 180:     residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
      " 181:                       padding='same', use_bias=False)(x)\n",
      " 182:     residual = BatchNormalization()(residual)\n",
      " 183: \n",
      " 184:     x = SeparableConv2D(64, (3, 3), padding='same',\n",
      " 185:                         kernel_regularizer=regularization,\n",
      " 186:                         use_bias=False)(x)\n",
      " 187:     x = BatchNormalization()(x)\n",
      " 188:     x = Activation('relu')(x)\n",
      " 189:     x = SeparableConv2D(64, (3, 3), padding='same',\n",
      " 190:                         kernel_regularizer=regularization,\n",
      " 191:                         use_bias=False)(x)\n",
      " 192:     x = BatchNormalization()(x)\n",
      " 193: \n",
      " 194:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 195:     x = layers.add([x, residual])\n",
      " 196: \n",
      " 197:     x = Conv2D(num_classes, (3, 3),\n",
      " 198:                # kernel_regularizer=regularization,\n",
      " 199:                padding='same')(x)\n",
      " 200:     x = GlobalAveragePooling2D()(x)\n",
      " 201:     output = Activation('softmax', name='predictions')(x)\n",
      " 202: \n",
      " 203:     model = Model(img_input, output)\n",
      " 204:     return model\n",
      " 205: \n",
      " 206: \n",
      " 207: def mini_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n",
      " 208:     regularization = l2(l2_regularization)\n",
      " 209: \n",
      " 210:     # base\n",
      " 211:     img_input = Input(input_shape)\n",
      " 212:     x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
      " 213:                use_bias=False)(img_input)\n",
      " 214:     x = BatchNormalization()(x)\n",
      " 215:     x = Activation('relu')(x)\n",
      " 216:     x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n",
      " 217:                use_bias=False)(x)\n",
      " 218:     x = BatchNormalization()(x)\n",
      " 219:     x = Activation('relu')(x)\n",
      " 220: \n",
      " 221:     # module 1\n",
      " 222:     residual = Conv2D(16, (1, 1), strides=(2, 2),\n",
      " 223:                       padding='same', use_bias=False)(x)\n",
      " 224:     residual = BatchNormalization()(residual)\n",
      " 225: \n",
      " 226:     x = SeparableConv2D(16, (3, 3), padding='same',\n",
      " 227:                         kernel_regularizer=regularization,\n",
      " 228:                         use_bias=False)(x)\n",
      " 229:     x = BatchNormalization()(x)\n",
      " 230:     x = Activation('relu')(x)\n",
      " 231:     x = SeparableConv2D(16, (3, 3), padding='same',\n",
      " 232:                         kernel_regularizer=regularization,\n",
      " 233:                         use_bias=False)(x)\n",
      " 234:     x = BatchNormalization()(x)\n",
      " 235: \n",
      " 236:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 237:     x = layers.add([x, residual])\n",
      " 238: \n",
      " 239:     # module 2\n",
      " 240:     residual = Conv2D(32, (1, 1), strides=(2, 2),\n",
      " 241:                       padding='same', use_bias=False)(x)\n",
      " 242:     residual = BatchNormalization()(residual)\n",
      " 243: \n",
      " 244:     x = SeparableConv2D(32, (3, 3), padding='same',\n",
      " 245:                         kernel_regularizer=regularization,\n",
      " 246:                         use_bias=False)(x)\n",
      " 247:     x = BatchNormalization()(x)\n",
      " 248:     x = Activation('relu')(x)\n",
      " 249:     x = SeparableConv2D(32, (3, 3), padding='same',\n",
      " 250:                         kernel_regularizer=regularization,\n",
      " 251:                         use_bias=False)(x)\n",
      " 252:     x = BatchNormalization()(x)\n",
      " 253: \n",
      " 254:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 255:     x = layers.add([x, residual])\n",
      " 256: \n",
      " 257:     # module 3\n",
      " 258:     residual = Conv2D(64, (1, 1), strides=(2, 2),\n",
      " 259:                       padding='same', use_bias=False)(x)\n",
      " 260:     residual = BatchNormalization()(residual)\n",
      " 261: \n",
      " 262:     x = SeparableConv2D(64, (3, 3), padding='same',\n",
      " 263:                         kernel_regularizer=regularization,\n",
      " 264:                         use_bias=False)(x)\n",
      " 265:     x = BatchNormalization()(x)\n",
      " 266:     x = Activation('relu')(x)\n",
      " 267:     x = SeparableConv2D(64, (3, 3), padding='same',\n",
      " 268:                         kernel_regularizer=regularization,\n",
      " 269:                         use_bias=False)(x)\n",
      " 270:     x = BatchNormalization()(x)\n",
      " 271: \n",
      " 272:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 273:     x = layers.add([x, residual])\n",
      " 274: \n",
      " 275:     # module 4\n",
      " 276:     residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
      " 277:                       padding='same', use_bias=False)(x)\n",
      " 278:     residual = BatchNormalization()(residual)\n",
      " 279: \n",
      " 280:     x = SeparableConv2D(128, (3, 3), padding='same',\n",
      " 281:                         kernel_regularizer=regularization,\n",
      " 282:                         use_bias=False)(x)\n",
      " 283:     x = BatchNormalization()(x)\n",
      " 284:     x = Activation('relu')(x)\n",
      " 285:     x = SeparableConv2D(128, (3, 3), padding='same',\n",
      " 286:                         kernel_regularizer=regularization,\n",
      " 287:                         use_bias=False)(x)\n",
      " 288:     x = BatchNormalization()(x)\n",
      " 289: \n",
      " 290:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 291:     x = layers.add([x, residual])\n",
      " 292: \n",
      " 293:     x = Conv2D(num_classes, (3, 3),\n",
      " 294:                # kernel_regularizer=regularization,\n",
      " 295:                padding='same')(x)\n",
      " 296:     x = GlobalAveragePooling2D()(x)\n",
      " 297:     output = Activation('softmax', name='predictions')(x)\n",
      " 298: \n",
      " 299:     model = Model(img_input, output)\n",
      " 300:     return model\n",
      " 301: \n",
      " 302: \n",
      " 303: def big_XCEPTION(input_shape, num_classes):\n",
      " 304:     img_input = Input(input_shape)\n",
      " 305:     x = Conv2D(32, (3, 3), strides=(2, 2), use_bias=False)(img_input)\n",
      " 306:     x = BatchNormalization(name='block1_conv1_bn')(x)\n",
      " 307:     x = Activation('relu', name='block1_conv1_act')(x)\n",
      " 308:     x = Conv2D(64, (3, 3), use_bias=False)(x)\n",
      " 309:     x = BatchNormalization(name='block1_conv2_bn')(x)\n",
      " 310:     x = Activation('relu', name='block1_conv2_act')(x)\n",
      " 311: \n",
      " 312:     residual = Conv2D(128, (1, 1), strides=(2, 2),\n",
      " 313:                       padding='same', use_bias=False)(x)\n",
      " 314:     residual = BatchNormalization()(residual)\n",
      " 315: \n",
      " 316:     x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False)(x)\n",
      " 317:     x = BatchNormalization(name='block2_sepconv1_bn')(x)\n",
      " 318:     x = Activation('relu', name='block2_sepconv2_act')(x)\n",
      " 319:     x = SeparableConv2D(128, (3, 3), padding='same', use_bias=False)(x)\n",
      " 320:     x = BatchNormalization(name='block2_sepconv2_bn')(x)\n",
      " 321: \n",
      " 322:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 323:     x = layers.add([x, residual])\n",
      " 324: \n",
      " 325:     residual = Conv2D(256, (1, 1), strides=(2, 2),\n",
      " 326:                       padding='same', use_bias=False)(x)\n",
      " 327:     residual = BatchNormalization()(residual)\n",
      " 328: \n",
      " 329:     x = Activation('relu', name='block3_sepconv1_act')(x)\n",
      " 330:     x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False)(x)\n",
      " 331:     x = BatchNormalization(name='block3_sepconv1_bn')(x)\n",
      " 332:     x = Activation('relu', name='block3_sepconv2_act')(x)\n",
      " 333:     x = SeparableConv2D(256, (3, 3), padding='same', use_bias=False)(x)\n",
      " 334:     x = BatchNormalization(name='block3_sepconv2_bn')(x)\n",
      " 335: \n",
      " 336:     x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)\n",
      " 337:     x = layers.add([x, residual])\n",
      " 338:     x = Conv2D(num_classes, (3, 3),\n",
      " 339:                # kernel_regularizer=regularization,\n",
      " 340:                padding='same')(x)\n",
      " 341:     x = GlobalAveragePooling2D()(x)\n",
      " 342:     output = Activation('softmax', name='predictions')(x)\n",
      " 343: \n",
      " 344:     model = Model(img_input, output)\n",
      " 345:     return model\n",
      " 346: \n",
      " 347: \n",
      " 348: if __name__ == \"__main__\":\n",
      " 349:     input_shape = (64, 64, 1)\n",
      " 350:     num_classes = 7\n",
      " 351:     # model = tiny_XCEPTION(input_shape, num_classes)\n",
      " 352:     # model.summary()\n",
      " 353:     # model = mini_XCEPTION(input_shape, num_classes)\n",
      " 354:     # model.summary()\n",
      " 355:     # model = big_XCEPTION(input_shape, num_classes)\n",
      " 356:     # model.summary()\n",
      " 357:     model = simple_CNN((48, 48, 1), num_classes)\n",
      " 358:     model.summary()\n",
      "=== END FILE: cnn.py ===\n",
      "\n",
      "Output for /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/models/cnn.py: ```json\n",
      "{\n",
      "  \"nodes\": [\n",
      "    {\n",
      "      \"id\": \"cnn.simple_CNN\",\n",
      "      \"file\": \"cnn.py\",\n",
      "      \"line_start\": 14,\n",
      "      \"line_end\": 56,\n",
      "      \"label\": \"simple_CNN\",\n",
      "      \"description\": \"Builds a simple CNN model using Keras Sequential API.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cnn.simpler_CNN\",\n",
      "      \"file\": \"cnn.py\",\n",
      "      \"line_start\": 59,\n",
      "      \"line_end\": 108,\n",
      "      \"label\": \"simpler_CNN\",\n",
      "      \"description\": \"Builds a more compact CNN model with down-sampling via strides.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cnn.tiny_XCEPTION\",\n",
      "      \"file\": \"cnn.py\",\n",
      "      \"line_start\": 111,\n",
      "      \"line_end\": 205,\n",
      "      \"label\": \"tiny_XCEPTION\",\n",
      "      \"description\": \"Constructs a tiny Xception-style model with separable convolutions and residual connections.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cnn.mini_XCEPTION\",\n",
      "      \"file\": \"cnn.py\",\n",
      "      \"line_start\": 207,\n",
      "      \"line_end\": 301,\n",
      "      \"label\": \"mini_XCEPTION\",\n",
      "      \"description\": \"Constructs a mini Xception-style model with smaller channel sizes.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cnn.big_XCEPTION\",\n",
      "      \"file\": \"cnn.py\",\n",
      "      \"line_start\": 303,\n",
      "      \"line_end\": 345,\n",
      "      \"label\": \"big_XCEPTION\",\n",
      "      \"description\": \"Constructs a larger Xception-style model with higher channel counts.\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"cnn.__main__\",\n",
      "      \"file\": \"cnn.py\",\n",
      "      \"line_start\": 348,\n",
      "      \"line_end\": 358,\n",
      "      \"label\": \"__main__\",\n",
      "      \"description\": \"Script entry point that selects and summaries a model.\"\n",
      "    }\n",
      "  ],\n",
      "  \"edges\": [\n",
      "    {\n",
      "      \"id\": \"e___main___simple_CNN\",\n",
      "      \"source\": \"cnn.__main__\",\n",
      "      \"target\": \"cnn.simple_CNN\",\n",
      "      \"type\": \"call\"\n",
      "    },\n",
      "    {\n",
      "      \"id\": \"e___main___Sequential.summary\",\n",
      "      \"source\": \"cnn.__main__\",\n",
      "      \"target\": \"keras.models.Sequential.summary\",\n",
      "      \"type\": \"call\"\n",
      "    }\n",
      "  ]\n",
      "}\n",
      "```\n",
      "Reasoning: {'id': 'rs_6829fb6581008191b7f2736b3254b7e00429a3f2a1e8050d', 'summary': [], 'type': 'reasoning'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_gradcam_demo.py': {'nodes': [{'id': 'image_gradcam_demo.main',\n",
       "    'file': 'image_gradcam_demo.py',\n",
       "    'line_start': 1,\n",
       "    'line_end': 85,\n",
       "    'label': 'main',\n",
       "    'description': 'Top-level script execution entry point.'}],\n",
       "  'edges': [{'id': 'e_main_get_labels',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.datasets.get_labels',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_load_model',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_load_detection_model',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.inference.load_detection_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_load_image_gray',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.inference.load_image',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_load_image_rgb',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.inference.load_image',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_detect_faces',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.inference.detect_faces',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_apply_offsets_1',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_apply_offsets_2',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_preprocess_input',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_compile_gradient_function',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.compile_gradient_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_register_gradient',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.register_gradient',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_modify_backprop',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.modify_backprop',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_compile_saliency_function',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.compile_saliency_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_calculate_guided_gradient_CAM',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.calculate_guided_gradient_CAM',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_draw_bounding_box',\n",
       "    'source': 'image_gradcam_demo.main',\n",
       "    'target': 'utils.inference.draw_bounding_box',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_gender_classifier.py': {'nodes': [{'id': 'train_gender_classifier.main',\n",
       "    'file': 'train_gender_classifier.py',\n",
       "    'line_start': 1,\n",
       "    'line_end': 71,\n",
       "    'label': 'main',\n",
       "    'description': 'Top-level script that loads data, builds and trains the gender classification model.'}],\n",
       "  'edges': [{'id': 'e_main_utils.datasets.DataManager',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'utils.datasets.DataManager',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.datasets.DataManager.get_data',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'utils.datasets.DataManager.get_data',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.datasets.split_imdb_data',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'utils.datasets.split_imdb_data',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.data_augmentation.ImageGenerator',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'utils.data_augmentation.ImageGenerator',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_models.cnn.mini_XCEPTION',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'models.cnn.mini_XCEPTION',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_model.compile',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'model.compile',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_model.summary',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'model.summary',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_keras.callbacks.EarlyStopping',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'keras.callbacks.EarlyStopping',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_keras.callbacks.ReduceLROnPlateau',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'keras.callbacks.ReduceLROnPlateau',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_keras.callbacks.CSVLogger',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'keras.callbacks.CSVLogger',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_keras.callbacks.ModelCheckpoint',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'keras.callbacks.ModelCheckpoint',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_image_generator.flow',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'utils.data_augmentation.ImageGenerator.flow',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_model.fit_generator',\n",
       "    'source': 'train_gender_classifier.main',\n",
       "    'target': 'model.fit_generator',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_gender_demo.py': {'nodes': [{'id': 'video_emotion_gender_demo.main',\n",
       "    'file': 'video_emotion_gender_demo.py',\n",
       "    'line_start': 1,\n",
       "    'line_end': 102,\n",
       "    'label': 'main',\n",
       "    'description': 'Main script entry point: loads models and runs the video processing loop.'}],\n",
       "  'edges': [{'id': 'e_main_utils.datasets.get_labels_fer2013',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.datasets.get_labels',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.datasets.get_labels_imdb',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.datasets.get_labels',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.load_detection_model',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.load_detection_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_keras.models.load_model_emotion',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_keras.models.load_model_gender',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.namedWindow',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.namedWindow',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.VideoCapture',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.VideoCapture',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_detect_faces',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.detect_faces',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_apply_offsets_gender',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_apply_offsets_emotion',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.resize_rgb_face',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.resize_gray_face',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.preprocessor.preprocess_input_gray',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_np.expand_dims_1',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'numpy.expand_dims',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_np.expand_dims_2',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'numpy.expand_dims',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_emotion_classifier.predict',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'keras.models.Model.predict',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_np.argmax_emotion',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'numpy.argmax',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_np.argmax_gender',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'numpy.argmax',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.preprocessor.preprocess_input_rgb',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_gender_classifier.predict',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'keras.models.Model.predict',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_statistics.mode_emotion',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'statistics.mode',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_statistics.mode_gender',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'statistics.mode',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.draw_bounding_box',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.draw_bounding_box',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.draw_text_gender',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.draw_text',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.draw_text_emotion',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.draw_text',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.cvtColor_gray',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.cvtColor',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.cvtColor_bgr',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.cvtColor',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.imshow',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.imshow',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.waitKey',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.waitKey',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_video_capture.release',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.VideoCapture.release',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.destroyAllWindows',\n",
       "    'source': 'video_emotion_gender_demo.main',\n",
       "    'target': 'cv2.destroyAllWindows',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_color_demo.py': {'nodes': [{'id': 'video_emotion_color_demo.main',\n",
       "    'file': 'video_emotion_color_demo.py',\n",
       "    'line_start': 1,\n",
       "    'line_end': 91,\n",
       "    'label': 'main',\n",
       "    'description': 'Top-level script executing video capture, face detection, emotion recognition, and annotated display loop.'}],\n",
       "  'edges': [{'id': 'e_main_utils.datasets.get_labels',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'utils.datasets.get_labels',\n",
       "    'type': 'call',\n",
       "    'description': 'Load emotion labels from dataset definitions'},\n",
       "   {'id': 'e_main_keras.models.load_model',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call',\n",
       "    'description': 'Load the pretrained emotion classification model'},\n",
       "   {'id': 'e_main_utils.inference.load_detection_model',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'utils.inference.load_detection_model',\n",
       "    'type': 'call',\n",
       "    'description': 'Load the face detection model (Haar cascade)'},\n",
       "   {'id': 'e_main_utils.inference.detect_faces',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'utils.inference.detect_faces',\n",
       "    'type': 'call',\n",
       "    'description': 'Detect face bounding boxes in a grayscale frame'},\n",
       "   {'id': 'e_main_utils.inference.apply_offsets',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call',\n",
       "    'description': 'Apply offsets to face coordinates for emotion ROI'},\n",
       "   {'id': 'e_main_utils.preprocessor.preprocess_input',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call',\n",
       "    'description': 'Normalize and scale the face image for model input'},\n",
       "   {'id': 'e_main_utils.inference.draw_bounding_box',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'utils.inference.draw_bounding_box',\n",
       "    'type': 'call',\n",
       "    'description': 'Draw the colored rectangle around detected face'},\n",
       "   {'id': 'e_main_utils.inference.draw_text',\n",
       "    'source': 'video_emotion_color_demo.main',\n",
       "    'target': 'utils.inference.draw_text',\n",
       "    'type': 'call',\n",
       "    'description': 'Render the emotion label text on the image'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_gradcam_demo.py': {'nodes': [{'id': 'video_gradcam_demo.main',\n",
       "    'file': 'video_gradcam_demo.py',\n",
       "    'line_start': 1,\n",
       "    'line_end': 92,\n",
       "    'label': 'main',\n",
       "    'description': 'Main script executing the video Grad-CAM demo.'}],\n",
       "  'edges': [{'id': 'e_main_load_model',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_compile_gradient_function',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.compile_gradient_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_register_gradient',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.register_gradient',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_modify_backprop',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.modify_backprop',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_compile_saliency_function',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.compile_saliency_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_load_detection_model',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.inference.load_detection_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_detect_faces',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.inference.detect_faces',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_apply_offsets',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_preprocess_input',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_calculate_guided_gradient_CAM',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.grad_cam.calculate_guided_gradient_CAM',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_draw_bounding_box',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'utils.inference.draw_bounding_box',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_namedWindow',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'cv2.namedWindow',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_VideoCapture',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'cv2.VideoCapture',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_read',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'cv2.VideoCapture.read',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cvtColor',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'cv2.cvtColor',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_resize',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_expand_dims',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'numpy.expand_dims',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_repeat',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'numpy.repeat',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_imshow',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'cv2.imshow',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_waitKey',\n",
       "    'source': 'video_gradcam_demo.main',\n",
       "    'target': 'cv2.waitKey',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_emotion_classifier.py': {'nodes': [{'id': 'train_emotion_classifier.main',\n",
       "    'file': 'train_emotion_classifier.py',\n",
       "    'line_start': 1,\n",
       "    'line_end': 72,\n",
       "    'label': 'main',\n",
       "    'description': 'Top‐level script that configures data generators, builds and trains the emotion classification model.'}],\n",
       "  'edges': [{'id': 'e_main_to_ImageDataGenerator',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.preprocessing.image.ImageDataGenerator',\n",
       "    'type': 'call',\n",
       "    'description': 'Instantiate image data generator for augmentation.'},\n",
       "   {'id': 'e_main_to_mini_XCEPTION',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'models.cnn.mini_XCEPTION',\n",
       "    'type': 'call',\n",
       "    'description': 'Create the mini_XCEPTION convolutional model.'},\n",
       "   {'id': 'e_main_to_Model_compile',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.models.Model.compile',\n",
       "    'type': 'call',\n",
       "    'description': 'Compile the model with optimizer, loss, and metrics.'},\n",
       "   {'id': 'e_main_to_Model_summary',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.models.Model.summary',\n",
       "    'type': 'call',\n",
       "    'description': 'Print model architecture summary.'},\n",
       "   {'id': 'e_main_to_CSVLogger',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.callbacks.CSVLogger',\n",
       "    'type': 'call',\n",
       "    'description': 'Create CSV logger callback.'},\n",
       "   {'id': 'e_main_to_ModelCheckpoint',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.callbacks.ModelCheckpoint',\n",
       "    'type': 'call',\n",
       "    'description': 'Create model checkpoint callback.'},\n",
       "   {'id': 'e_main_to_EarlyStopping',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.callbacks.EarlyStopping',\n",
       "    'type': 'call',\n",
       "    'description': 'Create early stopping callback.'},\n",
       "   {'id': 'e_main_to_ReduceLROnPlateau',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.callbacks.ReduceLROnPlateau',\n",
       "    'type': 'call',\n",
       "    'description': 'Create learning rate reduction callback.'},\n",
       "   {'id': 'e_main_to_DataManager',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'utils.datasets.DataManager',\n",
       "    'type': 'call',\n",
       "    'description': 'Instantiate data manager for loading dataset.'},\n",
       "   {'id': 'e_main_to_get_data',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'utils.datasets.DataManager.get_data',\n",
       "    'type': 'call',\n",
       "    'description': 'Load faces and emotion labels.'},\n",
       "   {'id': 'e_main_to_preprocess_input',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call',\n",
       "    'description': 'Apply preprocessing to face images.'},\n",
       "   {'id': 'e_main_to_split_data',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'utils.datasets.split_data',\n",
       "    'type': 'call',\n",
       "    'description': 'Split dataset into training and validation sets.'},\n",
       "   {'id': 'e_main_to_flow',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.preprocessing.image.ImageDataGenerator.flow',\n",
       "    'type': 'call',\n",
       "    'description': 'Generate augmented batches from training data.'},\n",
       "   {'id': 'e_main_to_fit_generator',\n",
       "    'source': 'train_emotion_classifier.main',\n",
       "    'target': 'keras.models.Model.fit_generator',\n",
       "    'type': 'call',\n",
       "    'description': 'Train the model using a data generator.'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_emotion_gender_demo.py': {'nodes': [{'id': 'image_emotion_gender_demo.main',\n",
       "    'file': 'image_emotion_gender_demo.py',\n",
       "    'line_start': 1,\n",
       "    'line_end': 82,\n",
       "    'label': 'main',\n",
       "    'description': 'Script entry point that loads models, processes the image, performs face detection, emotion and gender prediction, and outputs the annotated image.'}],\n",
       "  'edges': [{'id': 'e_main_utils.datasets.get_labels',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.datasets.get_labels',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_keras.models.load_model',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.load_detection_model',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.load_detection_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.load_image',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.load_image',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.detect_faces',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.detect_faces',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.apply_offsets',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.preprocessor.preprocess_input',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.draw_bounding_box',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.draw_bounding_box',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_utils.inference.draw_text',\n",
       "    'source': 'image_emotion_gender_demo.main',\n",
       "    'target': 'utils.inference.draw_text',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/faces.py': {'nodes': [{'id': 'faces.index',\n",
       "    'file': 'faces.py',\n",
       "    'line_start': 8,\n",
       "    'line_end': 10,\n",
       "    'label': 'index',\n",
       "    'description': 'Redirects root URL to external site.'},\n",
       "   {'id': 'faces.upload',\n",
       "    'file': 'faces.py',\n",
       "    'line_start': 12,\n",
       "    'line_end': 20,\n",
       "    'label': 'upload',\n",
       "    'description': 'Handles image upload, processes it, and returns the annotated image.'},\n",
       "   {'id': 'faces.bad_request',\n",
       "    'file': 'faces.py',\n",
       "    'line_start': 22,\n",
       "    'line_end': 24,\n",
       "    'label': 'bad_request',\n",
       "    'description': 'Custom handler for 400 errors, returns JSON error message.'},\n",
       "   {'id': 'faces.not_found',\n",
       "    'file': 'faces.py',\n",
       "    'line_start': 26,\n",
       "    'line_end': 28,\n",
       "    'label': 'not_found',\n",
       "    'description': 'Custom handler for 404 errors, returns JSON error message.'},\n",
       "   {'id': 'faces.main',\n",
       "    'file': 'faces.py',\n",
       "    'line_start': 30,\n",
       "    'line_end': 31,\n",
       "    'label': 'main',\n",
       "    'description': 'Starts the Flask application when run as a script.'}],\n",
       "  'edges': [{'id': 'e_faces.index_flask.redirect',\n",
       "    'source': 'faces.index',\n",
       "    'target': 'flask.redirect',\n",
       "    'type': 'call',\n",
       "    'description': 'Redirect to external URL.'},\n",
       "   {'id': 'e_faces.upload_emotion_gender_processor.process_image',\n",
       "    'source': 'faces.upload',\n",
       "    'target': 'emotion_gender_processor.process_image',\n",
       "    'type': 'call',\n",
       "    'description': 'Process the uploaded image.'},\n",
       "   {'id': 'e_faces.upload_flask.send_file',\n",
       "    'source': 'faces.upload',\n",
       "    'target': 'flask.send_file',\n",
       "    'type': 'call',\n",
       "    'description': 'Send back the processed image file.'},\n",
       "   {'id': 'e_faces.upload_flask.abort',\n",
       "    'source': 'faces.upload',\n",
       "    'target': 'flask.abort',\n",
       "    'type': 'call',\n",
       "    'description': 'Abort the request on error.'},\n",
       "   {'id': 'e_faces.bad_request_flask.make_response',\n",
       "    'source': 'faces.bad_request',\n",
       "    'target': 'flask.make_response',\n",
       "    'type': 'call',\n",
       "    'description': 'Construct the HTTP response for 400 errors.'},\n",
       "   {'id': 'e_faces.bad_request_flask.jsonify',\n",
       "    'source': 'faces.bad_request',\n",
       "    'target': 'flask.jsonify',\n",
       "    'type': 'call',\n",
       "    'description': 'Serialize error message to JSON.'},\n",
       "   {'id': 'e_faces.not_found_flask.make_response',\n",
       "    'source': 'faces.not_found',\n",
       "    'target': 'flask.make_response',\n",
       "    'type': 'call',\n",
       "    'description': 'Construct the HTTP response for 404 errors.'},\n",
       "   {'id': 'e_faces.not_found_flask.jsonify',\n",
       "    'source': 'faces.not_found',\n",
       "    'target': 'flask.jsonify',\n",
       "    'type': 'call',\n",
       "    'description': 'Serialize error message to JSON.'},\n",
       "   {'id': 'e_faces.main_flask.Flask.run',\n",
       "    'source': 'faces.main',\n",
       "    'target': 'flask.Flask.run',\n",
       "    'type': 'call',\n",
       "    'description': 'Run the Flask development server.'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/emotion_gender_processor.py': {'nodes': [{'id': 'emotion_gender_processor.process_image',\n",
       "    'file': 'emotion_gender_processor.py',\n",
       "    'line_start': 18,\n",
       "    'line_end': 87,\n",
       "    'label': 'process_image',\n",
       "    'description': 'Processes an input image: loads models, detects faces, predicts gender and emotion, and annotates the image.'}],\n",
       "  'edges': [{'id': 'e_process_image_utils.datasets.get_labels',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'utils.datasets.get_labels',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_keras.models.load_model',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_utils.inference.load_detection_model',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'utils.inference.load_detection_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_numpy.fromstring',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'numpy.fromstring',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_cv2.imdecode',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'cv2.imdecode',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_cv2.cvtColor',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'cv2.cvtColor',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_utils.inference.detect_faces',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'utils.inference.detect_faces',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_utils.inference.apply_offsets',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'utils.inference.apply_offsets',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_cv2.resize',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_utils.preprocessor.preprocess_input',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'utils.preprocessor.preprocess_input',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_numpy.expand_dims',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'numpy.expand_dims',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_gender_classifier.predict',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'gender_classifier.predict',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_numpy.argmax',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'numpy.argmax',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_emotion_classifier.predict',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'emotion_classifier.predict',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_utils.inference.draw_bounding_box',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'utils.inference.draw_bounding_box',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_utils.inference.draw_text',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'utils.inference.draw_text',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_process_image_cv2.imwrite',\n",
       "    'source': 'emotion_gender_processor.process_image',\n",
       "    'target': 'cv2.imwrite',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/datasets.py': {'nodes': [{'id': 'datasets.DataManager.__init__',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 12,\n",
       "    'line_end': 28,\n",
       "    'label': '__init__',\n",
       "    'description': 'Initializes DataManager, sets dataset_name, dataset_path, image_size, and resolves default paths.'},\n",
       "   {'id': 'datasets.DataManager.get_data',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 30,\n",
       "    'line_end': 37,\n",
       "    'label': 'get_data',\n",
       "    'description': 'Selects and invokes the appropriate dataset loading method based on dataset_name.'},\n",
       "   {'id': 'datasets.DataManager._load_imdb',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 39,\n",
       "    'line_end': 57,\n",
       "    'label': '_load_imdb',\n",
       "    'description': 'Loads and filters the IMDB dataset from a .mat file, returning a dict of image paths to gender labels.'},\n",
       "   {'id': 'datasets.DataManager._load_fer2013',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 59,\n",
       "    'line_end': 72,\n",
       "    'label': '_load_fer2013',\n",
       "    'description': 'Loads the FER2013 CSV dataset, processes pixel strings into resized images and one-hot emotion labels.'},\n",
       "   {'id': 'datasets.DataManager._load_KDEF',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 74,\n",
       "    'line_end': 102,\n",
       "    'label': '_load_KDEF',\n",
       "    'description': 'Traverses KDEF image folders, reads and resizes grayscale images, maps filenames to emotion classes.'},\n",
       "   {'id': 'datasets.get_labels',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 105,\n",
       "    'line_end': 114,\n",
       "    'label': 'get_labels',\n",
       "    'description': 'Returns a mapping from numeric label to class name for a given dataset_name.'},\n",
       "   {'id': 'datasets.get_class_to_arg',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 117,\n",
       "    'line_end': 126,\n",
       "    'label': 'get_class_to_arg',\n",
       "    'description': 'Returns a mapping from class name to numeric index for a given dataset_name.'},\n",
       "   {'id': 'datasets.split_imdb_data',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 129,\n",
       "    'line_end': 137,\n",
       "    'label': 'split_imdb_data',\n",
       "    'description': 'Splits IMDB ground truth dict keys into training and validation lists, optionally shuffling.'},\n",
       "   {'id': 'datasets.split_data',\n",
       "    'file': 'datasets.py',\n",
       "    'line_start': 140,\n",
       "    'line_end': 149,\n",
       "    'label': 'split_data',\n",
       "    'description': 'Splits arrays x and y into training and validation subsets based on validation_split.'}],\n",
       "  'edges': [{'id': 'e_get_data__load_imdb',\n",
       "    'source': 'datasets.DataManager.get_data',\n",
       "    'target': 'datasets.DataManager._load_imdb',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_get_data__load_fer2013',\n",
       "    'source': 'datasets.DataManager.get_data',\n",
       "    'target': 'datasets.DataManager._load_fer2013',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_get_data__load_KDEF',\n",
       "    'source': 'datasets.DataManager.get_data',\n",
       "    'target': 'datasets.DataManager._load_KDEF',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_imdb_loadmat',\n",
       "    'source': 'datasets.DataManager._load_imdb',\n",
       "    'target': 'scipy.io.loadmat',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_imdb_numpy.isnan',\n",
       "    'source': 'datasets.DataManager._load_imdb',\n",
       "    'target': 'numpy.isnan',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_imdb_numpy.logical_not',\n",
       "    'source': 'datasets.DataManager._load_imdb',\n",
       "    'target': 'numpy.logical_not',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_imdb_numpy.logical_and',\n",
       "    'source': 'datasets.DataManager._load_imdb',\n",
       "    'target': 'numpy.logical_and',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_fer2013_pandas.read_csv',\n",
       "    'source': 'datasets.DataManager._load_fer2013',\n",
       "    'target': 'pandas.read_csv',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_fer2013_numpy.asarray',\n",
       "    'source': 'datasets.DataManager._load_fer2013',\n",
       "    'target': 'numpy.asarray',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_fer2013_cv2.resize',\n",
       "    'source': 'datasets.DataManager._load_fer2013',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_fer2013_numpy.expand_dims',\n",
       "    'source': 'datasets.DataManager._load_fer2013',\n",
       "    'target': 'numpy.expand_dims',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_fer2013_pandas.get_dummies',\n",
       "    'source': 'datasets.DataManager._load_fer2013',\n",
       "    'target': 'pandas.get_dummies',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_KDEF_get_class_to_arg',\n",
       "    'source': 'datasets.DataManager._load_KDEF',\n",
       "    'target': 'datasets.get_class_to_arg',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_KDEF_os.walk',\n",
       "    'source': 'datasets.DataManager._load_KDEF',\n",
       "    'target': 'os.walk',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_KDEF_os.path.join',\n",
       "    'source': 'datasets.DataManager._load_KDEF',\n",
       "    'target': 'os.path.join',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_KDEF_cv2.imread',\n",
       "    'source': 'datasets.DataManager._load_KDEF',\n",
       "    'target': 'cv2.imread',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_KDEF_cv2.resize',\n",
       "    'source': 'datasets.DataManager._load_KDEF',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__load_KDEF_os.path.basename',\n",
       "    'source': 'datasets.DataManager._load_KDEF',\n",
       "    'target': 'os.path.basename',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_split_imdb_data_shuffle',\n",
       "    'source': 'datasets.split_imdb_data',\n",
       "    'target': 'random.shuffle',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/preprocessor.py': {'nodes': [{'id': 'preprocessor.preprocess_input',\n",
       "    'file': 'preprocessor.py',\n",
       "    'line_start': 5,\n",
       "    'line_end': 11,\n",
       "    'label': 'preprocess_input',\n",
       "    'description': 'Convert image array to float, normalize to [0,1], optionally scale to [-1,1]'},\n",
       "   {'id': 'preprocessor._imread',\n",
       "    'file': 'preprocessor.py',\n",
       "    'line_start': 14,\n",
       "    'line_end': 16,\n",
       "    'label': '_imread',\n",
       "    'description': 'Wrapper around scipy.misc.imread to read an image from disk'},\n",
       "   {'id': 'preprocessor._imresize',\n",
       "    'file': 'preprocessor.py',\n",
       "    'line_start': 18,\n",
       "    'line_end': 19,\n",
       "    'label': '_imresize',\n",
       "    'description': 'Wrapper around scipy.misc.imresize to resize an image array'},\n",
       "   {'id': 'preprocessor.to_categorical',\n",
       "    'file': 'preprocessor.py',\n",
       "    'line_start': 22,\n",
       "    'line_end': 27,\n",
       "    'label': 'to_categorical',\n",
       "    'description': 'Convert integer class labels to one-hot encoded (categorical) format'}],\n",
       "  'edges': [{'id': 'e_preprocess_input_imread',\n",
       "    'source': 'preprocessor._imread',\n",
       "    'target': 'scipy.misc.imread',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e__imresize_imresize',\n",
       "    'source': 'preprocessor._imresize',\n",
       "    'target': 'scipy.misc.imresize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_to_categorical_np_asarray',\n",
       "    'source': 'preprocessor.to_categorical',\n",
       "    'target': 'numpy.asarray',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_to_categorical_np_zeros',\n",
       "    'source': 'preprocessor.to_categorical',\n",
       "    'target': 'numpy.zeros',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_to_categorical_np_arange',\n",
       "    'source': 'preprocessor.to_categorical',\n",
       "    'target': 'numpy.arange',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/grad_cam.py': {'nodes': [{'id': 'grad_cam.reset_optimizer_weights',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 15,\n",
       "    'line_end': 18,\n",
       "    'label': 'reset_optimizer_weights',\n",
       "    'description': 'Remove optimizer weights from an HDF5 model file.'},\n",
       "   {'id': 'grad_cam.target_category_loss',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 21,\n",
       "    'line_end': 22,\n",
       "    'label': 'target_category_loss',\n",
       "    'description': 'Apply one-hot mask to model output for a target category.'},\n",
       "   {'id': 'grad_cam.target_category_loss_output_shape',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 25,\n",
       "    'line_end': 26,\n",
       "    'label': 'target_category_loss_output_shape',\n",
       "    'description': 'Return output shape unchanged for Lambda layer.'},\n",
       "   {'id': 'grad_cam.normalize',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 29,\n",
       "    'line_end': 31,\n",
       "    'label': 'normalize',\n",
       "    'description': 'Normalize a tensor by its L2 norm.'},\n",
       "   {'id': 'grad_cam.load_image',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 34,\n",
       "    'line_end': 37,\n",
       "    'label': 'load_image',\n",
       "    'description': 'Expand dims and preprocess input image array.'},\n",
       "   {'id': 'grad_cam.register_gradient',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 40,\n",
       "    'line_end': 47,\n",
       "    'label': 'register_gradient',\n",
       "    'description': 'Register guided backpropagation gradient override in TensorFlow.'},\n",
       "   {'id': 'grad_cam.compile_saliency_function',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 50,\n",
       "    'line_end': 55,\n",
       "    'label': 'compile_saliency_function',\n",
       "    'description': 'Compile a Keras function to compute saliency maps.'},\n",
       "   {'id': 'grad_cam.modify_backprop',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 58,\n",
       "    'line_end': 79,\n",
       "    'label': 'modify_backprop',\n",
       "    'description': 'Override ReLU gradient and reload model for guided backpropagation.'},\n",
       "   {'id': 'grad_cam.deprocess_image',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 82,\n",
       "    'line_end': 102,\n",
       "    'label': 'deprocess_image',\n",
       "    'description': 'Convert a tensor into a displayable uint8 image.'},\n",
       "   {'id': 'grad_cam.compile_gradient_function',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 105,\n",
       "    'line_end': 119,\n",
       "    'label': 'compile_gradient_function',\n",
       "    'description': 'Create a function to compute gradients and feature maps for CAM.'},\n",
       "   {'id': 'grad_cam.calculate_gradient_weighted_CAM',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 122,\n",
       "    'line_end': 141,\n",
       "    'label': 'calculate_gradient_weighted_CAM',\n",
       "    'description': 'Compute class activation map (CAM) using gradients.'},\n",
       "   {'id': 'grad_cam.calculate_guided_gradient_CAM',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 144,\n",
       "    'line_end': 151,\n",
       "    'label': 'calculate_guided_gradient_CAM',\n",
       "    'description': 'Combine CAM and guided saliency to produce guided Grad-CAM.'},\n",
       "   {'id': 'grad_cam.calculate_guided_gradient_CAM_v2',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 155,\n",
       "    'line_end': 167,\n",
       "    'label': 'calculate_guided_gradient_CAM_v2',\n",
       "    'description': 'Alternative guided Grad-CAM with resizing.'},\n",
       "   {'id': 'grad_cam.__main__',\n",
       "    'file': 'grad_cam.py',\n",
       "    'line_start': 170,\n",
       "    'line_end': 190,\n",
       "    'label': 'main',\n",
       "    'description': 'Script entry point: load model, compute and save guided Grad-CAM.'}],\n",
       "  'edges': [{'id': 'e_reset_optimizer_weights_h5py.File',\n",
       "    'source': 'grad_cam.reset_optimizer_weights',\n",
       "    'target': 'h5py.File',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_target_category_loss_tf.multiply',\n",
       "    'source': 'grad_cam.target_category_loss',\n",
       "    'target': 'tf.multiply',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_target_category_loss_K.one_hot',\n",
       "    'source': 'grad_cam.target_category_loss',\n",
       "    'target': 'keras.backend.one_hot',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_normalize_K.sqrt',\n",
       "    'source': 'grad_cam.normalize',\n",
       "    'target': 'keras.backend.sqrt',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_normalize_K.mean',\n",
       "    'source': 'grad_cam.normalize',\n",
       "    'target': 'keras.backend.mean',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_normalize_K.square',\n",
       "    'source': 'grad_cam.normalize',\n",
       "    'target': 'keras.backend.square',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_load_image_np.expand_dims',\n",
       "    'source': 'grad_cam.load_image',\n",
       "    'target': 'numpy.expand_dims',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_load_image_preprocess_input',\n",
       "    'source': 'grad_cam.load_image',\n",
       "    'target': 'grad_cam.preprocess_input',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_register_gradient_ops.RegisterGradient',\n",
       "    'source': 'grad_cam.register_gradient',\n",
       "    'target': 'tensorflow.python.framework.ops.RegisterGradient',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_saliency_function_model.get_layer',\n",
       "    'source': 'grad_cam.compile_saliency_function',\n",
       "    'target': 'keras.models.Model.get_layer',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_saliency_function_K.max',\n",
       "    'source': 'grad_cam.compile_saliency_function',\n",
       "    'target': 'keras.backend.max',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_saliency_function_K.gradients',\n",
       "    'source': 'grad_cam.compile_saliency_function',\n",
       "    'target': 'keras.backend.gradients',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_saliency_function_K.function',\n",
       "    'source': 'grad_cam.compile_saliency_function',\n",
       "    'target': 'keras.backend.function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_modify_backprop_tf.get_default_graph',\n",
       "    'source': 'grad_cam.modify_backprop',\n",
       "    'target': 'tensorflow.get_default_graph',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_modify_backprop_graph.gradient_override_map',\n",
       "    'source': 'grad_cam.modify_backprop',\n",
       "    'target': 'tensorflow.Graph.gradient_override_map',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_modify_backprop_load_model',\n",
       "    'source': 'grad_cam.modify_backprop',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_gradient_function_Sequential',\n",
       "    'source': 'grad_cam.compile_gradient_function',\n",
       "    'target': 'keras.models.Sequential',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_gradient_function_Lambda',\n",
       "    'source': 'grad_cam.compile_gradient_function',\n",
       "    'target': 'keras.layers.core.Lambda',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_gradient_function_K.sum',\n",
       "    'source': 'grad_cam.compile_gradient_function',\n",
       "    'target': 'keras.backend.sum',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_gradient_function_K.gradients',\n",
       "    'source': 'grad_cam.compile_gradient_function',\n",
       "    'target': 'keras.backend.gradients',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_gradient_function_normalize',\n",
       "    'source': 'grad_cam.compile_gradient_function',\n",
       "    'target': 'grad_cam.normalize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_compile_gradient_function_K.function',\n",
       "    'source': 'grad_cam.compile_gradient_function',\n",
       "    'target': 'keras.backend.function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_gradient_weighted_CAM_gradient_function',\n",
       "    'source': 'grad_cam.calculate_gradient_weighted_CAM',\n",
       "    'target': 'gradient_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_gradient_weighted_CAM_np.mean',\n",
       "    'source': 'grad_cam.calculate_gradient_weighted_CAM',\n",
       "    'target': 'numpy.mean',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_gradient_weighted_CAM_cv2.resize',\n",
       "    'source': 'grad_cam.calculate_gradient_weighted_CAM',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_gradient_weighted_CAM_cv2.applyColorMap',\n",
       "    'source': 'grad_cam.calculate_gradient_weighted_CAM',\n",
       "    'target': 'cv2.applyColorMap',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_calculate_gradient_weighted_CAM',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM',\n",
       "    'target': 'grad_cam.calculate_gradient_weighted_CAM',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_saliency_function',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM',\n",
       "    'target': 'saliency_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_deprocess_image',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM',\n",
       "    'target': 'grad_cam.deprocess_image',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_v2_calculate_gradient_weighted_CAM',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM_v2',\n",
       "    'target': 'grad_cam.calculate_gradient_weighted_CAM',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_v2_cv2.resize_1',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM_v2',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_v2_saliency_function',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM_v2',\n",
       "    'target': 'saliency_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_v2_cv2.resize_2',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM_v2',\n",
       "    'target': 'cv2.resize',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_calculate_guided_gradient_CAM_v2_deprocess_image',\n",
       "    'source': 'grad_cam.calculate_guided_gradient_CAM_v2',\n",
       "    'target': 'grad_cam.deprocess_image',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_pickle.load',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'pickle.load',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_load_model',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_load_image',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'grad_cam.load_image',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_model.predict',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'model.predict',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_np.argmax',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'numpy.argmax',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_compile_gradient_function',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'grad_cam.compile_gradient_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_register_gradient',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'grad_cam.register_gradient',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_modify_backprop',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'grad_cam.modify_backprop',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_compile_saliency_function',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'grad_cam.compile_saliency_function',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_calculate_guided_gradient_CAM',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'grad_cam.calculate_guided_gradient_CAM',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_main_cv2.imwrite',\n",
       "    'source': 'grad_cam.__main__',\n",
       "    'target': 'cv2.imwrite',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/inference.py': {'nodes': [{'id': 'inference.load_image',\n",
       "    'file': 'inference.py',\n",
       "    'line_start': 6,\n",
       "    'line_end': 8,\n",
       "    'label': 'load_image',\n",
       "    'description': 'Loads an image from disk and converts it to a numerical array.'},\n",
       "   {'id': 'inference.load_detection_model',\n",
       "    'file': 'inference.py',\n",
       "    'line_start': 10,\n",
       "    'line_end': 12,\n",
       "    'label': 'load_detection_model',\n",
       "    'description': 'Loads an OpenCV CascadeClassifier from the given model path.'},\n",
       "   {'id': 'inference.detect_faces',\n",
       "    'file': 'inference.py',\n",
       "    'line_start': 14,\n",
       "    'line_end': 15,\n",
       "    'label': 'detect_faces',\n",
       "    'description': 'Uses a CascadeClassifier to detect faces in a grayscale image array.'},\n",
       "   {'id': 'inference.draw_bounding_box',\n",
       "    'file': 'inference.py',\n",
       "    'line_start': 17,\n",
       "    'line_end': 20,\n",
       "    'label': 'draw_bounding_box',\n",
       "    'description': 'Draws a rectangle on an image array around detected face coordinates.'},\n",
       "   {'id': 'inference.apply_offsets',\n",
       "    'file': 'inference.py',\n",
       "    'line_start': 21,\n",
       "    'line_end': 24,\n",
       "    'label': 'apply_offsets',\n",
       "    'description': 'Applies horizontal and vertical offsets to face coordinates.'},\n",
       "   {'id': 'inference.draw_text',\n",
       "    'file': 'inference.py',\n",
       "    'line_start': 26,\n",
       "    'line_end': 31,\n",
       "    'label': 'draw_text',\n",
       "    'description': 'Renders text onto an image array at specified coordinates.'},\n",
       "   {'id': 'inference.get_colors',\n",
       "    'file': 'inference.py',\n",
       "    'line_start': 33,\n",
       "    'line_end': 36,\n",
       "    'label': 'get_colors',\n",
       "    'description': 'Generates distinct colors for each class using an HSV colormap.'}],\n",
       "  'edges': [{'id': 'e_load_image_keras.preprocessing.image.load_img',\n",
       "    'source': 'inference.load_image',\n",
       "    'target': 'keras.preprocessing.image.load_img',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_load_image_keras.preprocessing.image.img_to_array',\n",
       "    'source': 'inference.load_image',\n",
       "    'target': 'keras.preprocessing.image.img_to_array',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_load_detection_model_cv2.CascadeClassifier',\n",
       "    'source': 'inference.load_detection_model',\n",
       "    'target': 'cv2.CascadeClassifier',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_detect_faces_cv2.CascadeClassifier.detectMultiScale',\n",
       "    'source': 'inference.detect_faces',\n",
       "    'target': 'cv2.CascadeClassifier.detectMultiScale',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_draw_bounding_box_cv2.rectangle',\n",
       "    'source': 'inference.draw_bounding_box',\n",
       "    'target': 'cv2.rectangle',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_draw_text_cv2.putText',\n",
       "    'source': 'inference.draw_text',\n",
       "    'target': 'cv2.putText',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_get_colors_matplotlib.pyplot.cm.hsv',\n",
       "    'source': 'inference.get_colors',\n",
       "    'target': 'matplotlib.pyplot.cm.hsv',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_get_colors_numpy.linspace',\n",
       "    'source': 'inference.get_colors',\n",
       "    'target': 'numpy.linspace',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_get_colors_numpy.asarray',\n",
       "    'source': 'inference.get_colors',\n",
       "    'target': 'numpy.asarray',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/data_augmentation.py': {'nodes': [{'id': 'data_augmentation.ImageGenerator',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 11,\n",
       "    'line_end': 235,\n",
       "    'label': 'ImageGenerator',\n",
       "    'description': 'Class providing image augmentation: color jitter, flips, lighting, cropping, rotation, flow control.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.__init__',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 21,\n",
       "    'line_end': 60,\n",
       "    'label': '__init__',\n",
       "    'description': 'Initialize augmentation parameters and collect active color jitter methods.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator._do_random_crop',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 61,\n",
       "    'line_end': 81,\n",
       "    'label': '_do_random_crop',\n",
       "    'description': 'Apply a random scale and translation crop to the image (classification only).'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.do_random_rotation',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 83,\n",
       "    'line_end': 103,\n",
       "    'label': 'do_random_rotation',\n",
       "    'description': 'Apply random rotation (scaling + translation) to the image (classification only).'},\n",
       "   {'id': 'data_augmentation.ImageGenerator._gray_scale',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 105,\n",
       "    'line_end': 106,\n",
       "    'label': '_gray_scale',\n",
       "    'description': 'Compute per-pixel grayscale luminance.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.saturation',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 108,\n",
       "    'line_end': 114,\n",
       "    'label': 'saturation',\n",
       "    'description': 'Adjust image saturation by blending with grayscale.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.brightness',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 116,\n",
       "    'line_end': 120,\n",
       "    'label': 'brightness',\n",
       "    'description': 'Scale image brightness.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.contrast',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 122,\n",
       "    'line_end': 128,\n",
       "    'label': 'contrast',\n",
       "    'description': 'Adjust image contrast relative to mean grayscale.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.lighting',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 130,\n",
       "    'line_end': 137,\n",
       "    'label': 'lighting',\n",
       "    'description': 'Add PCA-based lighting noise.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.horizontal_flip',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 139,\n",
       "    'line_end': 144,\n",
       "    'label': 'horizontal_flip',\n",
       "    'description': 'Randomly flip image horizontally and adjust box corners.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.vertical_flip',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 146,\n",
       "    'line_end': 151,\n",
       "    'label': 'vertical_flip',\n",
       "    'description': 'Randomly flip image vertically and adjust box corners.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.transform',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 153,\n",
       "    'line_end': 168,\n",
       "    'label': 'transform',\n",
       "    'description': 'Apply color jitters, lighting, and flips to image (and boxes).'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.preprocess_images',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 170,\n",
       "    'line_end': 172,\n",
       "    'label': 'preprocess_images',\n",
       "    'description': 'Wrap call to external preprocess_input.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator.flow',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 173,\n",
       "    'line_end': 232,\n",
       "    'label': 'flow',\n",
       "    'description': 'Main generator loop: read, optionally crop, augment, batch and yield.'},\n",
       "   {'id': 'data_augmentation.ImageGenerator._wrap_in_dictionary',\n",
       "    'file': 'data_augmentation.py',\n",
       "    'line_start': 233,\n",
       "    'line_end': 235,\n",
       "    'label': '_wrap_in_dictionary',\n",
       "    'description': 'Package inputs and targets into dictionaries for model consumption.'}],\n",
       "  'edges': [{'id': 'e_transform__do_random_crop',\n",
       "    'source': 'data_augmentation.ImageGenerator.flow',\n",
       "    'target': 'data_augmentation.ImageGenerator._do_random_crop',\n",
       "    'type': 'call',\n",
       "    'description': 'Apply random crop if enabled.'},\n",
       "   {'id': 'e_flow_imread',\n",
       "    'source': 'data_augmentation.ImageGenerator.flow',\n",
       "    'target': 'preprocessor._imread',\n",
       "    'type': 'call',\n",
       "    'description': 'Load image from disk.'},\n",
       "   {'id': 'e_flow_imresize',\n",
       "    'source': 'data_augmentation.ImageGenerator.flow',\n",
       "    'target': 'preprocessor._imresize',\n",
       "    'type': 'call',\n",
       "    'description': 'Resize image to target size.'},\n",
       "   {'id': 'e_flow_transform',\n",
       "    'source': 'data_augmentation.ImageGenerator.flow',\n",
       "    'target': 'data_augmentation.ImageGenerator.transform',\n",
       "    'type': 'call',\n",
       "    'description': 'Apply augmentations during training/demo.'},\n",
       "   {'id': 'e_flow_preprocess_images',\n",
       "    'source': 'data_augmentation.ImageGenerator.flow',\n",
       "    'target': 'data_augmentation.ImageGenerator.preprocess_images',\n",
       "    'type': 'call',\n",
       "    'description': 'Final preprocessing before yield.'},\n",
       "   {'id': 'e_flow_to_categorical',\n",
       "    'source': 'data_augmentation.ImageGenerator.flow',\n",
       "    'target': 'preprocessor.to_categorical',\n",
       "    'type': 'call',\n",
       "    'description': 'Convert labels to one-hot encoding.'},\n",
       "   {'id': 'e_flow_wrap',\n",
       "    'source': 'data_augmentation.ImageGenerator.flow',\n",
       "    'target': 'data_augmentation.ImageGenerator._wrap_in_dictionary',\n",
       "    'type': 'call',\n",
       "    'description': 'Package batch for output.'},\n",
       "   {'id': 'e_transform_saturation',\n",
       "    'source': 'data_augmentation.ImageGenerator.transform',\n",
       "    'target': 'data_augmentation.ImageGenerator.saturation',\n",
       "    'type': 'call',\n",
       "    'description': 'Randomly adjust saturation.'},\n",
       "   {'id': 'e_transform_brightness',\n",
       "    'source': 'data_augmentation.ImageGenerator.transform',\n",
       "    'target': 'data_augmentation.ImageGenerator.brightness',\n",
       "    'type': 'call',\n",
       "    'description': 'Randomly adjust brightness.'},\n",
       "   {'id': 'e_transform_contrast',\n",
       "    'source': 'data_augmentation.ImageGenerator.transform',\n",
       "    'target': 'data_augmentation.ImageGenerator.contrast',\n",
       "    'type': 'call',\n",
       "    'description': 'Randomly adjust contrast.'},\n",
       "   {'id': 'e_transform_lighting',\n",
       "    'source': 'data_augmentation.ImageGenerator.transform',\n",
       "    'target': 'data_augmentation.ImageGenerator.lighting',\n",
       "    'type': 'call',\n",
       "    'description': 'Add lighting noise.'},\n",
       "   {'id': 'e_transform_hflip',\n",
       "    'source': 'data_augmentation.ImageGenerator.transform',\n",
       "    'target': 'data_augmentation.ImageGenerator.horizontal_flip',\n",
       "    'type': 'call',\n",
       "    'description': 'Random horizontal flip.'},\n",
       "   {'id': 'e_transform_vflip',\n",
       "    'source': 'data_augmentation.ImageGenerator.transform',\n",
       "    'target': 'data_augmentation.ImageGenerator.vertical_flip',\n",
       "    'type': 'call',\n",
       "    'description': 'Random vertical flip.'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/visualizer.py': {'nodes': [{'id': 'visualizer.make_mosaic',\n",
       "    'file': 'visualizer.py',\n",
       "    'line_start': 8,\n",
       "    'line_end': 24,\n",
       "    'label': 'make_mosaic',\n",
       "    'description': 'Arrange a set of images into a mosaic with optional border.'},\n",
       "   {'id': 'visualizer.make_mosaic_v2',\n",
       "    'file': 'visualizer.py',\n",
       "    'line_start': 27,\n",
       "    'line_end': 50,\n",
       "    'label': 'make_mosaic_v2',\n",
       "    'description': 'Alternate implementation to assemble images into a grid mosaic.'},\n",
       "   {'id': 'visualizer.pretty_imshow',\n",
       "    'file': 'visualizer.py',\n",
       "    'line_start': 53,\n",
       "    'line_end': 66,\n",
       "    'label': 'pretty_imshow',\n",
       "    'description': 'Display an image with a colorbar alongside using a specified colormap.'},\n",
       "   {'id': 'visualizer.normal_imshow',\n",
       "    'file': 'visualizer.py',\n",
       "    'line_start': 68,\n",
       "    'line_end': 80,\n",
       "    'label': 'normal_imshow',\n",
       "    'description': 'Display an image normally, optionally turning off axes.'},\n",
       "   {'id': 'visualizer.display_image',\n",
       "    'file': 'visualizer.py',\n",
       "    'line_start': 83,\n",
       "    'line_end': 100,\n",
       "    'label': 'display_image',\n",
       "    'description': 'Show a single image with optional class title and pretty or normal display.'},\n",
       "   {'id': 'visualizer.draw_mosaic',\n",
       "    'file': 'visualizer.py',\n",
       "    'line_start': 102,\n",
       "    'line_end': 127,\n",
       "    'label': 'draw_mosaic',\n",
       "    'description': 'Draw multiple images in a subplot grid with optional class titles.'},\n",
       "   {'id': 'visualizer.__main__',\n",
       "    'file': 'visualizer.py',\n",
       "    'line_start': 129,\n",
       "    'line_end': 177,\n",
       "    'label': '__main__',\n",
       "    'description': 'Script entry point: load data, create mosaics, visualize kernels of a trained model.'}],\n",
       "  'edges': [{'id': 'e___main___make_mosaic',\n",
       "    'source': 'visualizer.__main__',\n",
       "    'target': 'visualizer.make_mosaic',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e___main___pretty_imshow',\n",
       "    'source': 'visualizer.__main__',\n",
       "    'target': 'visualizer.pretty_imshow',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e___main___get_labels',\n",
       "    'source': 'visualizer.__main__',\n",
       "    'target': 'utils.utils.get_labels',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e___main___load_model',\n",
       "    'source': 'visualizer.__main__',\n",
       "    'target': 'keras.models.load_model',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e_display_image_pretty_imshow',\n",
       "    'source': 'visualizer.display_image',\n",
       "    'target': 'visualizer.pretty_imshow',\n",
       "    'type': 'call'}]},\n",
       " '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/models/cnn.py': {'nodes': [{'id': 'cnn.simple_CNN',\n",
       "    'file': 'cnn.py',\n",
       "    'line_start': 14,\n",
       "    'line_end': 56,\n",
       "    'label': 'simple_CNN',\n",
       "    'description': 'Builds a simple CNN model using Keras Sequential API.'},\n",
       "   {'id': 'cnn.simpler_CNN',\n",
       "    'file': 'cnn.py',\n",
       "    'line_start': 59,\n",
       "    'line_end': 108,\n",
       "    'label': 'simpler_CNN',\n",
       "    'description': 'Builds a more compact CNN model with down-sampling via strides.'},\n",
       "   {'id': 'cnn.tiny_XCEPTION',\n",
       "    'file': 'cnn.py',\n",
       "    'line_start': 111,\n",
       "    'line_end': 205,\n",
       "    'label': 'tiny_XCEPTION',\n",
       "    'description': 'Constructs a tiny Xception-style model with separable convolutions and residual connections.'},\n",
       "   {'id': 'cnn.mini_XCEPTION',\n",
       "    'file': 'cnn.py',\n",
       "    'line_start': 207,\n",
       "    'line_end': 301,\n",
       "    'label': 'mini_XCEPTION',\n",
       "    'description': 'Constructs a mini Xception-style model with smaller channel sizes.'},\n",
       "   {'id': 'cnn.big_XCEPTION',\n",
       "    'file': 'cnn.py',\n",
       "    'line_start': 303,\n",
       "    'line_end': 345,\n",
       "    'label': 'big_XCEPTION',\n",
       "    'description': 'Constructs a larger Xception-style model with higher channel counts.'},\n",
       "   {'id': 'cnn.__main__',\n",
       "    'file': 'cnn.py',\n",
       "    'line_start': 348,\n",
       "    'line_end': 358,\n",
       "    'label': '__main__',\n",
       "    'description': 'Script entry point that selects and summaries a model.'}],\n",
       "  'edges': [{'id': 'e___main___simple_CNN',\n",
       "    'source': 'cnn.__main__',\n",
       "    'target': 'cnn.simple_CNN',\n",
       "    'type': 'call'},\n",
       "   {'id': 'e___main___Sequential.summary',\n",
       "    'source': 'cnn.__main__',\n",
       "    'target': 'keras.models.Sequential.summary',\n",
       "    'type': 'call'}]}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = await client.generate_control_flow_graphs_for_directory(test2_dir, file_type)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8825c1a6",
   "metadata": {},
   "source": [
    "## 결과 확인 및 파일별 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62bfb070",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_gradcam_demo.py ===\n",
      "{'nodes': [{'id': 'image_gradcam_demo.main', 'file': 'image_gradcam_demo.py', 'line_start': 1, 'line_end': 85, 'label': 'main', 'description': 'Top-level script execution entry point.'}], 'edges': [{'id': 'e_main_get_labels', 'source': 'image_gradcam_demo.main', 'target': 'utils.datasets.get_labels', 'type': 'call'}, {'id': 'e_main_load_model', 'source': 'image_gradcam_demo.main', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_main_load_detection_model', 'source': 'image_gradcam_demo.main', 'target': 'utils.inference.load_detection_model', 'type': 'call'}, {'id': 'e_main_load_image_gray', 'source': 'image_gradcam_demo.main', 'target': 'utils.inference.load_image', 'type': 'call'}, {'id': 'e_main_load_image_rgb', 'source': 'image_gradcam_demo.main', 'target': 'utils.inference.load_image', 'type': 'call'}, {'id': 'e_main_detect_faces', 'source': 'image_gradcam_demo.main', 'target': 'utils.inference.detect_faces', 'type': 'call'}, {'id': 'e_main_apply_offsets_1', 'source': 'image_gradcam_demo.main', 'target': 'utils.inference.apply_offsets', 'type': 'call'}, {'id': 'e_main_apply_offsets_2', 'source': 'image_gradcam_demo.main', 'target': 'utils.inference.apply_offsets', 'type': 'call'}, {'id': 'e_main_preprocess_input', 'source': 'image_gradcam_demo.main', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call'}, {'id': 'e_main_compile_gradient_function', 'source': 'image_gradcam_demo.main', 'target': 'utils.grad_cam.compile_gradient_function', 'type': 'call'}, {'id': 'e_main_register_gradient', 'source': 'image_gradcam_demo.main', 'target': 'utils.grad_cam.register_gradient', 'type': 'call'}, {'id': 'e_main_modify_backprop', 'source': 'image_gradcam_demo.main', 'target': 'utils.grad_cam.modify_backprop', 'type': 'call'}, {'id': 'e_main_compile_saliency_function', 'source': 'image_gradcam_demo.main', 'target': 'utils.grad_cam.compile_saliency_function', 'type': 'call'}, {'id': 'e_main_calculate_guided_gradient_CAM', 'source': 'image_gradcam_demo.main', 'target': 'utils.grad_cam.calculate_guided_gradient_CAM', 'type': 'call'}, {'id': 'e_main_draw_bounding_box', 'source': 'image_gradcam_demo.main', 'target': 'utils.inference.draw_bounding_box', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_gender_classifier.py ===\n",
      "{'nodes': [{'id': 'train_gender_classifier.main', 'file': 'train_gender_classifier.py', 'line_start': 1, 'line_end': 71, 'label': 'main', 'description': 'Top-level script that loads data, builds and trains the gender classification model.'}], 'edges': [{'id': 'e_main_utils.datasets.DataManager', 'source': 'train_gender_classifier.main', 'target': 'utils.datasets.DataManager', 'type': 'call'}, {'id': 'e_main_utils.datasets.DataManager.get_data', 'source': 'train_gender_classifier.main', 'target': 'utils.datasets.DataManager.get_data', 'type': 'call'}, {'id': 'e_main_utils.datasets.split_imdb_data', 'source': 'train_gender_classifier.main', 'target': 'utils.datasets.split_imdb_data', 'type': 'call'}, {'id': 'e_main_utils.data_augmentation.ImageGenerator', 'source': 'train_gender_classifier.main', 'target': 'utils.data_augmentation.ImageGenerator', 'type': 'call'}, {'id': 'e_main_models.cnn.mini_XCEPTION', 'source': 'train_gender_classifier.main', 'target': 'models.cnn.mini_XCEPTION', 'type': 'call'}, {'id': 'e_main_model.compile', 'source': 'train_gender_classifier.main', 'target': 'model.compile', 'type': 'call'}, {'id': 'e_main_model.summary', 'source': 'train_gender_classifier.main', 'target': 'model.summary', 'type': 'call'}, {'id': 'e_main_keras.callbacks.EarlyStopping', 'source': 'train_gender_classifier.main', 'target': 'keras.callbacks.EarlyStopping', 'type': 'call'}, {'id': 'e_main_keras.callbacks.ReduceLROnPlateau', 'source': 'train_gender_classifier.main', 'target': 'keras.callbacks.ReduceLROnPlateau', 'type': 'call'}, {'id': 'e_main_keras.callbacks.CSVLogger', 'source': 'train_gender_classifier.main', 'target': 'keras.callbacks.CSVLogger', 'type': 'call'}, {'id': 'e_main_keras.callbacks.ModelCheckpoint', 'source': 'train_gender_classifier.main', 'target': 'keras.callbacks.ModelCheckpoint', 'type': 'call'}, {'id': 'e_main_image_generator.flow', 'source': 'train_gender_classifier.main', 'target': 'utils.data_augmentation.ImageGenerator.flow', 'type': 'call'}, {'id': 'e_main_model.fit_generator', 'source': 'train_gender_classifier.main', 'target': 'model.fit_generator', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_gender_demo.py ===\n",
      "{'nodes': [{'id': 'video_emotion_gender_demo.main', 'file': 'video_emotion_gender_demo.py', 'line_start': 1, 'line_end': 102, 'label': 'main', 'description': 'Main script entry point: loads models and runs the video processing loop.'}], 'edges': [{'id': 'e_main_utils.datasets.get_labels_fer2013', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.datasets.get_labels', 'type': 'call'}, {'id': 'e_main_utils.datasets.get_labels_imdb', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.datasets.get_labels', 'type': 'call'}, {'id': 'e_main_utils.inference.load_detection_model', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.inference.load_detection_model', 'type': 'call'}, {'id': 'e_main_keras.models.load_model_emotion', 'source': 'video_emotion_gender_demo.main', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_main_keras.models.load_model_gender', 'source': 'video_emotion_gender_demo.main', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_main_cv2.namedWindow', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.namedWindow', 'type': 'call'}, {'id': 'e_main_cv2.VideoCapture', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.VideoCapture', 'type': 'call'}, {'id': 'e_main_detect_faces', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.inference.detect_faces', 'type': 'call'}, {'id': 'e_main_apply_offsets_gender', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.inference.apply_offsets', 'type': 'call'}, {'id': 'e_main_apply_offsets_emotion', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.inference.apply_offsets', 'type': 'call'}, {'id': 'e_main_cv2.resize_rgb_face', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e_main_cv2.resize_gray_face', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e_main_utils.preprocessor.preprocess_input_gray', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call'}, {'id': 'e_main_np.expand_dims_1', 'source': 'video_emotion_gender_demo.main', 'target': 'numpy.expand_dims', 'type': 'call'}, {'id': 'e_main_np.expand_dims_2', 'source': 'video_emotion_gender_demo.main', 'target': 'numpy.expand_dims', 'type': 'call'}, {'id': 'e_main_emotion_classifier.predict', 'source': 'video_emotion_gender_demo.main', 'target': 'keras.models.Model.predict', 'type': 'call'}, {'id': 'e_main_np.argmax_emotion', 'source': 'video_emotion_gender_demo.main', 'target': 'numpy.argmax', 'type': 'call'}, {'id': 'e_main_np.argmax_gender', 'source': 'video_emotion_gender_demo.main', 'target': 'numpy.argmax', 'type': 'call'}, {'id': 'e_main_utils.preprocessor.preprocess_input_rgb', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call'}, {'id': 'e_main_gender_classifier.predict', 'source': 'video_emotion_gender_demo.main', 'target': 'keras.models.Model.predict', 'type': 'call'}, {'id': 'e_main_statistics.mode_emotion', 'source': 'video_emotion_gender_demo.main', 'target': 'statistics.mode', 'type': 'call'}, {'id': 'e_main_statistics.mode_gender', 'source': 'video_emotion_gender_demo.main', 'target': 'statistics.mode', 'type': 'call'}, {'id': 'e_main_utils.inference.draw_bounding_box', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.inference.draw_bounding_box', 'type': 'call'}, {'id': 'e_main_utils.inference.draw_text_gender', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.inference.draw_text', 'type': 'call'}, {'id': 'e_main_utils.inference.draw_text_emotion', 'source': 'video_emotion_gender_demo.main', 'target': 'utils.inference.draw_text', 'type': 'call'}, {'id': 'e_main_cv2.cvtColor_gray', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.cvtColor', 'type': 'call'}, {'id': 'e_main_cv2.cvtColor_bgr', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.cvtColor', 'type': 'call'}, {'id': 'e_main_cv2.imshow', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.imshow', 'type': 'call'}, {'id': 'e_main_cv2.waitKey', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.waitKey', 'type': 'call'}, {'id': 'e_main_video_capture.release', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.VideoCapture.release', 'type': 'call'}, {'id': 'e_main_cv2.destroyAllWindows', 'source': 'video_emotion_gender_demo.main', 'target': 'cv2.destroyAllWindows', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_emotion_color_demo.py ===\n",
      "{'nodes': [{'id': 'video_emotion_color_demo.main', 'file': 'video_emotion_color_demo.py', 'line_start': 1, 'line_end': 91, 'label': 'main', 'description': 'Top-level script executing video capture, face detection, emotion recognition, and annotated display loop.'}], 'edges': [{'id': 'e_main_utils.datasets.get_labels', 'source': 'video_emotion_color_demo.main', 'target': 'utils.datasets.get_labels', 'type': 'call', 'description': 'Load emotion labels from dataset definitions'}, {'id': 'e_main_keras.models.load_model', 'source': 'video_emotion_color_demo.main', 'target': 'keras.models.load_model', 'type': 'call', 'description': 'Load the pretrained emotion classification model'}, {'id': 'e_main_utils.inference.load_detection_model', 'source': 'video_emotion_color_demo.main', 'target': 'utils.inference.load_detection_model', 'type': 'call', 'description': 'Load the face detection model (Haar cascade)'}, {'id': 'e_main_utils.inference.detect_faces', 'source': 'video_emotion_color_demo.main', 'target': 'utils.inference.detect_faces', 'type': 'call', 'description': 'Detect face bounding boxes in a grayscale frame'}, {'id': 'e_main_utils.inference.apply_offsets', 'source': 'video_emotion_color_demo.main', 'target': 'utils.inference.apply_offsets', 'type': 'call', 'description': 'Apply offsets to face coordinates for emotion ROI'}, {'id': 'e_main_utils.preprocessor.preprocess_input', 'source': 'video_emotion_color_demo.main', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call', 'description': 'Normalize and scale the face image for model input'}, {'id': 'e_main_utils.inference.draw_bounding_box', 'source': 'video_emotion_color_demo.main', 'target': 'utils.inference.draw_bounding_box', 'type': 'call', 'description': 'Draw the colored rectangle around detected face'}, {'id': 'e_main_utils.inference.draw_text', 'source': 'video_emotion_color_demo.main', 'target': 'utils.inference.draw_text', 'type': 'call', 'description': 'Render the emotion label text on the image'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/video_gradcam_demo.py ===\n",
      "{'nodes': [{'id': 'video_gradcam_demo.main', 'file': 'video_gradcam_demo.py', 'line_start': 1, 'line_end': 92, 'label': 'main', 'description': 'Main script executing the video Grad-CAM demo.'}], 'edges': [{'id': 'e_main_load_model', 'source': 'video_gradcam_demo.main', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_main_compile_gradient_function', 'source': 'video_gradcam_demo.main', 'target': 'utils.grad_cam.compile_gradient_function', 'type': 'call'}, {'id': 'e_main_register_gradient', 'source': 'video_gradcam_demo.main', 'target': 'utils.grad_cam.register_gradient', 'type': 'call'}, {'id': 'e_main_modify_backprop', 'source': 'video_gradcam_demo.main', 'target': 'utils.grad_cam.modify_backprop', 'type': 'call'}, {'id': 'e_main_compile_saliency_function', 'source': 'video_gradcam_demo.main', 'target': 'utils.grad_cam.compile_saliency_function', 'type': 'call'}, {'id': 'e_main_load_detection_model', 'source': 'video_gradcam_demo.main', 'target': 'utils.inference.load_detection_model', 'type': 'call'}, {'id': 'e_main_detect_faces', 'source': 'video_gradcam_demo.main', 'target': 'utils.inference.detect_faces', 'type': 'call'}, {'id': 'e_main_apply_offsets', 'source': 'video_gradcam_demo.main', 'target': 'utils.inference.apply_offsets', 'type': 'call'}, {'id': 'e_main_preprocess_input', 'source': 'video_gradcam_demo.main', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call'}, {'id': 'e_main_calculate_guided_gradient_CAM', 'source': 'video_gradcam_demo.main', 'target': 'utils.grad_cam.calculate_guided_gradient_CAM', 'type': 'call'}, {'id': 'e_main_draw_bounding_box', 'source': 'video_gradcam_demo.main', 'target': 'utils.inference.draw_bounding_box', 'type': 'call'}, {'id': 'e_main_namedWindow', 'source': 'video_gradcam_demo.main', 'target': 'cv2.namedWindow', 'type': 'call'}, {'id': 'e_main_VideoCapture', 'source': 'video_gradcam_demo.main', 'target': 'cv2.VideoCapture', 'type': 'call'}, {'id': 'e_main_read', 'source': 'video_gradcam_demo.main', 'target': 'cv2.VideoCapture.read', 'type': 'call'}, {'id': 'e_main_cvtColor', 'source': 'video_gradcam_demo.main', 'target': 'cv2.cvtColor', 'type': 'call'}, {'id': 'e_main_resize', 'source': 'video_gradcam_demo.main', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e_main_expand_dims', 'source': 'video_gradcam_demo.main', 'target': 'numpy.expand_dims', 'type': 'call'}, {'id': 'e_main_repeat', 'source': 'video_gradcam_demo.main', 'target': 'numpy.repeat', 'type': 'call'}, {'id': 'e_main_imshow', 'source': 'video_gradcam_demo.main', 'target': 'cv2.imshow', 'type': 'call'}, {'id': 'e_main_waitKey', 'source': 'video_gradcam_demo.main', 'target': 'cv2.waitKey', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/train_emotion_classifier.py ===\n",
      "{'nodes': [{'id': 'train_emotion_classifier.main', 'file': 'train_emotion_classifier.py', 'line_start': 1, 'line_end': 72, 'label': 'main', 'description': 'Top‐level script that configures data generators, builds and trains the emotion classification model.'}], 'edges': [{'id': 'e_main_to_ImageDataGenerator', 'source': 'train_emotion_classifier.main', 'target': 'keras.preprocessing.image.ImageDataGenerator', 'type': 'call', 'description': 'Instantiate image data generator for augmentation.'}, {'id': 'e_main_to_mini_XCEPTION', 'source': 'train_emotion_classifier.main', 'target': 'models.cnn.mini_XCEPTION', 'type': 'call', 'description': 'Create the mini_XCEPTION convolutional model.'}, {'id': 'e_main_to_Model_compile', 'source': 'train_emotion_classifier.main', 'target': 'keras.models.Model.compile', 'type': 'call', 'description': 'Compile the model with optimizer, loss, and metrics.'}, {'id': 'e_main_to_Model_summary', 'source': 'train_emotion_classifier.main', 'target': 'keras.models.Model.summary', 'type': 'call', 'description': 'Print model architecture summary.'}, {'id': 'e_main_to_CSVLogger', 'source': 'train_emotion_classifier.main', 'target': 'keras.callbacks.CSVLogger', 'type': 'call', 'description': 'Create CSV logger callback.'}, {'id': 'e_main_to_ModelCheckpoint', 'source': 'train_emotion_classifier.main', 'target': 'keras.callbacks.ModelCheckpoint', 'type': 'call', 'description': 'Create model checkpoint callback.'}, {'id': 'e_main_to_EarlyStopping', 'source': 'train_emotion_classifier.main', 'target': 'keras.callbacks.EarlyStopping', 'type': 'call', 'description': 'Create early stopping callback.'}, {'id': 'e_main_to_ReduceLROnPlateau', 'source': 'train_emotion_classifier.main', 'target': 'keras.callbacks.ReduceLROnPlateau', 'type': 'call', 'description': 'Create learning rate reduction callback.'}, {'id': 'e_main_to_DataManager', 'source': 'train_emotion_classifier.main', 'target': 'utils.datasets.DataManager', 'type': 'call', 'description': 'Instantiate data manager for loading dataset.'}, {'id': 'e_main_to_get_data', 'source': 'train_emotion_classifier.main', 'target': 'utils.datasets.DataManager.get_data', 'type': 'call', 'description': 'Load faces and emotion labels.'}, {'id': 'e_main_to_preprocess_input', 'source': 'train_emotion_classifier.main', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call', 'description': 'Apply preprocessing to face images.'}, {'id': 'e_main_to_split_data', 'source': 'train_emotion_classifier.main', 'target': 'utils.datasets.split_data', 'type': 'call', 'description': 'Split dataset into training and validation sets.'}, {'id': 'e_main_to_flow', 'source': 'train_emotion_classifier.main', 'target': 'keras.preprocessing.image.ImageDataGenerator.flow', 'type': 'call', 'description': 'Generate augmented batches from training data.'}, {'id': 'e_main_to_fit_generator', 'source': 'train_emotion_classifier.main', 'target': 'keras.models.Model.fit_generator', 'type': 'call', 'description': 'Train the model using a data generator.'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/image_emotion_gender_demo.py ===\n",
      "{'nodes': [{'id': 'image_emotion_gender_demo.main', 'file': 'image_emotion_gender_demo.py', 'line_start': 1, 'line_end': 82, 'label': 'main', 'description': 'Script entry point that loads models, processes the image, performs face detection, emotion and gender prediction, and outputs the annotated image.'}], 'edges': [{'id': 'e_main_utils.datasets.get_labels', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.datasets.get_labels', 'type': 'call'}, {'id': 'e_main_keras.models.load_model', 'source': 'image_emotion_gender_demo.main', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_main_utils.inference.load_detection_model', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.inference.load_detection_model', 'type': 'call'}, {'id': 'e_main_utils.inference.load_image', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.inference.load_image', 'type': 'call'}, {'id': 'e_main_utils.inference.detect_faces', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.inference.detect_faces', 'type': 'call'}, {'id': 'e_main_utils.inference.apply_offsets', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.inference.apply_offsets', 'type': 'call'}, {'id': 'e_main_utils.preprocessor.preprocess_input', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call'}, {'id': 'e_main_utils.inference.draw_bounding_box', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.inference.draw_bounding_box', 'type': 'call'}, {'id': 'e_main_utils.inference.draw_text', 'source': 'image_emotion_gender_demo.main', 'target': 'utils.inference.draw_text', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/faces.py ===\n",
      "{'nodes': [{'id': 'faces.index', 'file': 'faces.py', 'line_start': 8, 'line_end': 10, 'label': 'index', 'description': 'Redirects root URL to external site.'}, {'id': 'faces.upload', 'file': 'faces.py', 'line_start': 12, 'line_end': 20, 'label': 'upload', 'description': 'Handles image upload, processes it, and returns the annotated image.'}, {'id': 'faces.bad_request', 'file': 'faces.py', 'line_start': 22, 'line_end': 24, 'label': 'bad_request', 'description': 'Custom handler for 400 errors, returns JSON error message.'}, {'id': 'faces.not_found', 'file': 'faces.py', 'line_start': 26, 'line_end': 28, 'label': 'not_found', 'description': 'Custom handler for 404 errors, returns JSON error message.'}, {'id': 'faces.main', 'file': 'faces.py', 'line_start': 30, 'line_end': 31, 'label': 'main', 'description': 'Starts the Flask application when run as a script.'}], 'edges': [{'id': 'e_faces.index_flask.redirect', 'source': 'faces.index', 'target': 'flask.redirect', 'type': 'call', 'description': 'Redirect to external URL.'}, {'id': 'e_faces.upload_emotion_gender_processor.process_image', 'source': 'faces.upload', 'target': 'emotion_gender_processor.process_image', 'type': 'call', 'description': 'Process the uploaded image.'}, {'id': 'e_faces.upload_flask.send_file', 'source': 'faces.upload', 'target': 'flask.send_file', 'type': 'call', 'description': 'Send back the processed image file.'}, {'id': 'e_faces.upload_flask.abort', 'source': 'faces.upload', 'target': 'flask.abort', 'type': 'call', 'description': 'Abort the request on error.'}, {'id': 'e_faces.bad_request_flask.make_response', 'source': 'faces.bad_request', 'target': 'flask.make_response', 'type': 'call', 'description': 'Construct the HTTP response for 400 errors.'}, {'id': 'e_faces.bad_request_flask.jsonify', 'source': 'faces.bad_request', 'target': 'flask.jsonify', 'type': 'call', 'description': 'Serialize error message to JSON.'}, {'id': 'e_faces.not_found_flask.make_response', 'source': 'faces.not_found', 'target': 'flask.make_response', 'type': 'call', 'description': 'Construct the HTTP response for 404 errors.'}, {'id': 'e_faces.not_found_flask.jsonify', 'source': 'faces.not_found', 'target': 'flask.jsonify', 'type': 'call', 'description': 'Serialize error message to JSON.'}, {'id': 'e_faces.main_flask.Flask.run', 'source': 'faces.main', 'target': 'flask.Flask.run', 'type': 'call', 'description': 'Run the Flask development server.'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/web/emotion_gender_processor.py ===\n",
      "{'nodes': [{'id': 'emotion_gender_processor.process_image', 'file': 'emotion_gender_processor.py', 'line_start': 18, 'line_end': 87, 'label': 'process_image', 'description': 'Processes an input image: loads models, detects faces, predicts gender and emotion, and annotates the image.'}], 'edges': [{'id': 'e_process_image_utils.datasets.get_labels', 'source': 'emotion_gender_processor.process_image', 'target': 'utils.datasets.get_labels', 'type': 'call'}, {'id': 'e_process_image_keras.models.load_model', 'source': 'emotion_gender_processor.process_image', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_process_image_utils.inference.load_detection_model', 'source': 'emotion_gender_processor.process_image', 'target': 'utils.inference.load_detection_model', 'type': 'call'}, {'id': 'e_process_image_numpy.fromstring', 'source': 'emotion_gender_processor.process_image', 'target': 'numpy.fromstring', 'type': 'call'}, {'id': 'e_process_image_cv2.imdecode', 'source': 'emotion_gender_processor.process_image', 'target': 'cv2.imdecode', 'type': 'call'}, {'id': 'e_process_image_cv2.cvtColor', 'source': 'emotion_gender_processor.process_image', 'target': 'cv2.cvtColor', 'type': 'call'}, {'id': 'e_process_image_utils.inference.detect_faces', 'source': 'emotion_gender_processor.process_image', 'target': 'utils.inference.detect_faces', 'type': 'call'}, {'id': 'e_process_image_utils.inference.apply_offsets', 'source': 'emotion_gender_processor.process_image', 'target': 'utils.inference.apply_offsets', 'type': 'call'}, {'id': 'e_process_image_cv2.resize', 'source': 'emotion_gender_processor.process_image', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e_process_image_utils.preprocessor.preprocess_input', 'source': 'emotion_gender_processor.process_image', 'target': 'utils.preprocessor.preprocess_input', 'type': 'call'}, {'id': 'e_process_image_numpy.expand_dims', 'source': 'emotion_gender_processor.process_image', 'target': 'numpy.expand_dims', 'type': 'call'}, {'id': 'e_process_image_gender_classifier.predict', 'source': 'emotion_gender_processor.process_image', 'target': 'gender_classifier.predict', 'type': 'call'}, {'id': 'e_process_image_numpy.argmax', 'source': 'emotion_gender_processor.process_image', 'target': 'numpy.argmax', 'type': 'call'}, {'id': 'e_process_image_emotion_classifier.predict', 'source': 'emotion_gender_processor.process_image', 'target': 'emotion_classifier.predict', 'type': 'call'}, {'id': 'e_process_image_utils.inference.draw_bounding_box', 'source': 'emotion_gender_processor.process_image', 'target': 'utils.inference.draw_bounding_box', 'type': 'call'}, {'id': 'e_process_image_utils.inference.draw_text', 'source': 'emotion_gender_processor.process_image', 'target': 'utils.inference.draw_text', 'type': 'call'}, {'id': 'e_process_image_cv2.imwrite', 'source': 'emotion_gender_processor.process_image', 'target': 'cv2.imwrite', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/datasets.py ===\n",
      "{'nodes': [{'id': 'datasets.DataManager.__init__', 'file': 'datasets.py', 'line_start': 12, 'line_end': 28, 'label': '__init__', 'description': 'Initializes DataManager, sets dataset_name, dataset_path, image_size, and resolves default paths.'}, {'id': 'datasets.DataManager.get_data', 'file': 'datasets.py', 'line_start': 30, 'line_end': 37, 'label': 'get_data', 'description': 'Selects and invokes the appropriate dataset loading method based on dataset_name.'}, {'id': 'datasets.DataManager._load_imdb', 'file': 'datasets.py', 'line_start': 39, 'line_end': 57, 'label': '_load_imdb', 'description': 'Loads and filters the IMDB dataset from a .mat file, returning a dict of image paths to gender labels.'}, {'id': 'datasets.DataManager._load_fer2013', 'file': 'datasets.py', 'line_start': 59, 'line_end': 72, 'label': '_load_fer2013', 'description': 'Loads the FER2013 CSV dataset, processes pixel strings into resized images and one-hot emotion labels.'}, {'id': 'datasets.DataManager._load_KDEF', 'file': 'datasets.py', 'line_start': 74, 'line_end': 102, 'label': '_load_KDEF', 'description': 'Traverses KDEF image folders, reads and resizes grayscale images, maps filenames to emotion classes.'}, {'id': 'datasets.get_labels', 'file': 'datasets.py', 'line_start': 105, 'line_end': 114, 'label': 'get_labels', 'description': 'Returns a mapping from numeric label to class name for a given dataset_name.'}, {'id': 'datasets.get_class_to_arg', 'file': 'datasets.py', 'line_start': 117, 'line_end': 126, 'label': 'get_class_to_arg', 'description': 'Returns a mapping from class name to numeric index for a given dataset_name.'}, {'id': 'datasets.split_imdb_data', 'file': 'datasets.py', 'line_start': 129, 'line_end': 137, 'label': 'split_imdb_data', 'description': 'Splits IMDB ground truth dict keys into training and validation lists, optionally shuffling.'}, {'id': 'datasets.split_data', 'file': 'datasets.py', 'line_start': 140, 'line_end': 149, 'label': 'split_data', 'description': 'Splits arrays x and y into training and validation subsets based on validation_split.'}], 'edges': [{'id': 'e_get_data__load_imdb', 'source': 'datasets.DataManager.get_data', 'target': 'datasets.DataManager._load_imdb', 'type': 'call'}, {'id': 'e_get_data__load_fer2013', 'source': 'datasets.DataManager.get_data', 'target': 'datasets.DataManager._load_fer2013', 'type': 'call'}, {'id': 'e_get_data__load_KDEF', 'source': 'datasets.DataManager.get_data', 'target': 'datasets.DataManager._load_KDEF', 'type': 'call'}, {'id': 'e__load_imdb_loadmat', 'source': 'datasets.DataManager._load_imdb', 'target': 'scipy.io.loadmat', 'type': 'call'}, {'id': 'e__load_imdb_numpy.isnan', 'source': 'datasets.DataManager._load_imdb', 'target': 'numpy.isnan', 'type': 'call'}, {'id': 'e__load_imdb_numpy.logical_not', 'source': 'datasets.DataManager._load_imdb', 'target': 'numpy.logical_not', 'type': 'call'}, {'id': 'e__load_imdb_numpy.logical_and', 'source': 'datasets.DataManager._load_imdb', 'target': 'numpy.logical_and', 'type': 'call'}, {'id': 'e__load_fer2013_pandas.read_csv', 'source': 'datasets.DataManager._load_fer2013', 'target': 'pandas.read_csv', 'type': 'call'}, {'id': 'e__load_fer2013_numpy.asarray', 'source': 'datasets.DataManager._load_fer2013', 'target': 'numpy.asarray', 'type': 'call'}, {'id': 'e__load_fer2013_cv2.resize', 'source': 'datasets.DataManager._load_fer2013', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e__load_fer2013_numpy.expand_dims', 'source': 'datasets.DataManager._load_fer2013', 'target': 'numpy.expand_dims', 'type': 'call'}, {'id': 'e__load_fer2013_pandas.get_dummies', 'source': 'datasets.DataManager._load_fer2013', 'target': 'pandas.get_dummies', 'type': 'call'}, {'id': 'e__load_KDEF_get_class_to_arg', 'source': 'datasets.DataManager._load_KDEF', 'target': 'datasets.get_class_to_arg', 'type': 'call'}, {'id': 'e__load_KDEF_os.walk', 'source': 'datasets.DataManager._load_KDEF', 'target': 'os.walk', 'type': 'call'}, {'id': 'e__load_KDEF_os.path.join', 'source': 'datasets.DataManager._load_KDEF', 'target': 'os.path.join', 'type': 'call'}, {'id': 'e__load_KDEF_cv2.imread', 'source': 'datasets.DataManager._load_KDEF', 'target': 'cv2.imread', 'type': 'call'}, {'id': 'e__load_KDEF_cv2.resize', 'source': 'datasets.DataManager._load_KDEF', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e__load_KDEF_os.path.basename', 'source': 'datasets.DataManager._load_KDEF', 'target': 'os.path.basename', 'type': 'call'}, {'id': 'e_split_imdb_data_shuffle', 'source': 'datasets.split_imdb_data', 'target': 'random.shuffle', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/preprocessor.py ===\n",
      "{'nodes': [{'id': 'preprocessor.preprocess_input', 'file': 'preprocessor.py', 'line_start': 5, 'line_end': 11, 'label': 'preprocess_input', 'description': 'Convert image array to float, normalize to [0,1], optionally scale to [-1,1]'}, {'id': 'preprocessor._imread', 'file': 'preprocessor.py', 'line_start': 14, 'line_end': 16, 'label': '_imread', 'description': 'Wrapper around scipy.misc.imread to read an image from disk'}, {'id': 'preprocessor._imresize', 'file': 'preprocessor.py', 'line_start': 18, 'line_end': 19, 'label': '_imresize', 'description': 'Wrapper around scipy.misc.imresize to resize an image array'}, {'id': 'preprocessor.to_categorical', 'file': 'preprocessor.py', 'line_start': 22, 'line_end': 27, 'label': 'to_categorical', 'description': 'Convert integer class labels to one-hot encoded (categorical) format'}], 'edges': [{'id': 'e_preprocess_input_imread', 'source': 'preprocessor._imread', 'target': 'scipy.misc.imread', 'type': 'call'}, {'id': 'e__imresize_imresize', 'source': 'preprocessor._imresize', 'target': 'scipy.misc.imresize', 'type': 'call'}, {'id': 'e_to_categorical_np_asarray', 'source': 'preprocessor.to_categorical', 'target': 'numpy.asarray', 'type': 'call'}, {'id': 'e_to_categorical_np_zeros', 'source': 'preprocessor.to_categorical', 'target': 'numpy.zeros', 'type': 'call'}, {'id': 'e_to_categorical_np_arange', 'source': 'preprocessor.to_categorical', 'target': 'numpy.arange', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/grad_cam.py ===\n",
      "{'nodes': [{'id': 'grad_cam.reset_optimizer_weights', 'file': 'grad_cam.py', 'line_start': 15, 'line_end': 18, 'label': 'reset_optimizer_weights', 'description': 'Remove optimizer weights from an HDF5 model file.'}, {'id': 'grad_cam.target_category_loss', 'file': 'grad_cam.py', 'line_start': 21, 'line_end': 22, 'label': 'target_category_loss', 'description': 'Apply one-hot mask to model output for a target category.'}, {'id': 'grad_cam.target_category_loss_output_shape', 'file': 'grad_cam.py', 'line_start': 25, 'line_end': 26, 'label': 'target_category_loss_output_shape', 'description': 'Return output shape unchanged for Lambda layer.'}, {'id': 'grad_cam.normalize', 'file': 'grad_cam.py', 'line_start': 29, 'line_end': 31, 'label': 'normalize', 'description': 'Normalize a tensor by its L2 norm.'}, {'id': 'grad_cam.load_image', 'file': 'grad_cam.py', 'line_start': 34, 'line_end': 37, 'label': 'load_image', 'description': 'Expand dims and preprocess input image array.'}, {'id': 'grad_cam.register_gradient', 'file': 'grad_cam.py', 'line_start': 40, 'line_end': 47, 'label': 'register_gradient', 'description': 'Register guided backpropagation gradient override in TensorFlow.'}, {'id': 'grad_cam.compile_saliency_function', 'file': 'grad_cam.py', 'line_start': 50, 'line_end': 55, 'label': 'compile_saliency_function', 'description': 'Compile a Keras function to compute saliency maps.'}, {'id': 'grad_cam.modify_backprop', 'file': 'grad_cam.py', 'line_start': 58, 'line_end': 79, 'label': 'modify_backprop', 'description': 'Override ReLU gradient and reload model for guided backpropagation.'}, {'id': 'grad_cam.deprocess_image', 'file': 'grad_cam.py', 'line_start': 82, 'line_end': 102, 'label': 'deprocess_image', 'description': 'Convert a tensor into a displayable uint8 image.'}, {'id': 'grad_cam.compile_gradient_function', 'file': 'grad_cam.py', 'line_start': 105, 'line_end': 119, 'label': 'compile_gradient_function', 'description': 'Create a function to compute gradients and feature maps for CAM.'}, {'id': 'grad_cam.calculate_gradient_weighted_CAM', 'file': 'grad_cam.py', 'line_start': 122, 'line_end': 141, 'label': 'calculate_gradient_weighted_CAM', 'description': 'Compute class activation map (CAM) using gradients.'}, {'id': 'grad_cam.calculate_guided_gradient_CAM', 'file': 'grad_cam.py', 'line_start': 144, 'line_end': 151, 'label': 'calculate_guided_gradient_CAM', 'description': 'Combine CAM and guided saliency to produce guided Grad-CAM.'}, {'id': 'grad_cam.calculate_guided_gradient_CAM_v2', 'file': 'grad_cam.py', 'line_start': 155, 'line_end': 167, 'label': 'calculate_guided_gradient_CAM_v2', 'description': 'Alternative guided Grad-CAM with resizing.'}, {'id': 'grad_cam.__main__', 'file': 'grad_cam.py', 'line_start': 170, 'line_end': 190, 'label': 'main', 'description': 'Script entry point: load model, compute and save guided Grad-CAM.'}], 'edges': [{'id': 'e_reset_optimizer_weights_h5py.File', 'source': 'grad_cam.reset_optimizer_weights', 'target': 'h5py.File', 'type': 'call'}, {'id': 'e_target_category_loss_tf.multiply', 'source': 'grad_cam.target_category_loss', 'target': 'tf.multiply', 'type': 'call'}, {'id': 'e_target_category_loss_K.one_hot', 'source': 'grad_cam.target_category_loss', 'target': 'keras.backend.one_hot', 'type': 'call'}, {'id': 'e_normalize_K.sqrt', 'source': 'grad_cam.normalize', 'target': 'keras.backend.sqrt', 'type': 'call'}, {'id': 'e_normalize_K.mean', 'source': 'grad_cam.normalize', 'target': 'keras.backend.mean', 'type': 'call'}, {'id': 'e_normalize_K.square', 'source': 'grad_cam.normalize', 'target': 'keras.backend.square', 'type': 'call'}, {'id': 'e_load_image_np.expand_dims', 'source': 'grad_cam.load_image', 'target': 'numpy.expand_dims', 'type': 'call'}, {'id': 'e_load_image_preprocess_input', 'source': 'grad_cam.load_image', 'target': 'grad_cam.preprocess_input', 'type': 'call'}, {'id': 'e_register_gradient_ops.RegisterGradient', 'source': 'grad_cam.register_gradient', 'target': 'tensorflow.python.framework.ops.RegisterGradient', 'type': 'call'}, {'id': 'e_compile_saliency_function_model.get_layer', 'source': 'grad_cam.compile_saliency_function', 'target': 'keras.models.Model.get_layer', 'type': 'call'}, {'id': 'e_compile_saliency_function_K.max', 'source': 'grad_cam.compile_saliency_function', 'target': 'keras.backend.max', 'type': 'call'}, {'id': 'e_compile_saliency_function_K.gradients', 'source': 'grad_cam.compile_saliency_function', 'target': 'keras.backend.gradients', 'type': 'call'}, {'id': 'e_compile_saliency_function_K.function', 'source': 'grad_cam.compile_saliency_function', 'target': 'keras.backend.function', 'type': 'call'}, {'id': 'e_modify_backprop_tf.get_default_graph', 'source': 'grad_cam.modify_backprop', 'target': 'tensorflow.get_default_graph', 'type': 'call'}, {'id': 'e_modify_backprop_graph.gradient_override_map', 'source': 'grad_cam.modify_backprop', 'target': 'tensorflow.Graph.gradient_override_map', 'type': 'call'}, {'id': 'e_modify_backprop_load_model', 'source': 'grad_cam.modify_backprop', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_compile_gradient_function_Sequential', 'source': 'grad_cam.compile_gradient_function', 'target': 'keras.models.Sequential', 'type': 'call'}, {'id': 'e_compile_gradient_function_Lambda', 'source': 'grad_cam.compile_gradient_function', 'target': 'keras.layers.core.Lambda', 'type': 'call'}, {'id': 'e_compile_gradient_function_K.sum', 'source': 'grad_cam.compile_gradient_function', 'target': 'keras.backend.sum', 'type': 'call'}, {'id': 'e_compile_gradient_function_K.gradients', 'source': 'grad_cam.compile_gradient_function', 'target': 'keras.backend.gradients', 'type': 'call'}, {'id': 'e_compile_gradient_function_normalize', 'source': 'grad_cam.compile_gradient_function', 'target': 'grad_cam.normalize', 'type': 'call'}, {'id': 'e_compile_gradient_function_K.function', 'source': 'grad_cam.compile_gradient_function', 'target': 'keras.backend.function', 'type': 'call'}, {'id': 'e_calculate_gradient_weighted_CAM_gradient_function', 'source': 'grad_cam.calculate_gradient_weighted_CAM', 'target': 'gradient_function', 'type': 'call'}, {'id': 'e_calculate_gradient_weighted_CAM_np.mean', 'source': 'grad_cam.calculate_gradient_weighted_CAM', 'target': 'numpy.mean', 'type': 'call'}, {'id': 'e_calculate_gradient_weighted_CAM_cv2.resize', 'source': 'grad_cam.calculate_gradient_weighted_CAM', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e_calculate_gradient_weighted_CAM_cv2.applyColorMap', 'source': 'grad_cam.calculate_gradient_weighted_CAM', 'target': 'cv2.applyColorMap', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_calculate_gradient_weighted_CAM', 'source': 'grad_cam.calculate_guided_gradient_CAM', 'target': 'grad_cam.calculate_gradient_weighted_CAM', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_saliency_function', 'source': 'grad_cam.calculate_guided_gradient_CAM', 'target': 'saliency_function', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_deprocess_image', 'source': 'grad_cam.calculate_guided_gradient_CAM', 'target': 'grad_cam.deprocess_image', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_v2_calculate_gradient_weighted_CAM', 'source': 'grad_cam.calculate_guided_gradient_CAM_v2', 'target': 'grad_cam.calculate_gradient_weighted_CAM', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_v2_cv2.resize_1', 'source': 'grad_cam.calculate_guided_gradient_CAM_v2', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_v2_saliency_function', 'source': 'grad_cam.calculate_guided_gradient_CAM_v2', 'target': 'saliency_function', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_v2_cv2.resize_2', 'source': 'grad_cam.calculate_guided_gradient_CAM_v2', 'target': 'cv2.resize', 'type': 'call'}, {'id': 'e_calculate_guided_gradient_CAM_v2_deprocess_image', 'source': 'grad_cam.calculate_guided_gradient_CAM_v2', 'target': 'grad_cam.deprocess_image', 'type': 'call'}, {'id': 'e_main_pickle.load', 'source': 'grad_cam.__main__', 'target': 'pickle.load', 'type': 'call'}, {'id': 'e_main_load_model', 'source': 'grad_cam.__main__', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_main_load_image', 'source': 'grad_cam.__main__', 'target': 'grad_cam.load_image', 'type': 'call'}, {'id': 'e_main_model.predict', 'source': 'grad_cam.__main__', 'target': 'model.predict', 'type': 'call'}, {'id': 'e_main_np.argmax', 'source': 'grad_cam.__main__', 'target': 'numpy.argmax', 'type': 'call'}, {'id': 'e_main_compile_gradient_function', 'source': 'grad_cam.__main__', 'target': 'grad_cam.compile_gradient_function', 'type': 'call'}, {'id': 'e_main_register_gradient', 'source': 'grad_cam.__main__', 'target': 'grad_cam.register_gradient', 'type': 'call'}, {'id': 'e_main_modify_backprop', 'source': 'grad_cam.__main__', 'target': 'grad_cam.modify_backprop', 'type': 'call'}, {'id': 'e_main_compile_saliency_function', 'source': 'grad_cam.__main__', 'target': 'grad_cam.compile_saliency_function', 'type': 'call'}, {'id': 'e_main_calculate_guided_gradient_CAM', 'source': 'grad_cam.__main__', 'target': 'grad_cam.calculate_guided_gradient_CAM', 'type': 'call'}, {'id': 'e_main_cv2.imwrite', 'source': 'grad_cam.__main__', 'target': 'cv2.imwrite', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/inference.py ===\n",
      "{'nodes': [{'id': 'inference.load_image', 'file': 'inference.py', 'line_start': 6, 'line_end': 8, 'label': 'load_image', 'description': 'Loads an image from disk and converts it to a numerical array.'}, {'id': 'inference.load_detection_model', 'file': 'inference.py', 'line_start': 10, 'line_end': 12, 'label': 'load_detection_model', 'description': 'Loads an OpenCV CascadeClassifier from the given model path.'}, {'id': 'inference.detect_faces', 'file': 'inference.py', 'line_start': 14, 'line_end': 15, 'label': 'detect_faces', 'description': 'Uses a CascadeClassifier to detect faces in a grayscale image array.'}, {'id': 'inference.draw_bounding_box', 'file': 'inference.py', 'line_start': 17, 'line_end': 20, 'label': 'draw_bounding_box', 'description': 'Draws a rectangle on an image array around detected face coordinates.'}, {'id': 'inference.apply_offsets', 'file': 'inference.py', 'line_start': 21, 'line_end': 24, 'label': 'apply_offsets', 'description': 'Applies horizontal and vertical offsets to face coordinates.'}, {'id': 'inference.draw_text', 'file': 'inference.py', 'line_start': 26, 'line_end': 31, 'label': 'draw_text', 'description': 'Renders text onto an image array at specified coordinates.'}, {'id': 'inference.get_colors', 'file': 'inference.py', 'line_start': 33, 'line_end': 36, 'label': 'get_colors', 'description': 'Generates distinct colors for each class using an HSV colormap.'}], 'edges': [{'id': 'e_load_image_keras.preprocessing.image.load_img', 'source': 'inference.load_image', 'target': 'keras.preprocessing.image.load_img', 'type': 'call'}, {'id': 'e_load_image_keras.preprocessing.image.img_to_array', 'source': 'inference.load_image', 'target': 'keras.preprocessing.image.img_to_array', 'type': 'call'}, {'id': 'e_load_detection_model_cv2.CascadeClassifier', 'source': 'inference.load_detection_model', 'target': 'cv2.CascadeClassifier', 'type': 'call'}, {'id': 'e_detect_faces_cv2.CascadeClassifier.detectMultiScale', 'source': 'inference.detect_faces', 'target': 'cv2.CascadeClassifier.detectMultiScale', 'type': 'call'}, {'id': 'e_draw_bounding_box_cv2.rectangle', 'source': 'inference.draw_bounding_box', 'target': 'cv2.rectangle', 'type': 'call'}, {'id': 'e_draw_text_cv2.putText', 'source': 'inference.draw_text', 'target': 'cv2.putText', 'type': 'call'}, {'id': 'e_get_colors_matplotlib.pyplot.cm.hsv', 'source': 'inference.get_colors', 'target': 'matplotlib.pyplot.cm.hsv', 'type': 'call'}, {'id': 'e_get_colors_numpy.linspace', 'source': 'inference.get_colors', 'target': 'numpy.linspace', 'type': 'call'}, {'id': 'e_get_colors_numpy.asarray', 'source': 'inference.get_colors', 'target': 'numpy.asarray', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/data_augmentation.py ===\n",
      "{'nodes': [{'id': 'data_augmentation.ImageGenerator', 'file': 'data_augmentation.py', 'line_start': 11, 'line_end': 235, 'label': 'ImageGenerator', 'description': 'Class providing image augmentation: color jitter, flips, lighting, cropping, rotation, flow control.'}, {'id': 'data_augmentation.ImageGenerator.__init__', 'file': 'data_augmentation.py', 'line_start': 21, 'line_end': 60, 'label': '__init__', 'description': 'Initialize augmentation parameters and collect active color jitter methods.'}, {'id': 'data_augmentation.ImageGenerator._do_random_crop', 'file': 'data_augmentation.py', 'line_start': 61, 'line_end': 81, 'label': '_do_random_crop', 'description': 'Apply a random scale and translation crop to the image (classification only).'}, {'id': 'data_augmentation.ImageGenerator.do_random_rotation', 'file': 'data_augmentation.py', 'line_start': 83, 'line_end': 103, 'label': 'do_random_rotation', 'description': 'Apply random rotation (scaling + translation) to the image (classification only).'}, {'id': 'data_augmentation.ImageGenerator._gray_scale', 'file': 'data_augmentation.py', 'line_start': 105, 'line_end': 106, 'label': '_gray_scale', 'description': 'Compute per-pixel grayscale luminance.'}, {'id': 'data_augmentation.ImageGenerator.saturation', 'file': 'data_augmentation.py', 'line_start': 108, 'line_end': 114, 'label': 'saturation', 'description': 'Adjust image saturation by blending with grayscale.'}, {'id': 'data_augmentation.ImageGenerator.brightness', 'file': 'data_augmentation.py', 'line_start': 116, 'line_end': 120, 'label': 'brightness', 'description': 'Scale image brightness.'}, {'id': 'data_augmentation.ImageGenerator.contrast', 'file': 'data_augmentation.py', 'line_start': 122, 'line_end': 128, 'label': 'contrast', 'description': 'Adjust image contrast relative to mean grayscale.'}, {'id': 'data_augmentation.ImageGenerator.lighting', 'file': 'data_augmentation.py', 'line_start': 130, 'line_end': 137, 'label': 'lighting', 'description': 'Add PCA-based lighting noise.'}, {'id': 'data_augmentation.ImageGenerator.horizontal_flip', 'file': 'data_augmentation.py', 'line_start': 139, 'line_end': 144, 'label': 'horizontal_flip', 'description': 'Randomly flip image horizontally and adjust box corners.'}, {'id': 'data_augmentation.ImageGenerator.vertical_flip', 'file': 'data_augmentation.py', 'line_start': 146, 'line_end': 151, 'label': 'vertical_flip', 'description': 'Randomly flip image vertically and adjust box corners.'}, {'id': 'data_augmentation.ImageGenerator.transform', 'file': 'data_augmentation.py', 'line_start': 153, 'line_end': 168, 'label': 'transform', 'description': 'Apply color jitters, lighting, and flips to image (and boxes).'}, {'id': 'data_augmentation.ImageGenerator.preprocess_images', 'file': 'data_augmentation.py', 'line_start': 170, 'line_end': 172, 'label': 'preprocess_images', 'description': 'Wrap call to external preprocess_input.'}, {'id': 'data_augmentation.ImageGenerator.flow', 'file': 'data_augmentation.py', 'line_start': 173, 'line_end': 232, 'label': 'flow', 'description': 'Main generator loop: read, optionally crop, augment, batch and yield.'}, {'id': 'data_augmentation.ImageGenerator._wrap_in_dictionary', 'file': 'data_augmentation.py', 'line_start': 233, 'line_end': 235, 'label': '_wrap_in_dictionary', 'description': 'Package inputs and targets into dictionaries for model consumption.'}], 'edges': [{'id': 'e_transform__do_random_crop', 'source': 'data_augmentation.ImageGenerator.flow', 'target': 'data_augmentation.ImageGenerator._do_random_crop', 'type': 'call', 'description': 'Apply random crop if enabled.'}, {'id': 'e_flow_imread', 'source': 'data_augmentation.ImageGenerator.flow', 'target': 'preprocessor._imread', 'type': 'call', 'description': 'Load image from disk.'}, {'id': 'e_flow_imresize', 'source': 'data_augmentation.ImageGenerator.flow', 'target': 'preprocessor._imresize', 'type': 'call', 'description': 'Resize image to target size.'}, {'id': 'e_flow_transform', 'source': 'data_augmentation.ImageGenerator.flow', 'target': 'data_augmentation.ImageGenerator.transform', 'type': 'call', 'description': 'Apply augmentations during training/demo.'}, {'id': 'e_flow_preprocess_images', 'source': 'data_augmentation.ImageGenerator.flow', 'target': 'data_augmentation.ImageGenerator.preprocess_images', 'type': 'call', 'description': 'Final preprocessing before yield.'}, {'id': 'e_flow_to_categorical', 'source': 'data_augmentation.ImageGenerator.flow', 'target': 'preprocessor.to_categorical', 'type': 'call', 'description': 'Convert labels to one-hot encoding.'}, {'id': 'e_flow_wrap', 'source': 'data_augmentation.ImageGenerator.flow', 'target': 'data_augmentation.ImageGenerator._wrap_in_dictionary', 'type': 'call', 'description': 'Package batch for output.'}, {'id': 'e_transform_saturation', 'source': 'data_augmentation.ImageGenerator.transform', 'target': 'data_augmentation.ImageGenerator.saturation', 'type': 'call', 'description': 'Randomly adjust saturation.'}, {'id': 'e_transform_brightness', 'source': 'data_augmentation.ImageGenerator.transform', 'target': 'data_augmentation.ImageGenerator.brightness', 'type': 'call', 'description': 'Randomly adjust brightness.'}, {'id': 'e_transform_contrast', 'source': 'data_augmentation.ImageGenerator.transform', 'target': 'data_augmentation.ImageGenerator.contrast', 'type': 'call', 'description': 'Randomly adjust contrast.'}, {'id': 'e_transform_lighting', 'source': 'data_augmentation.ImageGenerator.transform', 'target': 'data_augmentation.ImageGenerator.lighting', 'type': 'call', 'description': 'Add lighting noise.'}, {'id': 'e_transform_hflip', 'source': 'data_augmentation.ImageGenerator.transform', 'target': 'data_augmentation.ImageGenerator.horizontal_flip', 'type': 'call', 'description': 'Random horizontal flip.'}, {'id': 'e_transform_vflip', 'source': 'data_augmentation.ImageGenerator.transform', 'target': 'data_augmentation.ImageGenerator.vertical_flip', 'type': 'call', 'description': 'Random vertical flip.'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/utils/visualizer.py ===\n",
      "{'nodes': [{'id': 'visualizer.make_mosaic', 'file': 'visualizer.py', 'line_start': 8, 'line_end': 24, 'label': 'make_mosaic', 'description': 'Arrange a set of images into a mosaic with optional border.'}, {'id': 'visualizer.make_mosaic_v2', 'file': 'visualizer.py', 'line_start': 27, 'line_end': 50, 'label': 'make_mosaic_v2', 'description': 'Alternate implementation to assemble images into a grid mosaic.'}, {'id': 'visualizer.pretty_imshow', 'file': 'visualizer.py', 'line_start': 53, 'line_end': 66, 'label': 'pretty_imshow', 'description': 'Display an image with a colorbar alongside using a specified colormap.'}, {'id': 'visualizer.normal_imshow', 'file': 'visualizer.py', 'line_start': 68, 'line_end': 80, 'label': 'normal_imshow', 'description': 'Display an image normally, optionally turning off axes.'}, {'id': 'visualizer.display_image', 'file': 'visualizer.py', 'line_start': 83, 'line_end': 100, 'label': 'display_image', 'description': 'Show a single image with optional class title and pretty or normal display.'}, {'id': 'visualizer.draw_mosaic', 'file': 'visualizer.py', 'line_start': 102, 'line_end': 127, 'label': 'draw_mosaic', 'description': 'Draw multiple images in a subplot grid with optional class titles.'}, {'id': 'visualizer.__main__', 'file': 'visualizer.py', 'line_start': 129, 'line_end': 177, 'label': '__main__', 'description': 'Script entry point: load data, create mosaics, visualize kernels of a trained model.'}], 'edges': [{'id': 'e___main___make_mosaic', 'source': 'visualizer.__main__', 'target': 'visualizer.make_mosaic', 'type': 'call'}, {'id': 'e___main___pretty_imshow', 'source': 'visualizer.__main__', 'target': 'visualizer.pretty_imshow', 'type': 'call'}, {'id': 'e___main___get_labels', 'source': 'visualizer.__main__', 'target': 'utils.utils.get_labels', 'type': 'call'}, {'id': 'e___main___load_model', 'source': 'visualizer.__main__', 'target': 'keras.models.load_model', 'type': 'call'}, {'id': 'e_display_image_pretty_imshow', 'source': 'visualizer.display_image', 'target': 'visualizer.pretty_imshow', 'type': 'call'}]}\n",
      "\n",
      "=== /Users/yeonjoon-mac/Desktop/Workspace/code-diagram/face_classification/src/models/cnn.py ===\n",
      "{'nodes': [{'id': 'cnn.simple_CNN', 'file': 'cnn.py', 'line_start': 14, 'line_end': 56, 'label': 'simple_CNN', 'description': 'Builds a simple CNN model using Keras Sequential API.'}, {'id': 'cnn.simpler_CNN', 'file': 'cnn.py', 'line_start': 59, 'line_end': 108, 'label': 'simpler_CNN', 'description': 'Builds a more compact CNN model with down-sampling via strides.'}, {'id': 'cnn.tiny_XCEPTION', 'file': 'cnn.py', 'line_start': 111, 'line_end': 205, 'label': 'tiny_XCEPTION', 'description': 'Constructs a tiny Xception-style model with separable convolutions and residual connections.'}, {'id': 'cnn.mini_XCEPTION', 'file': 'cnn.py', 'line_start': 207, 'line_end': 301, 'label': 'mini_XCEPTION', 'description': 'Constructs a mini Xception-style model with smaller channel sizes.'}, {'id': 'cnn.big_XCEPTION', 'file': 'cnn.py', 'line_start': 303, 'line_end': 345, 'label': 'big_XCEPTION', 'description': 'Constructs a larger Xception-style model with higher channel counts.'}, {'id': 'cnn.__main__', 'file': 'cnn.py', 'line_start': 348, 'line_end': 358, 'label': '__main__', 'description': 'Script entry point that selects and summaries a model.'}], 'edges': [{'id': 'e___main___simple_CNN', 'source': 'cnn.__main__', 'target': 'cnn.simple_CNN', 'type': 'call'}, {'id': 'e___main___Sequential.summary', 'source': 'cnn.__main__', 'target': 'keras.models.Sequential.summary', 'type': 'call'}]}\n"
     ]
    }
   ],
   "source": [
    "for file, result in results.items():\n",
    "    print(f\"\\n=== {file} ===\")\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "code-diagram",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
