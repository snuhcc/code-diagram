{
    "nodes": [
        {
            "id": "image_emotion_gender_demo.Global",
            "function_name": "Global",
            "file": "face_classification/image_emotion_gender_demo.py",
            "line_start": 1,
            "line_end": 82,
            "description": "Global scope"
        }
    ],
    "edges": [
        {
            "id": "image_emotion_gender_demo.e0",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.datasets.get_labels"
        },
        {
            "id": "image_emotion_gender_demo.e1",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.datasets.get_labels"
        },
        {
            "id": "image_emotion_gender_demo.e2",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.load_detection_model"
        },
        {
            "id": "image_emotion_gender_demo.e3",
            "source": "image_emotion_gender_demo.Global",
            "target": "keras.models.load_model"
        },
        {
            "id": "image_emotion_gender_demo.e4",
            "source": "image_emotion_gender_demo.Global",
            "target": "keras.models.load_model"
        },
        {
            "id": "image_emotion_gender_demo.e5",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.load_image"
        },
        {
            "id": "image_emotion_gender_demo.e6",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.load_image"
        },
        {
            "id": "image_emotion_gender_demo.e7",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.detect_faces"
        },
        {
            "id": "image_emotion_gender_demo.e8",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.apply_offsets"
        },
        {
            "id": "image_emotion_gender_demo.e9",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.apply_offsets"
        },
        {
            "id": "image_emotion_gender_demo.e10",
            "source": "image_emotion_gender_demo.Global",
            "target": "cv2.resize"
        },
        {
            "id": "image_emotion_gender_demo.e11",
            "source": "image_emotion_gender_demo.Global",
            "target": "cv2.resize"
        },
        {
            "id": "image_emotion_gender_demo.e12",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.preprocessor.preprocess_input"
        },
        {
            "id": "image_emotion_gender_demo.e13",
            "source": "image_emotion_gender_demo.Global",
            "target": "numpy.argmax"
        },
        {
            "id": "image_emotion_gender_demo.e14",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.preprocessor.preprocess_input"
        },
        {
            "id": "image_emotion_gender_demo.e15",
            "source": "image_emotion_gender_demo.Global",
            "target": "numpy.argmax"
        },
        {
            "id": "image_emotion_gender_demo.e16",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.draw_bounding_box"
        },
        {
            "id": "image_emotion_gender_demo.e17",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.draw_text"
        },
        {
            "id": "image_emotion_gender_demo.e18",
            "source": "image_emotion_gender_demo.Global",
            "target": "utils.inference.draw_text"
        },
        {
            "id": "image_emotion_gender_demo.e19",
            "source": "image_emotion_gender_demo.Global",
            "target": "cv2.cvtColor"
        },
        {
            "id": "image_emotion_gender_demo.e20",
            "source": "image_emotion_gender_demo.Global",
            "target": "cv2.imwrite"
        }
    ]
}