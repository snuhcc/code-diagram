{'/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/image_gradcam_demo.py': "   1: import sys\n   2: \n   3: import cv2\n   4: import numpy as np\n   5: from keras.models import load_model\n   6: \n   7: from utils.grad_cam import compile_gradient_function\n   8: from utils.grad_cam import compile_saliency_function\n   9: from utils.grad_cam import register_gradient\n  10: from utils.grad_cam import modify_backprop\n  11: from utils.grad_cam import calculate_guided_gradient_CAM\n  12: from utils.datasets import get_labels\n  13: from utils.inference import detect_faces\n  14: from utils.inference import apply_offsets\n  15: from utils.inference import load_detection_model\n  16: from utils.preprocessor import preprocess_input\n  17: from utils.inference import draw_bounding_box\n  18: from utils.inference import load_image\n  19: \n  20: \n  21: # parameters\n  22: image_path = sys.argv[1]\n  23: # task = sys.argv[2]\n  24: task = 'emotion'\n  25: if task == 'emotion':\n  26:     labels = get_labels('fer2013')\n  27:     offsets = (0, 0)\n  28:     # model_filename = '../trained_models/fer2013_big_XCEPTION.54-0.66.hdf5'\n  29:     model_filename = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n  30: elif task == 'gender':\n  31:     labels = get_labels('imdb')\n  32:     offsets = (30, 60)\n  33:     model_filename = '../trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5'\n  34: \n  35: color = (0, 255, 0)\n  36: \n  37: # loading models\n  38: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n  39: model = load_model(model_filename, compile=False)\n  40: target_size = model.input_shape[1:3]\n  41: face_detection = load_detection_model(detection_model_path)\n  42: \n  43: # loading images\n  44: rgb_image = load_image(image_path, grayscale=False)\n  45: gray_image = load_image(image_path, grayscale=True)\n  46: gray_image = np.squeeze(gray_image)\n  47: gray_image = gray_image.astype('uint8')\n  48: faces = detect_faces(face_detection, gray_image)\n  49: \n  50: # start prediction for every image\n  51: for face_coordinates in faces:\n  52: \n  53:     x1, x2, y1, y2 = apply_offsets(face_coordinates, offsets)\n  54:     rgb_face = rgb_image[y1:y2, x1:x2]\n  55: \n  56:     x1, x2, y1, y2 = apply_offsets(face_coordinates, offsets)\n  57:     gray_face = gray_image[y1:y2, x1:x2]\n  58: \n  59:     # processing input\n  60:     try:\n  61:         gray_face = cv2.resize(gray_face, (target_size))\n  62:     except:\n  63:         continue\n  64:     gray_face = preprocess_input(gray_face, True)\n  65:     gray_face = np.expand_dims(gray_face, 0)\n  66:     gray_face = np.expand_dims(gray_face, -1)\n  67: \n  68:     # prediction\n  69:     predicted_class = np.argmax(model.predict(gray_face))\n  70:     label_text = labels[predicted_class]\n  71: \n  72:     gradient_function = compile_gradient_function(model,\n  73:                             predicted_class, 'conv2d_7')\n  74:     register_gradient()\n  75:     guided_model = modify_backprop(model, 'GuidedBackProp', task)\n  76:     saliency_function = compile_saliency_function(guided_model, 'conv2d_7')\n  77: \n  78:     guided_gradCAM = calculate_guided_gradient_CAM(gray_face,\n  79:                         gradient_function, saliency_function)\n  80:     guided_gradCAM = cv2.resize(guided_gradCAM, (x2-x1, y2-y1))\n  81:     rgb_guided_gradCAM = np.repeat(guided_gradCAM[:, :, np.newaxis], 3, axis=2)\n  82:     rgb_image[y1:y2, x1:x2, :] = rgb_guided_gradCAM\n  83:     draw_bounding_box((x1, y1, x2 - x1, y2 - y1), rgb_image, color)\n  84: bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n  85: cv2.imwrite('../images/guided_gradCAM.png', bgr_image)", '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/train_gender_classifier.py': '   1: """\n   2: File: train_gender_classifier.py\n   3: Author: Octavio Arriaga\n   4: Email: arriaga.camargo@gmail.com\n   5: Github: https://github.com/oarriaga\n   6: Description: Train gender classification model\n   7: """\n   8: \n   9: from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n  10: from keras.callbacks import ReduceLROnPlateau\n  11: from utils.datasets import DataManager\n  12: from models.cnn import mini_XCEPTION\n  13: from utils.data_augmentation import ImageGenerator\n  14: from utils.datasets import split_imdb_data\n  15: \n  16: # parameters\n  17: batch_size = 32\n  18: num_epochs = 1000\n  19: validation_split = .2\n  20: do_random_crop = False\n  21: patience = 100\n  22: num_classes = 2\n  23: dataset_name = \'imdb\'\n  24: input_shape = (64, 64, 1)\n  25: if input_shape[2] == 1:\n  26:     grayscale = True\n  27: images_path = \'../datasets/imdb_crop/\'\n  28: log_file_path = \'../trained_models/gender_models/gender_training.log\'\n  29: trained_models_path = \'../trained_models/gender_models/gender_mini_XCEPTION\'\n  30: \n  31: # data loader\n  32: data_loader = DataManager(dataset_name)\n  33: ground_truth_data = data_loader.get_data()\n  34: train_keys, val_keys = split_imdb_data(ground_truth_data, validation_split)\n  35: print(\'Number of training samples:\', len(train_keys))\n  36: print(\'Number of validation samples:\', len(val_keys))\n  37: image_generator = ImageGenerator(ground_truth_data, batch_size,\n  38:                                  input_shape[:2],\n  39:                                  train_keys, val_keys, None,\n  40:                                  path_prefix=images_path,\n  41:                                  vertical_flip_probability=0,\n  42:                                  grayscale=grayscale,\n  43:                                  do_random_crop=do_random_crop)\n  44: \n  45: # model parameters/compilation\n  46: model = mini_XCEPTION(input_shape, num_classes)\n  47: model.compile(optimizer=\'adam\',\n  48:               loss=\'categorical_crossentropy\',\n  49:               metrics=[\'accuracy\'])\n  50: model.summary()\n  51: \n  52: # model callbacks\n  53: early_stop = EarlyStopping(\'val_loss\', patience=patience)\n  54: reduce_lr = ReduceLROnPlateau(\'val_loss\', factor=0.1,\n  55:                               patience=int(patience/2), verbose=1)\n  56: csv_logger = CSVLogger(log_file_path, append=False)\n  57: model_names = trained_models_path + \'.{epoch:02d}-{val_acc:.2f}.hdf5\'\n  58: model_checkpoint = ModelCheckpoint(model_names,\n  59:                                    monitor=\'val_loss\',\n  60:                                    verbose=1,\n  61:                                    save_best_only=True,\n  62:                                    save_weights_only=False)\n  63: callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n  64: \n  65: # training model\n  66: model.fit_generator(image_generator.flow(mode=\'train\'),\n  67:                     steps_per_epoch=int(len(train_keys) / batch_size),\n  68:                     epochs=num_epochs, verbose=1,\n  69:                     callbacks=callbacks,\n  70:                     validation_data=image_generator.flow(\'val\'),\n  71:                     validation_steps=int(len(val_keys) / batch_size))', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/video_emotion_gender_demo.py': "   1: from statistics import mode\n   2: \n   3: import cv2\n   4: from keras.models import load_model\n   5: import numpy as np\n   6: \n   7: from utils.datasets import get_labels\n   8: from utils.inference import detect_faces\n   9: from utils.inference import draw_text\n  10: from utils.inference import draw_bounding_box\n  11: from utils.inference import apply_offsets\n  12: from utils.inference import load_detection_model\n  13: from utils.preprocessor import preprocess_input\n  14: \n  15: # parameters for loading data and images\n  16: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n  17: emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n  18: gender_model_path = '../trained_models/gender_models/simple_CNN.81-0.96.hdf5'\n  19: emotion_labels = get_labels('fer2013')\n  20: gender_labels = get_labels('imdb')\n  21: font = cv2.FONT_HERSHEY_SIMPLEX\n  22: \n  23: # hyper-parameters for bounding boxes shape\n  24: frame_window = 10\n  25: gender_offsets = (30, 60)\n  26: emotion_offsets = (20, 40)\n  27: \n  28: # loading models\n  29: face_detection = load_detection_model(detection_model_path)\n  30: emotion_classifier = load_model(emotion_model_path, compile=False)\n  31: gender_classifier = load_model(gender_model_path, compile=False)\n  32: \n  33: # getting input model shapes for inference\n  34: emotion_target_size = emotion_classifier.input_shape[1:3]\n  35: gender_target_size = gender_classifier.input_shape[1:3]\n  36: \n  37: # starting lists for calculating modes\n  38: gender_window = []\n  39: emotion_window = []\n  40: \n  41: # starting video streaming\n  42: cv2.namedWindow('window_frame')\n  43: video_capture = cv2.VideoCapture(0)\n  44: while True:\n  45: \n  46:     bgr_image = video_capture.read()[1]\n  47:     gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n  48:     rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n  49:     faces = detect_faces(face_detection, gray_image)\n  50: \n  51:     for face_coordinates in faces:\n  52: \n  53:         x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)\n  54:         rgb_face = rgb_image[y1:y2, x1:x2]\n  55: \n  56:         x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n  57:         gray_face = gray_image[y1:y2, x1:x2]\n  58:         try:\n  59:             rgb_face = cv2.resize(rgb_face, (gender_target_size))\n  60:             gray_face = cv2.resize(gray_face, (emotion_target_size))\n  61:         except:\n  62:             continue\n  63:         gray_face = preprocess_input(gray_face, False)\n  64:         gray_face = np.expand_dims(gray_face, 0)\n  65:         gray_face = np.expand_dims(gray_face, -1)\n  66:         emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n  67:         emotion_text = emotion_labels[emotion_label_arg]\n  68:         emotion_window.append(emotion_text)\n  69: \n  70:         rgb_face = np.expand_dims(rgb_face, 0)\n  71:         rgb_face = preprocess_input(rgb_face, False)\n  72:         gender_prediction = gender_classifier.predict(rgb_face)\n  73:         gender_label_arg = np.argmax(gender_prediction)\n  74:         gender_text = gender_labels[gender_label_arg]\n  75:         gender_window.append(gender_text)\n  76: \n  77:         if len(gender_window) > frame_window:\n  78:             emotion_window.pop(0)\n  79:             gender_window.pop(0)\n  80:         try:\n  81:             emotion_mode = mode(emotion_window)\n  82:             gender_mode = mode(gender_window)\n  83:         except:\n  84:             continue\n  85: \n  86:         if gender_text == gender_labels[0]:\n  87:             color = (0, 0, 255)\n  88:         else:\n  89:             color = (255, 0, 0)\n  90: \n  91:         draw_bounding_box(face_coordinates, rgb_image, color)\n  92:         draw_text(face_coordinates, rgb_image, gender_mode,\n  93:                   color, 0, -20, 1, 1)\n  94:         draw_text(face_coordinates, rgb_image, emotion_mode,\n  95:                   color, 0, -45, 1, 1)\n  96: \n  97:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n  98:     cv2.imshow('window_frame', bgr_image)\n  99:     if cv2.waitKey(1) & 0xFF == ord('q'):\n 100:         break\n 101: video_capture.release()\n 102: cv2.destroyAllWindows()", '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/video_emotion_color_demo.py': "   1: from statistics import mode\n   2: \n   3: import cv2\n   4: from keras.models import load_model\n   5: import numpy as np\n   6: \n   7: from utils.datasets import get_labels\n   8: from utils.inference import detect_faces\n   9: from utils.inference import draw_text\n  10: from utils.inference import draw_bounding_box\n  11: from utils.inference import apply_offsets\n  12: from utils.inference import load_detection_model\n  13: from utils.preprocessor import preprocess_input\n  14: \n  15: # parameters for loading data and images\n  16: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n  17: emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n  18: emotion_labels = get_labels('fer2013')\n  19: \n  20: # hyper-parameters for bounding boxes shape\n  21: frame_window = 10\n  22: emotion_offsets = (20, 40)\n  23: \n  24: # loading models\n  25: face_detection = load_detection_model(detection_model_path)\n  26: emotion_classifier = load_model(emotion_model_path, compile=False)\n  27: \n  28: # getting input model shapes for inference\n  29: emotion_target_size = emotion_classifier.input_shape[1:3]\n  30: \n  31: # starting lists for calculating modes\n  32: emotion_window = []\n  33: \n  34: # starting video streaming\n  35: cv2.namedWindow('window_frame')\n  36: video_capture = cv2.VideoCapture(0)\n  37: while True:\n  38:     bgr_image = video_capture.read()[1]\n  39:     gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n  40:     rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n  41:     faces = detect_faces(face_detection, gray_image)\n  42: \n  43:     for face_coordinates in faces:\n  44: \n  45:         x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n  46:         gray_face = gray_image[y1:y2, x1:x2]\n  47:         try:\n  48:             gray_face = cv2.resize(gray_face, (emotion_target_size))\n  49:         except:\n  50:             continue\n  51: \n  52:         gray_face = preprocess_input(gray_face, True)\n  53:         gray_face = np.expand_dims(gray_face, 0)\n  54:         gray_face = np.expand_dims(gray_face, -1)\n  55:         emotion_prediction = emotion_classifier.predict(gray_face)\n  56:         emotion_probability = np.max(emotion_prediction)\n  57:         emotion_label_arg = np.argmax(emotion_prediction)\n  58:         emotion_text = emotion_labels[emotion_label_arg]\n  59:         emotion_window.append(emotion_text)\n  60: \n  61:         if len(emotion_window) > frame_window:\n  62:             emotion_window.pop(0)\n  63:         try:\n  64:             emotion_mode = mode(emotion_window)\n  65:         except:\n  66:             continue\n  67: \n  68:         if emotion_text == 'angry':\n  69:             color = emotion_probability * np.asarray((255, 0, 0))\n  70:         elif emotion_text == 'sad':\n  71:             color = emotion_probability * np.asarray((0, 0, 255))\n  72:         elif emotion_text == 'happy':\n  73:             color = emotion_probability * np.asarray((255, 255, 0))\n  74:         elif emotion_text == 'surprise':\n  75:             color = emotion_probability * np.asarray((0, 255, 255))\n  76:         else:\n  77:             color = emotion_probability * np.asarray((0, 255, 0))\n  78: \n  79:         color = color.astype(int)\n  80:         color = color.tolist()\n  81: \n  82:         draw_bounding_box(face_coordinates, rgb_image, color)\n  83:         draw_text(face_coordinates, rgb_image, emotion_mode,\n  84:                   color, 0, -45, 1, 1)\n  85: \n  86:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n  87:     cv2.imshow('window_frame', bgr_image)\n  88:     if cv2.waitKey(1) & 0xFF == ord('q'):\n  89:         break\n  90: video_capture.release()\n  91: cv2.destroyAllWindows()", '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/video_gradcam_demo.py': "   1: import sys\n   2: \n   3: import cv2\n   4: import numpy as np\n   5: from keras.models import load_model\n   6: from utils.grad_cam import compile_gradient_function\n   7: from utils.grad_cam import compile_saliency_function\n   8: from utils.grad_cam import register_gradient\n   9: from utils.grad_cam import modify_backprop\n  10: from utils.grad_cam import calculate_guided_gradient_CAM\n  11: from utils.inference import detect_faces\n  12: from utils.inference import apply_offsets\n  13: from utils.inference import load_detection_model\n  14: from utils.preprocessor import preprocess_input\n  15: from utils.inference import draw_bounding_box\n  16: from utils.datasets import get_class_to_arg\n  17: \n  18: # getting the correct model given the input\n  19: # task = sys.argv[1]\n  20: # class_name = sys.argv[2]\n  21: task = 'emotion'\n  22: if task == 'gender':\n  23:     model_filename = '../trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5'\n  24:     class_to_arg = get_class_to_arg('imdb')\n  25:     # predicted_class = class_to_arg[class_name]\n  26:     predicted_class = 0\n  27:     offsets = (0, 0)\n  28: elif task == 'emotion':\n  29:     model_filename = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n  30:     # model_filename = '../trained_models/fer2013_big_XCEPTION.54-0.66.hdf5'\n  31:     class_to_arg = get_class_to_arg('fer2013')\n  32:     # predicted_class = class_to_arg[class_name]\n  33:     predicted_class = 0\n  34:     offsets = (0, 0)\n  35: \n  36: model = load_model(model_filename, compile=False)\n  37: gradient_function = compile_gradient_function(model, predicted_class, 'conv2d_7')\n  38: register_gradient()\n  39: guided_model = modify_backprop(model, 'GuidedBackProp', task)\n  40: saliency_function = compile_saliency_function(guided_model, 'conv2d_7')\n  41: \n  42: # parameters for loading data and images\n  43: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n  44: face_detection = load_detection_model(detection_model_path)\n  45: color = (0, 255, 0)\n  46: \n  47: # getting input model shapes for inference\n  48: target_size = model.input_shape[1:3]\n  49: \n  50: # starting lists for calculating modes\n  51: emotion_window = []\n  52: \n  53: # starting video streaming\n  54: cv2.namedWindow('window_frame')\n  55: video_capture = cv2.VideoCapture(0)\n  56: while True:\n  57:     bgr_image = video_capture.read()[1]\n  58:     gray_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2GRAY)\n  59:     rgb_image = cv2.cvtColor(bgr_image, cv2.COLOR_BGR2RGB)\n  60:     faces = detect_faces(face_detection, gray_image)\n  61: \n  62:     for face_coordinates in faces:\n  63: \n  64:         x1, x2, y1, y2 = apply_offsets(face_coordinates, offsets)\n  65:         gray_face = gray_image[y1:y2, x1:x2]\n  66:         try:\n  67:             gray_face = cv2.resize(gray_face, (target_size))\n  68:         except:\n  69:             continue\n  70: \n  71:         gray_face = preprocess_input(gray_face, True)\n  72:         gray_face = np.expand_dims(gray_face, 0)\n  73:         gray_face = np.expand_dims(gray_face, -1)\n  74:         guided_gradCAM = calculate_guided_gradient_CAM(gray_face,\n  75:                             gradient_function, saliency_function)\n  76:         guided_gradCAM = cv2.resize(guided_gradCAM, (x2-x1, y2-y1))\n  77:         try:\n  78:             rgb_guided_gradCAM = np.repeat(guided_gradCAM[:, :, np.newaxis],\n  79:                                                                 3, axis=2)\n  80:             rgb_image[y1:y2, x1:x2, :] = rgb_guided_gradCAM\n  81:         except:\n  82:             continue\n  83:         draw_bounding_box((x1, y1, x2 - x1, y2 - y1), rgb_image, color)\n  84:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n  85:     try:\n  86:         cv2.imshow('window_frame', bgr_image)\n  87:     except:\n  88:         continue\n  89:     if cv2.waitKey(1) & 0xFF == ord('q'):\n  90:         break\n  91: \n  92: ", '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/train_emotion_classifier.py': '   1: """\n   2: File: train_emotion_classifier.py\n   3: Author: Octavio Arriaga\n   4: Email: arriaga.camargo@gmail.com\n   5: Github: https://github.com/oarriaga\n   6: Description: Train emotion classification model\n   7: """\n   8: \n   9: from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping\n  10: from keras.callbacks import ReduceLROnPlateau\n  11: from keras.preprocessing.image import ImageDataGenerator\n  12: \n  13: from models.cnn import mini_XCEPTION\n  14: from utils.datasets import DataManager\n  15: from utils.datasets import split_data\n  16: from utils.preprocessor import preprocess_input\n  17: \n  18: # parameters\n  19: batch_size = 32\n  20: num_epochs = 10000\n  21: input_shape = (64, 64, 1)\n  22: validation_split = .2\n  23: verbose = 1\n  24: num_classes = 7\n  25: patience = 50\n  26: base_path = \'../trained_models/emotion_models/\'\n  27: \n  28: # data generator\n  29: data_generator = ImageDataGenerator(\n  30:                         featurewise_center=False,\n  31:                         featurewise_std_normalization=False,\n  32:                         rotation_range=10,\n  33:                         width_shift_range=0.1,\n  34:                         height_shift_range=0.1,\n  35:                         zoom_range=.1,\n  36:                         horizontal_flip=True)\n  37: \n  38: # model parameters/compilation\n  39: model = mini_XCEPTION(input_shape, num_classes)\n  40: model.compile(optimizer=\'adam\', loss=\'categorical_crossentropy\',\n  41:               metrics=[\'accuracy\'])\n  42: model.summary()\n  43: \n  44: \n  45: datasets = [\'fer2013\']\n  46: for dataset_name in datasets:\n  47:     print(\'Training dataset:\', dataset_name)\n  48: \n  49:     # callbacks\n  50:     log_file_path = base_path + dataset_name + \'_emotion_training.log\'\n  51:     csv_logger = CSVLogger(log_file_path, append=False)\n  52:     early_stop = EarlyStopping(\'val_loss\', patience=patience)\n  53:     reduce_lr = ReduceLROnPlateau(\'val_loss\', factor=0.1,\n  54:                                   patience=int(patience/4), verbose=1)\n  55:     trained_models_path = base_path + dataset_name + \'_mini_XCEPTION\'\n  56:     model_names = trained_models_path + \'.{epoch:02d}-{val_acc:.2f}.hdf5\'\n  57:     model_checkpoint = ModelCheckpoint(model_names, \'val_loss\', verbose=1,\n  58:                                                     save_best_only=True)\n  59:     callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]\n  60: \n  61:     # loading dataset\n  62:     data_loader = DataManager(dataset_name, image_size=input_shape[:2])\n  63:     faces, emotions = data_loader.get_data()\n  64:     faces = preprocess_input(faces)\n  65:     num_samples, num_classes = emotions.shape\n  66:     train_data, val_data = split_data(faces, emotions, validation_split)\n  67:     train_faces, train_emotions = train_data\n  68:     model.fit_generator(data_generator.flow(train_faces, train_emotions,\n  69:                                             batch_size),\n  70:                         steps_per_epoch=len(train_faces) / batch_size,\n  71:                         epochs=num_epochs, verbose=1, callbacks=callbacks,\n  72:                         validation_data=val_data)', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/image_emotion_gender_demo.py': "   1: import sys\n   2: \n   3: import cv2\n   4: from keras.models import load_model\n   5: import numpy as np\n   6: \n   7: from utils.datasets import get_labels\n   8: from utils.inference import detect_faces\n   9: from utils.inference import draw_text\n  10: from utils.inference import draw_bounding_box\n  11: from utils.inference import apply_offsets\n  12: from utils.inference import load_detection_model\n  13: from utils.inference import load_image\n  14: from utils.preprocessor import preprocess_input\n  15: \n  16: # parameters for loading data and images\n  17: image_path = sys.argv[1]\n  18: detection_model_path = '../trained_models/detection_models/haarcascade_frontalface_default.xml'\n  19: emotion_model_path = '../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5'\n  20: gender_model_path = '../trained_models/gender_models/simple_CNN.81-0.96.hdf5'\n  21: emotion_labels = get_labels('fer2013')\n  22: gender_labels = get_labels('imdb')\n  23: font = cv2.FONT_HERSHEY_SIMPLEX\n  24: \n  25: # hyper-parameters for bounding boxes shape\n  26: gender_offsets = (30, 60)\n  27: gender_offsets = (10, 10)\n  28: emotion_offsets = (20, 40)\n  29: emotion_offsets = (0, 0)\n  30: \n  31: # loading models\n  32: face_detection = load_detection_model(detection_model_path)\n  33: emotion_classifier = load_model(emotion_model_path, compile=False)\n  34: gender_classifier = load_model(gender_model_path, compile=False)\n  35: \n  36: # getting input model shapes for inference\n  37: emotion_target_size = emotion_classifier.input_shape[1:3]\n  38: gender_target_size = gender_classifier.input_shape[1:3]\n  39: \n  40: # loading images\n  41: rgb_image = load_image(image_path, grayscale=False)\n  42: gray_image = load_image(image_path, grayscale=True)\n  43: gray_image = np.squeeze(gray_image)\n  44: gray_image = gray_image.astype('uint8')\n  45: \n  46: faces = detect_faces(face_detection, gray_image)\n  47: for face_coordinates in faces:\n  48:     x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)\n  49:     rgb_face = rgb_image[y1:y2, x1:x2]\n  50: \n  51:     x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n  52:     gray_face = gray_image[y1:y2, x1:x2]\n  53: \n  54:     try:\n  55:         rgb_face = cv2.resize(rgb_face, (gender_target_size))\n  56:         gray_face = cv2.resize(gray_face, (emotion_target_size))\n  57:     except:\n  58:         continue\n  59: \n  60:     rgb_face = preprocess_input(rgb_face, False)\n  61:     rgb_face = np.expand_dims(rgb_face, 0)\n  62:     gender_prediction = gender_classifier.predict(rgb_face)\n  63:     gender_label_arg = np.argmax(gender_prediction)\n  64:     gender_text = gender_labels[gender_label_arg]\n  65: \n  66:     gray_face = preprocess_input(gray_face, True)\n  67:     gray_face = np.expand_dims(gray_face, 0)\n  68:     gray_face = np.expand_dims(gray_face, -1)\n  69:     emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n  70:     emotion_text = emotion_labels[emotion_label_arg]\n  71: \n  72:     if gender_text == gender_labels[0]:\n  73:         color = (0, 0, 255)\n  74:     else:\n  75:         color = (255, 0, 0)\n  76: \n  77:     draw_bounding_box(face_coordinates, rgb_image, color)\n  78:     draw_text(face_coordinates, rgb_image, gender_text, color, 0, -20, 1, 2)\n  79:     draw_text(face_coordinates, rgb_image, emotion_text, color, 0, -50, 1, 2)\n  80: \n  81: bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n  82: cv2.imwrite('../images/predicted_test_image.png', bgr_image)", '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/web/faces.py': '   1: from flask import Flask, jsonify, make_response, request, abort, redirect, send_file\n   2: import logging\n   3: \n   4: import emotion_gender_processor as eg_processor\n   5: \n   6: app = Flask(__name__)\n   7: \n   8: @app.route(\'/\')\n   9: def index():\n  10:     return redirect("https://ekholabs.ai", code=302)\n  11: \n  12: @app.route(\'/classifyImage\', methods=[\'POST\'])\n  13: def upload():\n  14:     try:\n  15:         image = request.files[\'image\'].read()\n  16:         eg_processor.process_image(image)\n  17:         return send_file(\'/ekholabs/face-classifier/result/predicted_image.png\', mimetype=\'image/png\')\n  18:     except Exception as err:\n  19:         logging.error(\'An error has occurred whilst processing the file: "{0}"\'.format(err))\n  20:         abort(400)\n  21: \n  22: @app.errorhandler(400)\n  23: def bad_request(erro):\n  24:     return make_response(jsonify({\'error\': \'We cannot process the file sent in the request.\'}), 400)\n  25: \n  26: @app.errorhandler(404)\n  27: def not_found(error):\n  28:     return make_response(jsonify({\'error\': \'Resource no found.\'}), 404)\n  29: \n  30: if __name__ == \'__main__\':\n  31:     app.run(debug=True, host=\'0.0.0.0\', port=8084)', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/web/emotion_gender_processor.py': '   1: import os\n   2: import sys\n   3: import logging\n   4: \n   5: import cv2\n   6: from keras.models import load_model\n   7: import numpy as np\n   8: \n   9: from utils.datasets import get_labels\n  10: from utils.inference import detect_faces\n  11: from utils.inference import draw_text\n  12: from utils.inference import draw_bounding_box\n  13: from utils.inference import apply_offsets\n  14: from utils.inference import load_detection_model\n  15: from utils.inference import load_image\n  16: from utils.preprocessor import preprocess_input\n  17: \n  18: def process_image(image):\n  19: \n  20:     try:\n  21:         # parameters for loading data and images\n  22:         detection_model_path = \'./trained_models/detection_models/haarcascade_frontalface_default.xml\'\n  23:         emotion_model_path = \'./trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5\'\n  24:         gender_model_path = \'./trained_models/gender_models/simple_CNN.81-0.96.hdf5\'\n  25:         emotion_labels = get_labels(\'fer2013\')\n  26:         gender_labels = get_labels(\'imdb\')\n  27:         font = cv2.FONT_HERSHEY_SIMPLEX\n  28: \n  29:         # hyper-parameters for bounding boxes shape\n  30:         gender_offsets = (30, 60)\n  31:         gender_offsets = (10, 10)\n  32:         emotion_offsets = (20, 40)\n  33:         emotion_offsets = (0, 0)\n  34: \n  35:         # loading models\n  36:         face_detection = load_detection_model(detection_model_path)\n  37:         emotion_classifier = load_model(emotion_model_path, compile=False)\n  38:         gender_classifier = load_model(gender_model_path, compile=False)\n  39: \n  40:         # getting input model shapes for inference\n  41:         emotion_target_size = emotion_classifier.input_shape[1:3]\n  42:         gender_target_size = gender_classifier.input_shape[1:3]\n  43: \n  44:         # loading images\n  45:         image_array = np.fromstring(image, np.uint8)\n  46:         unchanged_image = cv2.imdecode(image_array, cv2.IMREAD_UNCHANGED)\n  47: \n  48:         rgb_image = cv2.cvtColor(unchanged_image, cv2.COLOR_BGR2RGB)\n  49:         gray_image = cv2.cvtColor(unchanged_image, cv2.COLOR_BGR2GRAY)\n  50: \n  51:         faces = detect_faces(face_detection, gray_image)\n  52:         for face_coordinates in faces:\n  53:             x1, x2, y1, y2 = apply_offsets(face_coordinates, gender_offsets)\n  54:             rgb_face = rgb_image[y1:y2, x1:x2]\n  55: \n  56:             x1, x2, y1, y2 = apply_offsets(face_coordinates, emotion_offsets)\n  57:             gray_face = gray_image[y1:y2, x1:x2]\n  58: \n  59:             try:\n  60:                 rgb_face = cv2.resize(rgb_face, (gender_target_size))\n  61:                 gray_face = cv2.resize(gray_face, (emotion_target_size))\n  62:             except:\n  63:                 continue\n  64: \n  65:             rgb_face = preprocess_input(rgb_face, False)\n  66:             rgb_face = np.expand_dims(rgb_face, 0)\n  67:             gender_prediction = gender_classifier.predict(rgb_face)\n  68:             gender_label_arg = np.argmax(gender_prediction)\n  69:             gender_text = gender_labels[gender_label_arg]\n  70: \n  71:             gray_face = preprocess_input(gray_face, True)\n  72:             gray_face = np.expand_dims(gray_face, 0)\n  73:             gray_face = np.expand_dims(gray_face, -1)\n  74:             emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n  75:             emotion_text = emotion_labels[emotion_label_arg]\n  76: \n  77:             if gender_text == gender_labels[0]:\n  78:                 color = (0, 0, 255)\n  79:             else:\n  80:                 color = (255, 0, 0)\n  81: \n  82:             draw_bounding_box(face_coordinates, rgb_image, color)\n  83:             draw_text(face_coordinates, rgb_image, gender_text, color, 0, -20, 1, 2)\n  84:             draw_text(face_coordinates, rgb_image, emotion_text, color, 0, -50, 1, 2)\n  85:     except Exception as err:\n  86:         logging.error(\'Error in emotion gender processor: "{0}"\'.format(err))\n  87: \n  88:     bgr_image = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR)\n  89: \n  90:     dirname = \'result\'\n  91:     if not os.path.exists(dirname):\n  92:         os.mkdir(dirname)\n  93: \n  94:     cv2.imwrite(os.path.join(dirname, \'predicted_image.png\'), bgr_image)', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/utils/datasets.py': '   1: from scipy.io import loadmat\n   2: import pandas as pd\n   3: import numpy as np\n   4: from random import shuffle\n   5: import os\n   6: import cv2\n   7: \n   8: \n   9: class DataManager(object):\n  10:     """Class for loading fer2013 emotion classification dataset or\n  11:         imdb gender classification dataset."""\n  12:     def __init__(self, dataset_name=\'imdb\',\n  13:                  dataset_path=None, image_size=(48, 48)):\n  14: \n  15:         self.dataset_name = dataset_name\n  16:         self.dataset_path = dataset_path\n  17:         self.image_size = image_size\n  18:         if self.dataset_path is not None:\n  19:             self.dataset_path = dataset_path\n  20:         elif self.dataset_name == \'imdb\':\n  21:             self.dataset_path = \'../datasets/imdb_crop/imdb.mat\'\n  22:         elif self.dataset_name == \'fer2013\':\n  23:             self.dataset_path = \'../datasets/fer2013/fer2013.csv\'\n  24:         elif self.dataset_name == \'KDEF\':\n  25:             self.dataset_path = \'../datasets/KDEF/\'\n  26:         else:\n  27:             raise Exception(\n  28:                     \'Incorrect dataset name, please input imdb or fer2013\')\n  29: \n  30:     def get_data(self):\n  31:         if self.dataset_name == \'imdb\':\n  32:             ground_truth_data = self._load_imdb()\n  33:         elif self.dataset_name == \'fer2013\':\n  34:             ground_truth_data = self._load_fer2013()\n  35:         elif self.dataset_name == \'KDEF\':\n  36:             ground_truth_data = self._load_KDEF()\n  37:         return ground_truth_data\n  38: \n  39:     def _load_imdb(self):\n  40:         face_score_treshold = 3\n  41:         dataset = loadmat(self.dataset_path)\n  42:         image_names_array = dataset[\'imdb\'][\'full_path\'][0, 0][0]\n  43:         gender_classes = dataset[\'imdb\'][\'gender\'][0, 0][0]\n  44:         face_score = dataset[\'imdb\'][\'face_score\'][0, 0][0]\n  45:         second_face_score = dataset[\'imdb\'][\'second_face_score\'][0, 0][0]\n  46:         face_score_mask = face_score > face_score_treshold\n  47:         second_face_score_mask = np.isnan(second_face_score)\n  48:         unknown_gender_mask = np.logical_not(np.isnan(gender_classes))\n  49:         mask = np.logical_and(face_score_mask, second_face_score_mask)\n  50:         mask = np.logical_and(mask, unknown_gender_mask)\n  51:         image_names_array = image_names_array[mask]\n  52:         gender_classes = gender_classes[mask].tolist()\n  53:         image_names = []\n  54:         for image_name_arg in range(image_names_array.shape[0]):\n  55:             image_name = image_names_array[image_name_arg][0]\n  56:             image_names.append(image_name)\n  57:         return dict(zip(image_names, gender_classes))\n  58: \n  59:     def _load_fer2013(self):\n  60:         data = pd.read_csv(self.dataset_path)\n  61:         pixels = data[\'pixels\'].tolist()\n  62:         width, height = 48, 48\n  63:         faces = []\n  64:         for pixel_sequence in pixels:\n  65:             face = [int(pixel) for pixel in pixel_sequence.split(\' \')]\n  66:             face = np.asarray(face).reshape(width, height)\n  67:             face = cv2.resize(face.astype(\'uint8\'), self.image_size)\n  68:             faces.append(face.astype(\'float32\'))\n  69:         faces = np.asarray(faces)\n  70:         faces = np.expand_dims(faces, -1)\n  71:         emotions = pd.get_dummies(data[\'emotion\']).as_matrix()\n  72:         return faces, emotions\n  73: \n  74:     def _load_KDEF(self):\n  75:         class_to_arg = get_class_to_arg(self.dataset_name)\n  76:         num_classes = len(class_to_arg)\n  77: \n  78:         file_paths = []\n  79:         for folder, subfolders, filenames in os.walk(self.dataset_path):\n  80:             for filename in filenames:\n  81:                 if filename.lower().endswith((\'.jpg\')):\n  82:                     file_paths.append(os.path.join(folder, filename))\n  83: \n  84:         num_faces = len(file_paths)\n  85:         y_size, x_size = self.image_size\n  86:         faces = np.zeros(shape=(num_faces, y_size, x_size))\n  87:         emotions = np.zeros(shape=(num_faces, num_classes))\n  88:         for file_arg, file_path in enumerate(file_paths):\n  89:             image_array = cv2.imread(file_path, cv2.IMREAD_GRAYSCALE)\n  90:             image_array = cv2.resize(image_array, (y_size, x_size))\n  91:             faces[file_arg] = image_array\n  92:             file_basename = os.path.basename(file_path)\n  93:             file_emotion = file_basename[4:6]\n  94:             # there are two file names in the dataset\n  95:             # that don\'t match the given classes\n  96:             try:\n  97:                 emotion_arg = class_to_arg[file_emotion]\n  98:             except:\n  99:                 continue\n 100:             emotions[file_arg, emotion_arg] = 1\n 101:         faces = np.expand_dims(faces, -1)\n 102:         return faces, emotions\n 103: \n 104: \n 105: def get_labels(dataset_name):\n 106:     if dataset_name == \'fer2013\':\n 107:         return {0: \'angry\', 1: \'disgust\', 2: \'fear\', 3: \'happy\',\n 108:                 4: \'sad\', 5: \'surprise\', 6: \'neutral\'}\n 109:     elif dataset_name == \'imdb\':\n 110:         return {0: \'woman\', 1: \'man\'}\n 111:     elif dataset_name == \'KDEF\':\n 112:         return {0: \'AN\', 1: \'DI\', 2: \'AF\', 3: \'HA\', 4: \'SA\', 5: \'SU\', 6: \'NE\'}\n 113:     else:\n 114:         raise Exception(\'Invalid dataset name\')\n 115: \n 116: \n 117: def get_class_to_arg(dataset_name=\'fer2013\'):\n 118:     if dataset_name == \'fer2013\':\n 119:         return {\'angry\': 0, \'disgust\': 1, \'fear\': 2, \'happy\': 3, \'sad\': 4,\n 120:                 \'surprise\': 5, \'neutral\': 6}\n 121:     elif dataset_name == \'imdb\':\n 122:         return {\'woman\': 0, \'man\': 1}\n 123:     elif dataset_name == \'KDEF\':\n 124:         return {\'AN\': 0, \'DI\': 1, \'AF\': 2, \'HA\': 3, \'SA\': 4, \'SU\': 5, \'NE\': 6}\n 125:     else:\n 126:         raise Exception(\'Invalid dataset name\')\n 127: \n 128: \n 129: def split_imdb_data(ground_truth_data, validation_split=.2, do_shuffle=False):\n 130:     ground_truth_keys = sorted(ground_truth_data.keys())\n 131:     if do_shuffle is not False:\n 132:         shuffle(ground_truth_keys)\n 133:     training_split = 1 - validation_split\n 134:     num_train = int(training_split * len(ground_truth_keys))\n 135:     train_keys = ground_truth_keys[:num_train]\n 136:     validation_keys = ground_truth_keys[num_train:]\n 137:     return train_keys, validation_keys\n 138: \n 139: \n 140: def split_data(x, y, validation_split=.2):\n 141:     num_samples = len(x)\n 142:     num_train_samples = int((1 - validation_split)*num_samples)\n 143:     train_x = x[:num_train_samples]\n 144:     train_y = y[:num_train_samples]\n 145:     val_x = x[num_train_samples:]\n 146:     val_y = y[num_train_samples:]\n 147:     train_data = (train_x, train_y)\n 148:     val_data = (val_x, val_y)\n 149:     return train_data, val_data', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/utils/preprocessor.py': "   1: import numpy as np\n   2: from scipy.misc import imread, imresize\n   3: \n   4: \n   5: def preprocess_input(x, v2=True):\n   6:     x = x.astype('float32')\n   7:     x = x / 255.0\n   8:     if v2:\n   9:         x = x - 0.5\n  10:         x = x * 2.0\n  11:     return x\n  12: \n  13: \n  14: def _imread(image_name):\n  15:         return imread(image_name)\n  16: \n  17: \n  18: def _imresize(image_array, size):\n  19:         return imresize(image_array, size)\n  20: \n  21: \n  22: def to_categorical(integer_classes, num_classes=2):\n  23:     integer_classes = np.asarray(integer_classes, dtype='int')\n  24:     num_samples = integer_classes.shape[0]\n  25:     categorical = np.zeros((num_samples, num_classes))\n  26:     categorical[np.arange(num_samples), integer_classes] = 1\n  27:     return categorical", '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/utils/grad_cam.py': '   1: import cv2\n   2: import h5py\n   3: import keras\n   4: import keras.backend as K\n   5: from keras.layers.core import Lambda\n   6: from keras.models import Sequential\n   7: from keras.models import load_model\n   8: import numpy as np\n   9: import tensorflow as tf\n  10: from tensorflow.python.framework import ops\n  11: \n  12: from .preprocessor import preprocess_input\n  13: \n  14: \n  15: def reset_optimizer_weights(model_filename):\n  16:     model = h5py.File(model_filename, \'r+\')\n  17:     del model[\'optimizer_weights\']\n  18:     model.close()\n  19: \n  20: \n  21: def target_category_loss(x, category_index, num_classes):\n  22:     return tf.multiply(x, K.one_hot([category_index], num_classes))\n  23: \n  24: \n  25: def target_category_loss_output_shape(input_shape):\n  26:     return input_shape\n  27: \n  28: \n  29: def normalize(x):\n  30:     # utility function to normalize a tensor by its L2 norm\n  31:     return x / (K.sqrt(K.mean(K.square(x))) + 1e-5)\n  32: \n  33: \n  34: def load_image(image_array):\n  35:     image_array = np.expand_dims(image_array, axis=0)\n  36:     image_array = preprocess_input(image_array)\n  37:     return image_array\n  38: \n  39: \n  40: def register_gradient():\n  41:     if "GuidedBackProp" not in ops._gradient_registry._registry:\n  42:         @ops.RegisterGradient("GuidedBackProp")\n  43:         def _GuidedBackProp(op, gradient):\n  44:             dtype = op.inputs[0].dtype\n  45:             guided_gradient = (gradient * tf.cast(gradient > 0., dtype) *\n  46:                                tf.cast(op.inputs[0] > 0., dtype))\n  47:             return guided_gradient\n  48: \n  49: \n  50: def compile_saliency_function(model, activation_layer=\'conv2d_7\'):\n  51:     input_image = model.input\n  52:     layer_output = model.get_layer(activation_layer).output\n  53:     max_output = K.max(layer_output, axis=3)\n  54:     saliency = K.gradients(K.sum(max_output), input_image)[0]\n  55:     return K.function([input_image, K.learning_phase()], [saliency])\n  56: \n  57: \n  58: def modify_backprop(model, name, task):\n  59:     graph = tf.get_default_graph()\n  60:     with graph.gradient_override_map({\'Relu\': name}):\n  61: \n  62:         # get layers that have an activation\n  63:         activation_layers = [layer for layer in model.layers\n  64:                              if hasattr(layer, \'activation\')]\n  65: \n  66:         # replace relu activation\n  67:         for layer in activation_layers:\n  68:             if layer.activation == keras.activations.relu:\n  69:                 layer.activation = tf.nn.relu\n  70: \n  71:         # re-instanciate a new model\n  72:         if task == \'gender\':\n  73:             model_path = \'../trained_models/gender_models/gender_mini_XCEPTION.21-0.95.hdf5\'\n  74:         elif task == \'emotion\':\n  75:             model_path = \'../trained_models/emotion_models/fer2013_mini_XCEPTION.102-0.66.hdf5\'\n  76:             # model_path = \'../trained_models/fer2013_mini_XCEPTION.119-0.65.hdf5\'\n  77:             # model_path = \'../trained_models/fer2013_big_XCEPTION.54-0.66.hdf5\'\n  78:         new_model = load_model(model_path, compile=False)\n  79:     return new_model\n  80: \n  81: \n  82: def deprocess_image(x):\n  83:     """ Same normalization as in:\n  84:     https://github.com/fchollet/keras/blob/master/examples/conv_filter_visualization.py\n  85:     """\n  86:     if np.ndim(x) > 3:\n  87:         x = np.squeeze(x)\n  88:     # normalize tensor: center on 0., ensure std is 0.1\n  89:     x = x - x.mean()\n  90:     x = x / (x.std() + 1e-5)\n  91:     x = x * 0.1\n  92: \n  93:     # clip to [0, 1]\n  94:     x = x + 0.5\n  95:     x = np.clip(x, 0, 1)\n  96: \n  97:     # convert to RGB array\n  98:     x = x * 255\n  99:     if K.image_dim_ordering() == \'th\':\n 100:         x = x.transpose((1, 2, 0))\n 101:     x = np.clip(x, 0, 255).astype(\'uint8\')\n 102:     return x\n 103: \n 104: \n 105: def compile_gradient_function(input_model, category_index, layer_name):\n 106:     model = Sequential()\n 107:     model.add(input_model)\n 108: \n 109:     num_classes = model.output_shape[1]\n 110:     target_layer = lambda x: target_category_loss(x, category_index, num_classes)\n 111:     model.add(Lambda(target_layer,\n 112:                      output_shape=target_category_loss_output_shape))\n 113: \n 114:     loss = K.sum(model.layers[-1].output)\n 115:     conv_output = model.layers[0].get_layer(layer_name).output\n 116:     gradients = normalize(K.gradients(loss, conv_output)[0])\n 117:     gradient_function = K.function([model.layers[0].input, K.learning_phase()],\n 118:                                    [conv_output, gradients])\n 119:     return gradient_function\n 120: \n 121: \n 122: def calculate_gradient_weighted_CAM(gradient_function, image):\n 123:     output, evaluated_gradients = gradient_function([image, False])\n 124:     output, evaluated_gradients = output[0, :], evaluated_gradients[0, :, :, :]\n 125:     weights = np.mean(evaluated_gradients, axis=(0, 1))\n 126:     CAM = np.ones(output.shape[0: 2], dtype=np.float32)\n 127:     for weight_arg, weight in enumerate(weights):\n 128:         CAM = CAM + (weight * output[:, :, weight_arg])\n 129:     CAM = cv2.resize(CAM, (64, 64))\n 130:     CAM = np.maximum(CAM, 0)\n 131:     heatmap = CAM / np.max(CAM)\n 132: \n 133:     # Return to BGR [0..255] from the preprocessed image\n 134:     image = image[0, :]\n 135:     image = image - np.min(image)\n 136:     image = np.minimum(image, 255)\n 137: \n 138:     CAM = cv2.applyColorMap(np.uint8(255 * heatmap), cv2.COLORMAP_JET)\n 139:     CAM = np.float32(CAM) + np.float32(image)\n 140:     CAM = 255 * CAM / np.max(CAM)\n 141:     return np.uint8(CAM), heatmap\n 142: \n 143: \n 144: def calculate_guided_gradient_CAM(\n 145:         preprocessed_input, gradient_function, saliency_function):\n 146:     CAM, heatmap = calculate_gradient_weighted_CAM(\n 147:             gradient_function, preprocessed_input)\n 148:     saliency = saliency_function([preprocessed_input, 0])\n 149:     # gradCAM = saliency[0] * heatmap[..., np.newaxis]\n 150:     # return deprocess_image(gradCAM)\n 151:     return deprocess_image(saliency[0])\n 152:     # return saliency[0]\n 153: \n 154: \n 155: def calculate_guided_gradient_CAM_v2(\n 156:         preprocessed_input, gradient_function,\n 157:         saliency_function, target_size=(128, 128)):\n 158:     CAM, heatmap = calculate_gradient_weighted_CAM(\n 159:             gradient_function, preprocessed_input)\n 160:     heatmap = np.squeeze(heatmap)\n 161:     heatmap = cv2.resize(heatmap.astype(\'uint8\'), target_size)\n 162:     saliency = saliency_function([preprocessed_input, 0])\n 163:     saliency = np.squeeze(saliency[0])\n 164:     saliency = cv2.resize(saliency.astype(\'uint8\'), target_size)\n 165:     gradCAM = saliency * heatmap\n 166:     gradCAM = deprocess_image(gradCAM)\n 167:     return np.expand_dims(gradCAM, -1)\n 168: \n 169: \n 170: if __name__ == \'__main__\':\n 171:     import pickle\n 172:     faces = pickle.load(open(\'faces.pkl\', \'rb\'))\n 173:     face = faces[0]\n 174:     model_filename = \'../../trained_models/emotion_models/mini_XCEPTION.523-0.65.hdf5\'\n 175:     # reset_optimizer_weights(model_filename)\n 176:     model = load_model(model_filename)\n 177: \n 178:     preprocessed_input = load_image(face)\n 179:     predictions = model.predict(preprocessed_input)\n 180:     predicted_class = np.argmax(predictions)\n 181:     gradient_function = compile_gradient_function(\n 182:             model, predicted_class, \'conv2d_6\')\n 183:     register_gradient()\n 184:     guided_model = modify_backprop(model, \'GuidedBackProp\')\n 185:     saliency_function = compile_saliency_function(guided_model)\n 186:     guided_gradCAM = calculate_guided_gradient_CAM(\n 187:             preprocessed_input, gradient_function, saliency_function)\n 188: \n 189:     cv2.imwrite(\'guided_gradCAM.jpg\', guided_gradCAM)\n 190: \n 191: ', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/utils/inference.py': '   1: import cv2\n   2: import matplotlib.pyplot as plt\n   3: import numpy as np\n   4: from keras.preprocessing import image\n   5: \n   6: def load_image(image_path, grayscale=False, target_size=None):\n   7:     pil_image = image.load_img(image_path, grayscale, target_size)\n   8:     return image.img_to_array(pil_image)\n   9: \n  10: def load_detection_model(model_path):\n  11:     detection_model = cv2.CascadeClassifier(model_path)\n  12:     return detection_model\n  13: \n  14: def detect_faces(detection_model, gray_image_array):\n  15:     return detection_model.detectMultiScale(gray_image_array, 1.3, 5)\n  16: \n  17: def draw_bounding_box(face_coordinates, image_array, color):\n  18:     x, y, w, h = face_coordinates\n  19:     cv2.rectangle(image_array, (x, y), (x + w, y + h), color, 2)\n  20: \n  21: def apply_offsets(face_coordinates, offsets):\n  22:     x, y, width, height = face_coordinates\n  23:     x_off, y_off = offsets\n  24:     return (x - x_off, x + width + x_off, y - y_off, y + height + y_off)\n  25: \n  26: def draw_text(coordinates, image_array, text, color, x_offset=0, y_offset=0,\n  27:                                                 font_scale=2, thickness=2):\n  28:     x, y = coordinates[:2]\n  29:     cv2.putText(image_array, text, (x + x_offset, y + y_offset),\n  30:                 cv2.FONT_HERSHEY_SIMPLEX,\n  31:                 font_scale, color, thickness, cv2.LINE_AA)\n  32: \n  33: def get_colors(num_classes):\n  34:     colors = plt.cm.hsv(np.linspace(0, 1, num_classes)).tolist()\n  35:     colors = np.asarray(colors) * 255\n  36:     return colors\n  37: ', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/utils/data_augmentation.py': '   1: import numpy as np\n   2: from random import shuffle\n   3: from .preprocessor import preprocess_input\n   4: from .preprocessor import _imread as imread\n   5: from .preprocessor import _imresize as imresize\n   6: from .preprocessor import to_categorical\n   7: import scipy.ndimage as ndi\n   8: import cv2\n   9: \n  10: \n  11: class ImageGenerator(object):\n  12:     """ Image generator with saturation, brightness, lighting, contrast,\n  13:     horizontal flip and vertical flip transformations. It supports\n  14:     bounding boxes coordinates.\n  15: \n  16:     TODO:\n  17:         - Finish support for not using bounding_boxes\n  18:             - Random crop\n  19:             - Test other transformations\n  20:     """\n  21:     def __init__(self, ground_truth_data, batch_size, image_size,\n  22:                  train_keys, validation_keys,\n  23:                  ground_truth_transformer=None,\n  24:                  path_prefix=None,\n  25:                  saturation_var=0.5,\n  26:                  brightness_var=0.5,\n  27:                  contrast_var=0.5,\n  28:                  lighting_std=0.5,\n  29:                  horizontal_flip_probability=0.5,\n  30:                  vertical_flip_probability=0.5,\n  31:                  do_random_crop=False,\n  32:                  grayscale=False,\n  33:                  zoom_range=[0.75, 1.25],\n  34:                  translation_factor=.3):\n  35: \n  36:         self.ground_truth_data = ground_truth_data\n  37:         self.ground_truth_transformer = ground_truth_transformer\n  38:         self.batch_size = batch_size\n  39:         self.path_prefix = path_prefix\n  40:         self.train_keys = train_keys\n  41:         self.validation_keys = validation_keys\n  42:         self.image_size = image_size\n  43:         self.grayscale = grayscale\n  44:         self.color_jitter = []\n  45:         if saturation_var:\n  46:             self.saturation_var = saturation_var\n  47:             self.color_jitter.append(self.saturation)\n  48:         if brightness_var:\n  49:             self.brightness_var = brightness_var\n  50:             self.color_jitter.append(self.brightness)\n  51:         if contrast_var:\n  52:             self.contrast_var = contrast_var\n  53:             self.color_jitter.append(self.contrast)\n  54:         self.lighting_std = lighting_std\n  55:         self.horizontal_flip_probability = horizontal_flip_probability\n  56:         self.vertical_flip_probability = vertical_flip_probability\n  57:         self.do_random_crop = do_random_crop\n  58:         self.zoom_range = zoom_range\n  59:         self.translation_factor = translation_factor\n  60: \n  61:     def _do_random_crop(self, image_array):\n  62:         """IMPORTANT: random crop only works for classification since the\n  63:         current implementation does no transform bounding boxes"""\n  64:         height = image_array.shape[0]\n  65:         width = image_array.shape[1]\n  66:         x_offset = np.random.uniform(0, self.translation_factor * width)\n  67:         y_offset = np.random.uniform(0, self.translation_factor * height)\n  68:         offset = np.array([x_offset, y_offset])\n  69:         scale_factor = np.random.uniform(self.zoom_range[0],\n  70:                                          self.zoom_range[1])\n  71:         crop_matrix = np.array([[scale_factor, 0],\n  72:                                 [0, scale_factor]])\n  73: \n  74:         image_array = np.rollaxis(image_array, axis=-1, start=0)\n  75:         image_channel = [ndi.interpolation.affine_transform(image_channel,\n  76:                          crop_matrix, offset=offset, order=0, mode=\'nearest\',\n  77:                          cval=0.0) for image_channel in image_array]\n  78: \n  79:         image_array = np.stack(image_channel, axis=0)\n  80:         image_array = np.rollaxis(image_array, 0, 3)\n  81:         return image_array\n  82: \n  83:     def do_random_rotation(self, image_array):\n  84:         """IMPORTANT: random rotation only works for classification since the\n  85:         current implementation does no transform bounding boxes"""\n  86:         height = image_array.shape[0]\n  87:         width = image_array.shape[1]\n  88:         x_offset = np.random.uniform(0, self.translation_factor * width)\n  89:         y_offset = np.random.uniform(0, self.translation_factor * height)\n  90:         offset = np.array([x_offset, y_offset])\n  91:         scale_factor = np.random.uniform(self.zoom_range[0],\n  92:                                          self.zoom_range[1])\n  93:         crop_matrix = np.array([[scale_factor, 0],\n  94:                                 [0, scale_factor]])\n  95: \n  96:         image_array = np.rollaxis(image_array, axis=-1, start=0)\n  97:         image_channel = [ndi.interpolation.affine_transform(image_channel,\n  98:                          crop_matrix, offset=offset, order=0, mode=\'nearest\',\n  99:                          cval=0.0) for image_channel in image_array]\n 100: \n 101:         image_array = np.stack(image_channel, axis=0)\n 102:         image_array = np.rollaxis(image_array, 0, 3)\n 103:         return image_array\n 104: \n 105:     def _gray_scale(self, image_array):\n 106:         return image_array.dot([0.299, 0.587, 0.114])\n 107: \n 108:     def saturation(self, image_array):\n 109:         gray_scale = self._gray_scale(image_array)\n 110:         alpha = 2.0 * np.random.random() * self.brightness_var\n 111:         alpha = alpha + 1 - self.saturation_var\n 112:         image_array = (alpha * image_array + (1 - alpha) *\n 113:                        gray_scale[:, :, None])\n 114:         return np.clip(image_array, 0, 255)\n 115: \n 116:     def brightness(self, image_array):\n 117:         alpha = 2 * np.random.random() * self.brightness_var\n 118:         alpha = alpha + 1 - self.saturation_var\n 119:         image_array = alpha * image_array\n 120:         return np.clip(image_array, 0, 255)\n 121: \n 122:     def contrast(self, image_array):\n 123:         gray_scale = (self._gray_scale(image_array).mean() *\n 124:                       np.ones_like(image_array))\n 125:         alpha = 2 * np.random.random() * self.contrast_var\n 126:         alpha = alpha + 1 - self.contrast_var\n 127:         image_array = image_array * alpha + (1 - alpha) * gray_scale\n 128:         return np.clip(image_array, 0, 255)\n 129: \n 130:     def lighting(self, image_array):\n 131:         covariance_matrix = np.cov(image_array.reshape(-1, 3) /\n 132:                                    255.0, rowvar=False)\n 133:         eigen_values, eigen_vectors = np.linalg.eigh(covariance_matrix)\n 134:         noise = np.random.randn(3) * self.lighting_std\n 135:         noise = eigen_vectors.dot(eigen_values * noise) * 255\n 136:         image_array = image_array + noise\n 137:         return np.clip(image_array, 0, 255)\n 138: \n 139:     def horizontal_flip(self, image_array, box_corners=None):\n 140:         if np.random.random() < self.horizontal_flip_probability:\n 141:             image_array = image_array[:, ::-1]\n 142:             if box_corners is not None:\n 143:                 box_corners[:, [0, 2]] = 1 - box_corners[:, [2, 0]]\n 144:         return image_array, box_corners\n 145: \n 146:     def vertical_flip(self, image_array, box_corners=None):\n 147:         if (np.random.random() < self.vertical_flip_probability):\n 148:             image_array = image_array[::-1]\n 149:             if box_corners is not None:\n 150:                 box_corners[:, [1, 3]] = 1 - box_corners[:, [3, 1]]\n 151:         return image_array, box_corners\n 152: \n 153:     def transform(self, image_array, box_corners=None):\n 154:         shuffle(self.color_jitter)\n 155:         for jitter in self.color_jitter:\n 156:             image_array = jitter(image_array)\n 157: \n 158:         if self.lighting_std:\n 159:             image_array = self.lighting(image_array)\n 160: \n 161:         if self.horizontal_flip_probability > 0:\n 162:             image_array, box_corners = self.horizontal_flip(image_array,\n 163:                                                             box_corners)\n 164: \n 165:         if self.vertical_flip_probability > 0:\n 166:             image_array, box_corners = self.vertical_flip(image_array,\n 167:                                                           box_corners)\n 168:         return image_array, box_corners\n 169: \n 170:     def preprocess_images(self, image_array):\n 171:         return preprocess_input(image_array)\n 172: \n 173:     def flow(self, mode=\'train\'):\n 174:             while True:\n 175:                 if mode == \'train\':\n 176:                     shuffle(self.train_keys)\n 177:                     keys = self.train_keys\n 178:                 elif mode == \'val\' or mode == \'demo\':\n 179:                     shuffle(self.validation_keys)\n 180:                     keys = self.validation_keys\n 181:                 else:\n 182:                     raise Exception(\'invalid mode: %s\' % mode)\n 183: \n 184:                 inputs = []\n 185:                 targets = []\n 186:                 for key in keys:\n 187:                     image_path = self.path_prefix + key\n 188:                     image_array = imread(image_path)\n 189:                     image_array = imresize(image_array, self.image_size)\n 190: \n 191:                     num_image_channels = len(image_array.shape)\n 192:                     if num_image_channels != 3:\n 193:                         continue\n 194: \n 195:                     ground_truth = self.ground_truth_data[key]\n 196: \n 197:                     if self.do_random_crop:\n 198:                         image_array = self._do_random_crop(image_array)\n 199: \n 200:                     image_array = image_array.astype(\'float32\')\n 201:                     if mode == \'train\' or mode == \'demo\':\n 202:                         if self.ground_truth_transformer is not None:\n 203:                             image_array, ground_truth = self.transform(\n 204:                                                                 image_array,\n 205:                                                                 ground_truth)\n 206:                             ground_truth = (\n 207:                                 self.ground_truth_transformer.assign_boxes(\n 208:                                                             ground_truth))\n 209:                         else:\n 210:                             image_array = self.transform(image_array)[0]\n 211: \n 212:                     if self.grayscale:\n 213:                         image_array = cv2.cvtColor(\n 214:                                 image_array.astype(\'uint8\'),\n 215:                                 cv2.COLOR_RGB2GRAY).astype(\'float32\')\n 216:                         image_array = np.expand_dims(image_array, -1)\n 217: \n 218:                     inputs.append(image_array)\n 219:                     targets.append(ground_truth)\n 220:                     if len(targets) == self.batch_size:\n 221:                         inputs = np.asarray(inputs)\n 222:                         targets = np.asarray(targets)\n 223:                         # this will not work for boxes\n 224:                         targets = to_categorical(targets)\n 225:                         if mode == \'train\' or mode == \'val\':\n 226:                             inputs = self.preprocess_images(inputs)\n 227:                             yield self._wrap_in_dictionary(inputs, targets)\n 228:                         if mode == \'demo\':\n 229:                             yield self._wrap_in_dictionary(inputs, targets)\n 230:                         inputs = []\n 231:                         targets = []\n 232: \n 233:     def _wrap_in_dictionary(self, image_array, targets):\n 234:         return [{\'input_1\': image_array},\n 235:                 {\'predictions\': targets}]', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/utils/visualizer.py': '   1: import numpy as np\n   2: import matplotlib.cm as cm\n   3: from mpl_toolkits.axes_grid1 import make_axes_locatable\n   4: import matplotlib.pyplot as plt\n   5: import numpy.ma as ma\n   6: \n   7: \n   8: def make_mosaic(images, num_rows, num_cols, border=1, class_names=None):\n   9:     num_images = len(images)\n  10:     image_shape = images.shape[1:]\n  11:     mosaic = ma.masked_all(\n  12:             (num_rows * image_shape[0] + (num_rows - 1) * border,\n  13:              num_cols * image_shape[1] + (num_cols - 1) * border),\n  14:             dtype=np.float32)\n  15:     paddedh = image_shape[0] + border\n  16:     paddedw = image_shape[1] + border\n  17:     for image_arg in range(num_images):\n  18:         row = int(np.floor(image_arg / num_cols))\n  19:         col = image_arg % num_cols\n  20:         image = np.squeeze(images[image_arg])\n  21:         image_shape = image.shape\n  22:         mosaic[row * paddedh:row * paddedh + image_shape[0],\n  23:                col * paddedw:col * paddedw + image_shape[1]] = image\n  24:     return mosaic\n  25: \n  26: \n  27: def make_mosaic_v2(images, num_mosaic_rows=None,\n  28:                    num_mosaic_cols=None, border=1):\n  29:     images = np.squeeze(images)\n  30:     num_images, image_pixels_rows, image_pixels_cols = images.shape\n  31:     if num_mosaic_rows is None and num_mosaic_cols is None:\n  32:         box_size = int(np.ceil(np.sqrt(num_images)))\n  33:         num_mosaic_rows = num_mosaic_cols = box_size\n  34:     num_mosaic_pixel_rows = num_mosaic_rows * (image_pixels_rows + border)\n  35:     num_mosaic_pixel_cols = num_mosaic_cols * (image_pixels_cols + border)\n  36:     mosaic = np.empty(shape=(num_mosaic_pixel_rows, num_mosaic_pixel_cols))\n  37:     mosaic_col_arg = 0\n  38:     mosaic_row_arg = 0\n  39:     for image_arg in range(num_images):\n  40:         if image_arg % num_mosaic_cols == 0 and image_arg != 0:\n  41:             mosaic_col_arg = mosaic_col_arg + 1\n  42:             mosaic_row_arg = 0\n  43:         x0 = image_pixels_cols * (mosaic_row_arg)\n  44:         x1 = image_pixels_cols * (mosaic_row_arg + 1)\n  45:         y0 = image_pixels_rows * (mosaic_col_arg)\n  46:         y1 = image_pixels_rows * (mosaic_col_arg + 1)\n  47:         image = images[image_arg]\n  48:         mosaic[y0:y1, x0:x1] = image\n  49:         mosaic_row_arg = mosaic_row_arg + 1\n  50:     return mosaic\n  51: \n  52: \n  53: def pretty_imshow(axis, data, vmin=None, vmax=None, cmap=None):\n  54:     if cmap is None:\n  55:         cmap = cm.jet\n  56:     if vmin is None:\n  57:         vmin = data.min()\n  58:     if vmax is None:\n  59:         vmax = data.max()\n  60:     cax = None\n  61:     divider = make_axes_locatable(axis)\n  62:     cax = divider.append_axes(\'right\', size=\'5%\', pad=0.05)\n  63:     image = axis.imshow(data, vmin=vmin, vmax=vmax,\n  64:                         interpolation=\'nearest\', cmap=cmap)\n  65:     plt.colorbar(image, cax=cax)\n  66: \n  67: \n  68: def normal_imshow(axis, data, vmin=None, vmax=None,\n  69:                   cmap=None, axis_off=True):\n  70:     if cmap is None:\n  71:         cmap = cm.jet\n  72:     if vmin is None:\n  73:         vmin = data.min()\n  74:     if vmax is None:\n  75:         vmax = data.max()\n  76:     image = axis.imshow(data, vmin=vmin, vmax=vmax,\n  77:                         interpolation=\'nearest\', cmap=cmap)\n  78:     if axis_off:\n  79:         plt.axis(\'off\')\n  80:     return image\n  81: \n  82: \n  83: def display_image(face, class_vector=None,\n  84:                   class_decoder=None, pretty=False):\n  85:     if class_vector is not None and class_decoder is None:\n  86:         raise Exception(\'Provide class decoder\')\n  87:     face = np.squeeze(face)\n  88:     color_map = None\n  89:     if len(face.shape) < 3:\n  90:         color_map = \'gray\'\n  91:     plt.figure()\n  92:     if class_vector is not None:\n  93:         class_arg = np.argmax(class_vector)\n  94:         class_name = class_decoder[class_arg]\n  95:         plt.title(class_name)\n  96:     if pretty:\n  97:         pretty_imshow(plt.gca(), face, cmap=color_map)\n  98:     else:\n  99:         plt.imshow(face, color_map)\n 100: \n 101: \n 102: def draw_mosaic(data, num_rows, num_cols, class_vectors=None,\n 103:                 class_decoder=None, cmap=\'gray\'):\n 104: \n 105:     if class_vectors is not None and class_decoder is None:\n 106:         raise Exception(\'Provide class decoder\')\n 107: \n 108:     figure, axis_array = plt.subplots(num_rows, num_cols)\n 109:     figure.set_size_inches(8, 8, forward=True)\n 110:     titles = []\n 111:     if class_vectors is not None:\n 112:         for vector_arg in range(len(class_vectors)):\n 113:             class_arg = np.argmax(class_vectors[vector_arg])\n 114:             class_name = class_decoder[class_arg]\n 115:             titles.append(class_name)\n 116: \n 117:     image_arg = 0\n 118:     for row_arg in range(num_rows):\n 119:         for col_arg in range(num_cols):\n 120:             image = data[image_arg]\n 121:             image = np.squeeze(image)\n 122:             axis_array[row_arg, col_arg].axis(\'off\')\n 123:             axis_array[row_arg, col_arg].imshow(image, cmap=cmap)\n 124:             axis_array[row_arg, col_arg].set_title(titles[image_arg])\n 125:             image_arg = image_arg + 1\n 126:     plt.tight_layout()\n 127: \n 128: \n 129: if __name__ == \'__main__\':\n 130:     # from utils.data_manager import DataManager\n 131:     from utils.utils import get_labels\n 132:     from keras.models import load_model\n 133:     import pickle\n 134: \n 135:     # dataset_name = \'fer2013\'\n 136:     # model_path = \'../trained_models/emotion_models/simple_CNN.985-0.66.hdf5\'\n 137:     dataset_name = \'fer2013\'\n 138:     class_decoder = get_labels(dataset_name)\n 139:     # data_manager = DataManager(dataset_name)\n 140:     # faces, emotions = data_manager.get_data()\n 141:     faces = pickle.load(open(\'faces.pkl\', \'rb\'))\n 142:     emotions = pickle.load(open(\'emotions.pkl\', \'rb\'))\n 143:     pretty_imshow(plt.gca(), make_mosaic(faces[:4], 2, 2), cmap=\'gray\')\n 144:     plt.show()\n 145: \n 146:     """\n 147:     image_arg = 0\n 148:     face = faces[image_arg:image_arg + 1]\n 149:     emotion = emotions[image_arg:image_arg + 1]\n 150:     display_image(face, emotion, class_decoder)\n 151:     plt.show()\n 152: \n 153:     normal_imshow(plt.gca(), make_mosaic(faces[:4], 3, 3), cmap=\'gray\')\n 154:     plt.show()\n 155: \n 156:     draw_mosaic(faces, 2, 2, emotions, class_decoder)\n 157:     plt.show()\n 158: \n 159:     """\n 160:     model = load_model(\'../trained_models/emotion_models/simple_CNN.985-0.66.hdf5\')\n 161:     conv1_weights = model.layers[2].get_weights()\n 162:     kernel_conv1_weights = conv1_weights[0]\n 163:     kernel_conv1_weights = np.squeeze(kernel_conv1_weights)\n 164:     kernel_conv1_weights = np.rollaxis(kernel_conv1_weights, 2, 0)\n 165:     kernel_conv1_weights = np.expand_dims(kernel_conv1_weights, -1)\n 166:     num_kernels = kernel_conv1_weights.shape[0]\n 167:     box_size = int(np.ceil(np.sqrt(num_kernels)))\n 168:     print(\'Box size:\', box_size)\n 169: \n 170:     print(\'Kernel shape\', kernel_conv1_weights.shape)\n 171:     plt.figure(figsize=(15, 15))\n 172:     plt.title(\'conv1 weights\')\n 173:     pretty_imshow(\n 174:             plt.gca(),\n 175:             make_mosaic(kernel_conv1_weights, box_size, box_size),\n 176:             cmap=cm.binary)\n 177:     plt.show()', '/Users/yeonjoon-mac/Desktop/Workspace/code-diagram/study_1/face_classification/models/cnn.py': '   1: from keras.layers import Activation, Convolution2D, Dropout, Conv2D\n   2: from keras.layers import AveragePooling2D, BatchNormalization\n   3: from keras.layers import GlobalAveragePooling2D\n   4: from keras.models import Sequential\n   5: from keras.layers import Flatten\n   6: from keras.models import Model\n   7: from keras.layers import Input\n   8: from keras.layers import MaxPooling2D\n   9: from keras.layers import SeparableConv2D\n  10: from keras import layers\n  11: from keras.regularizers import l2\n  12: \n  13: \n  14: def simple_CNN(input_shape, num_classes):\n  15: \n  16:     model = Sequential()\n  17:     model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding=\'same\',\n  18:                             name=\'image_array\', input_shape=input_shape))\n  19:     model.add(BatchNormalization())\n  20:     model.add(Convolution2D(filters=16, kernel_size=(7, 7), padding=\'same\'))\n  21:     model.add(BatchNormalization())\n  22:     model.add(Activation(\'relu\'))\n  23:     model.add(AveragePooling2D(pool_size=(2, 2), padding=\'same\'))\n  24:     model.add(Dropout(.5))\n  25: \n  26:     model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding=\'same\'))\n  27:     model.add(BatchNormalization())\n  28:     model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding=\'same\'))\n  29:     model.add(BatchNormalization())\n  30:     model.add(Activation(\'relu\'))\n  31:     model.add(AveragePooling2D(pool_size=(2, 2), padding=\'same\'))\n  32:     model.add(Dropout(.5))\n  33: \n  34:     model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding=\'same\'))\n  35:     model.add(BatchNormalization())\n  36:     model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding=\'same\'))\n  37:     model.add(BatchNormalization())\n  38:     model.add(Activation(\'relu\'))\n  39:     model.add(AveragePooling2D(pool_size=(2, 2), padding=\'same\'))\n  40:     model.add(Dropout(.5))\n  41: \n  42:     model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding=\'same\'))\n  43:     model.add(BatchNormalization())\n  44:     model.add(Convolution2D(filters=128, kernel_size=(3, 3), padding=\'same\'))\n  45:     model.add(BatchNormalization())\n  46:     model.add(Activation(\'relu\'))\n  47:     model.add(AveragePooling2D(pool_size=(2, 2), padding=\'same\'))\n  48:     model.add(Dropout(.5))\n  49: \n  50:     model.add(Convolution2D(filters=256, kernel_size=(3, 3), padding=\'same\'))\n  51:     model.add(BatchNormalization())\n  52:     model.add(Convolution2D(\n  53:         filters=num_classes, kernel_size=(3, 3), padding=\'same\'))\n  54:     model.add(GlobalAveragePooling2D())\n  55:     model.add(Activation(\'softmax\', name=\'predictions\'))\n  56:     return model\n  57: \n  58: \n  59: def simpler_CNN(input_shape, num_classes):\n  60: \n  61:     model = Sequential()\n  62:     model.add(Convolution2D(filters=16, kernel_size=(5, 5), padding=\'same\',\n  63:                             name=\'image_array\', input_shape=input_shape))\n  64:     model.add(BatchNormalization())\n  65:     model.add(Convolution2D(filters=16, kernel_size=(5, 5),\n  66:                             strides=(2, 2), padding=\'same\'))\n  67:     model.add(BatchNormalization())\n  68:     model.add(Activation(\'relu\'))\n  69:     model.add(Dropout(.25))\n  70: \n  71:     model.add(Convolution2D(filters=32, kernel_size=(5, 5), padding=\'same\'))\n  72:     model.add(BatchNormalization())\n  73:     model.add(Convolution2D(filters=32, kernel_size=(5, 5),\n  74:                             strides=(2, 2), padding=\'same\'))\n  75:     model.add(BatchNormalization())\n  76:     model.add(Activation(\'relu\'))\n  77:     model.add(Dropout(.25))\n  78: \n  79:     model.add(Convolution2D(filters=64, kernel_size=(3, 3), padding=\'same\'))\n  80:     model.add(BatchNormalization())\n  81:     model.add(Convolution2D(filters=64, kernel_size=(3, 3),\n  82:                             strides=(2, 2), padding=\'same\'))\n  83:     model.add(BatchNormalization())\n  84:     model.add(Activation(\'relu\'))\n  85:     model.add(Dropout(.25))\n  86: \n  87:     model.add(Convolution2D(filters=64, kernel_size=(1, 1), padding=\'same\'))\n  88:     model.add(BatchNormalization())\n  89:     model.add(Convolution2D(filters=128, kernel_size=(3, 3),\n  90:                             strides=(2, 2), padding=\'same\'))\n  91:     model.add(BatchNormalization())\n  92:     model.add(Activation(\'relu\'))\n  93:     model.add(Dropout(.25))\n  94: \n  95:     model.add(Convolution2D(filters=256, kernel_size=(1, 1), padding=\'same\'))\n  96:     model.add(BatchNormalization())\n  97:     model.add(Convolution2D(filters=128, kernel_size=(3, 3),\n  98:                             strides=(2, 2), padding=\'same\'))\n  99: \n 100:     model.add(Convolution2D(filters=256, kernel_size=(1, 1), padding=\'same\'))\n 101:     model.add(BatchNormalization())\n 102:     model.add(Convolution2D(filters=num_classes, kernel_size=(3, 3),\n 103:                             strides=(2, 2), padding=\'same\'))\n 104: \n 105:     model.add(Flatten())\n 106:     # model.add(GlobalAveragePooling2D())\n 107:     model.add(Activation(\'softmax\', name=\'predictions\'))\n 108:     return model\n 109: \n 110: \n 111: def tiny_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n 112:     regularization = l2(l2_regularization)\n 113: \n 114:     # base\n 115:     img_input = Input(input_shape)\n 116:     x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n 117:                use_bias=False)(img_input)\n 118:     x = BatchNormalization()(x)\n 119:     x = Activation(\'relu\')(x)\n 120:     x = Conv2D(5, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n 121:                use_bias=False)(x)\n 122:     x = BatchNormalization()(x)\n 123:     x = Activation(\'relu\')(x)\n 124: \n 125:     # module 1\n 126:     residual = Conv2D(8, (1, 1), strides=(2, 2),\n 127:                       padding=\'same\', use_bias=False)(x)\n 128:     residual = BatchNormalization()(residual)\n 129: \n 130:     x = SeparableConv2D(8, (3, 3), padding=\'same\',\n 131:                         kernel_regularizer=regularization,\n 132:                         use_bias=False)(x)\n 133:     x = BatchNormalization()(x)\n 134:     x = Activation(\'relu\')(x)\n 135:     x = SeparableConv2D(8, (3, 3), padding=\'same\',\n 136:                         kernel_regularizer=regularization,\n 137:                         use_bias=False)(x)\n 138:     x = BatchNormalization()(x)\n 139: \n 140:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 141:     x = layers.add([x, residual])\n 142: \n 143:     # module 2\n 144:     residual = Conv2D(16, (1, 1), strides=(2, 2),\n 145:                       padding=\'same\', use_bias=False)(x)\n 146:     residual = BatchNormalization()(residual)\n 147: \n 148:     x = SeparableConv2D(16, (3, 3), padding=\'same\',\n 149:                         kernel_regularizer=regularization,\n 150:                         use_bias=False)(x)\n 151:     x = BatchNormalization()(x)\n 152:     x = Activation(\'relu\')(x)\n 153:     x = SeparableConv2D(16, (3, 3), padding=\'same\',\n 154:                         kernel_regularizer=regularization,\n 155:                         use_bias=False)(x)\n 156:     x = BatchNormalization()(x)\n 157: \n 158:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 159:     x = layers.add([x, residual])\n 160: \n 161:     # module 3\n 162:     residual = Conv2D(32, (1, 1), strides=(2, 2),\n 163:                       padding=\'same\', use_bias=False)(x)\n 164:     residual = BatchNormalization()(residual)\n 165: \n 166:     x = SeparableConv2D(32, (3, 3), padding=\'same\',\n 167:                         kernel_regularizer=regularization,\n 168:                         use_bias=False)(x)\n 169:     x = BatchNormalization()(x)\n 170:     x = Activation(\'relu\')(x)\n 171:     x = SeparableConv2D(32, (3, 3), padding=\'same\',\n 172:                         kernel_regularizer=regularization,\n 173:                         use_bias=False)(x)\n 174:     x = BatchNormalization()(x)\n 175: \n 176:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 177:     x = layers.add([x, residual])\n 178: \n 179:     # module 4\n 180:     residual = Conv2D(64, (1, 1), strides=(2, 2),\n 181:                       padding=\'same\', use_bias=False)(x)\n 182:     residual = BatchNormalization()(residual)\n 183: \n 184:     x = SeparableConv2D(64, (3, 3), padding=\'same\',\n 185:                         kernel_regularizer=regularization,\n 186:                         use_bias=False)(x)\n 187:     x = BatchNormalization()(x)\n 188:     x = Activation(\'relu\')(x)\n 189:     x = SeparableConv2D(64, (3, 3), padding=\'same\',\n 190:                         kernel_regularizer=regularization,\n 191:                         use_bias=False)(x)\n 192:     x = BatchNormalization()(x)\n 193: \n 194:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 195:     x = layers.add([x, residual])\n 196: \n 197:     x = Conv2D(num_classes, (3, 3),\n 198:                # kernel_regularizer=regularization,\n 199:                padding=\'same\')(x)\n 200:     x = GlobalAveragePooling2D()(x)\n 201:     output = Activation(\'softmax\', name=\'predictions\')(x)\n 202: \n 203:     model = Model(img_input, output)\n 204:     return model\n 205: \n 206: \n 207: def mini_XCEPTION(input_shape, num_classes, l2_regularization=0.01):\n 208:     regularization = l2(l2_regularization)\n 209: \n 210:     # base\n 211:     img_input = Input(input_shape)\n 212:     x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n 213:                use_bias=False)(img_input)\n 214:     x = BatchNormalization()(x)\n 215:     x = Activation(\'relu\')(x)\n 216:     x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization,\n 217:                use_bias=False)(x)\n 218:     x = BatchNormalization()(x)\n 219:     x = Activation(\'relu\')(x)\n 220: \n 221:     # module 1\n 222:     residual = Conv2D(16, (1, 1), strides=(2, 2),\n 223:                       padding=\'same\', use_bias=False)(x)\n 224:     residual = BatchNormalization()(residual)\n 225: \n 226:     x = SeparableConv2D(16, (3, 3), padding=\'same\',\n 227:                         kernel_regularizer=regularization,\n 228:                         use_bias=False)(x)\n 229:     x = BatchNormalization()(x)\n 230:     x = Activation(\'relu\')(x)\n 231:     x = SeparableConv2D(16, (3, 3), padding=\'same\',\n 232:                         kernel_regularizer=regularization,\n 233:                         use_bias=False)(x)\n 234:     x = BatchNormalization()(x)\n 235: \n 236:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 237:     x = layers.add([x, residual])\n 238: \n 239:     # module 2\n 240:     residual = Conv2D(32, (1, 1), strides=(2, 2),\n 241:                       padding=\'same\', use_bias=False)(x)\n 242:     residual = BatchNormalization()(residual)\n 243: \n 244:     x = SeparableConv2D(32, (3, 3), padding=\'same\',\n 245:                         kernel_regularizer=regularization,\n 246:                         use_bias=False)(x)\n 247:     x = BatchNormalization()(x)\n 248:     x = Activation(\'relu\')(x)\n 249:     x = SeparableConv2D(32, (3, 3), padding=\'same\',\n 250:                         kernel_regularizer=regularization,\n 251:                         use_bias=False)(x)\n 252:     x = BatchNormalization()(x)\n 253: \n 254:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 255:     x = layers.add([x, residual])\n 256: \n 257:     # module 3\n 258:     residual = Conv2D(64, (1, 1), strides=(2, 2),\n 259:                       padding=\'same\', use_bias=False)(x)\n 260:     residual = BatchNormalization()(residual)\n 261: \n 262:     x = SeparableConv2D(64, (3, 3), padding=\'same\',\n 263:                         kernel_regularizer=regularization,\n 264:                         use_bias=False)(x)\n 265:     x = BatchNormalization()(x)\n 266:     x = Activation(\'relu\')(x)\n 267:     x = SeparableConv2D(64, (3, 3), padding=\'same\',\n 268:                         kernel_regularizer=regularization,\n 269:                         use_bias=False)(x)\n 270:     x = BatchNormalization()(x)\n 271: \n 272:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 273:     x = layers.add([x, residual])\n 274: \n 275:     # module 4\n 276:     residual = Conv2D(128, (1, 1), strides=(2, 2),\n 277:                       padding=\'same\', use_bias=False)(x)\n 278:     residual = BatchNormalization()(residual)\n 279: \n 280:     x = SeparableConv2D(128, (3, 3), padding=\'same\',\n 281:                         kernel_regularizer=regularization,\n 282:                         use_bias=False)(x)\n 283:     x = BatchNormalization()(x)\n 284:     x = Activation(\'relu\')(x)\n 285:     x = SeparableConv2D(128, (3, 3), padding=\'same\',\n 286:                         kernel_regularizer=regularization,\n 287:                         use_bias=False)(x)\n 288:     x = BatchNormalization()(x)\n 289: \n 290:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 291:     x = layers.add([x, residual])\n 292: \n 293:     x = Conv2D(num_classes, (3, 3),\n 294:                # kernel_regularizer=regularization,\n 295:                padding=\'same\')(x)\n 296:     x = GlobalAveragePooling2D()(x)\n 297:     output = Activation(\'softmax\', name=\'predictions\')(x)\n 298: \n 299:     model = Model(img_input, output)\n 300:     return model\n 301: \n 302: \n 303: def big_XCEPTION(input_shape, num_classes):\n 304:     img_input = Input(input_shape)\n 305:     x = Conv2D(32, (3, 3), strides=(2, 2), use_bias=False)(img_input)\n 306:     x = BatchNormalization(name=\'block1_conv1_bn\')(x)\n 307:     x = Activation(\'relu\', name=\'block1_conv1_act\')(x)\n 308:     x = Conv2D(64, (3, 3), use_bias=False)(x)\n 309:     x = BatchNormalization(name=\'block1_conv2_bn\')(x)\n 310:     x = Activation(\'relu\', name=\'block1_conv2_act\')(x)\n 311: \n 312:     residual = Conv2D(128, (1, 1), strides=(2, 2),\n 313:                       padding=\'same\', use_bias=False)(x)\n 314:     residual = BatchNormalization()(residual)\n 315: \n 316:     x = SeparableConv2D(128, (3, 3), padding=\'same\', use_bias=False)(x)\n 317:     x = BatchNormalization(name=\'block2_sepconv1_bn\')(x)\n 318:     x = Activation(\'relu\', name=\'block2_sepconv2_act\')(x)\n 319:     x = SeparableConv2D(128, (3, 3), padding=\'same\', use_bias=False)(x)\n 320:     x = BatchNormalization(name=\'block2_sepconv2_bn\')(x)\n 321: \n 322:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 323:     x = layers.add([x, residual])\n 324: \n 325:     residual = Conv2D(256, (1, 1), strides=(2, 2),\n 326:                       padding=\'same\', use_bias=False)(x)\n 327:     residual = BatchNormalization()(residual)\n 328: \n 329:     x = Activation(\'relu\', name=\'block3_sepconv1_act\')(x)\n 330:     x = SeparableConv2D(256, (3, 3), padding=\'same\', use_bias=False)(x)\n 331:     x = BatchNormalization(name=\'block3_sepconv1_bn\')(x)\n 332:     x = Activation(\'relu\', name=\'block3_sepconv2_act\')(x)\n 333:     x = SeparableConv2D(256, (3, 3), padding=\'same\', use_bias=False)(x)\n 334:     x = BatchNormalization(name=\'block3_sepconv2_bn\')(x)\n 335: \n 336:     x = MaxPooling2D((3, 3), strides=(2, 2), padding=\'same\')(x)\n 337:     x = layers.add([x, residual])\n 338:     x = Conv2D(num_classes, (3, 3),\n 339:                # kernel_regularizer=regularization,\n 340:                padding=\'same\')(x)\n 341:     x = GlobalAveragePooling2D()(x)\n 342:     output = Activation(\'softmax\', name=\'predictions\')(x)\n 343: \n 344:     model = Model(img_input, output)\n 345:     return model\n 346: \n 347: \n 348: if __name__ == "__main__":\n 349:     input_shape = (64, 64, 1)\n 350:     num_classes = 7\n 351:     # model = tiny_XCEPTION(input_shape, num_classes)\n 352:     # model.summary()\n 353:     # model = mini_XCEPTION(input_shape, num_classes)\n 354:     # model.summary()\n 355:     # model = big_XCEPTION(input_shape, num_classes)\n 356:     # model.summary()\n 357:     model = simple_CNN((48, 48, 1), num_classes)\n 358:     model.summary()'}